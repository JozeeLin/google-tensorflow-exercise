{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "基于LSTM的语言模型.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/JozeeLin/google-tensorflow-exercise/blob/master/%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "533Pfu9McJN6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "循环神经网络由于神经网络结构的进步和GPU上深度学习训练效率的突破，RNN变得越来越流行。RNN对时间序列数据非常有效，其每个神经元可通过内部组件保存之前输入的信息。\n",
        "\n",
        "人每次思考时不会从头开始，而是保留之前思考的一些结果为现在的决策提供支持。例如我们对话时，会根据上下文的信息理解一句话的含义，而不是对每一句话从头进行分析。\n",
        "\n",
        "例如卷积神经网络虽然可以对图像进行分类，但是可能无法对视频中每一帧图像发生的事情进行关联分析，我们无法利用前一帧图像的信息，而循环神经网络则可以解决这个问题。\n",
        "\n",
        "RNN最大特点是神经元的某些输出可作为其输入再次传输到神经元中，因此可以利用之前的信息。\n",
        "\n",
        "RNN虽然被设计成可以处理整个时间序列信息，但是其记忆最深的还是最后输入的一些信号。而更早之前的信号的强度则越来越低，最后只能起到一点辅助的作用，即决定RNN输出的还是最后输入的一些信号。\n",
        "\n",
        "对于某些简单的问题，可能只需要最后输入的少量时序信息即可解决。但对某些复杂的问题，可能需要更早的一些信息，甚至是时间序列开头的信息，但间隔太远的输入信息，RNN是难以记忆的。因此长程依赖是传统RNN的致命伤。\n",
        "\n",
        "## LSTM\n",
        "包含4层神经网络\n",
        "\n",
        "## 语言模型\n",
        "\n",
        "语言模型是NLP中非常重要的一个部分，同时也是语音识别、机器翻译和由图片生成标题等任务的基础和关键。语言模型是一个可以预测语言的概率模型。给定上文的语境，\n",
        "\n",
        "即历史出现的单词，语言模型可以预测下一个单词出现的频率。\n",
        "\n",
        "**Penn Tree Bank(PTB)是在语言模型训练中经常使用的一个数据集**，它的质量比较高，可以用来评测语言模型的准确率，同时数据集不大，训练也比较快。参考论文[Recurrent Neural Network Regularization](https://arxiv.org/pdf/1409.2329.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "EJr1mllG2dAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "7d51e7bd-441e-498a-cf80-f1a06de771c8"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Counting objects: 16124, done.\u001b[K\n",
            "remote: Total 16124 (delta 2), reused 2 (delta 2), pack-reused 16121\u001b[K\n",
            "Receiving objects: 100% (16124/16124), 424.07 MiB | 20.66 MiB/s, done.\n",
            "Resolving deltas: 100% (9519/9519), done.\n",
            "Checking out files: 100% (2163/2163), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5jFnRs432iA0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('models/tutorials/rnn/ptb')\n",
        "import time \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import reader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mUNOp6f2cGO0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "27b9c5c9-5af8-4e14-c8eb-1facd1835f01"
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-05-07 09:07:03--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\r\n",
            "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
            "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34869662 (33M) [application/x-gtar]\n",
            "Saving to: ‘simple-examples.tgz’\n",
            "\n",
            "simple-examples.tgz 100%[===================>]  33.25M  3.84MB/s    in 9.4s    \n",
            "\n",
            "2018-05-07 09:07:13 (3.56 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qE25Vqztiv3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1453
        },
        "outputId": "1ffad125-c20c-4e08-d558-e1bb3d381aa8"
      },
      "cell_type": "code",
      "source": [
        "!tar xvf simple-examples.tgz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./\r\n",
            "./simple-examples/\r\n",
            "./simple-examples/data/\r\n",
            "./simple-examples/data/ptb.test.txt\r\n",
            "./simple-examples/data/ptb.train.txt\n",
            "./simple-examples/data/ptb.valid.txt\n",
            "./simple-examples/data/README\n",
            "./simple-examples/data/ptb.char.train.txt\n",
            "./simple-examples/data/ptb.char.test.txt\n",
            "./simple-examples/data/ptb.char.valid.txt\n",
            "./simple-examples/models/\n",
            "./simple-examples/models/swb.ngram.model\n",
            "./simple-examples/models/swb.rnn.model\n",
            "./simple-examples/models/README\n",
            "./simple-examples/rnnlm-0.2b/\n",
            "./simple-examples/rnnlm-0.2b/CHANGE.log\n",
            "./simple-examples/rnnlm-0.2b/FAQ.txt\n",
            "./simple-examples/rnnlm-0.2b/convert.c\n",
            "./simple-examples/rnnlm-0.2b/makefile\n",
            "./simple-examples/rnnlm-0.2b/rnnlm.cpp\n",
            "./simple-examples/rnnlm-0.2b/rnnlmlib.cpp\n",
            "./simple-examples/rnnlm-0.2b/rnnlmlib.h\n",
            "./simple-examples/rnnlm-0.2b/prob.c\n",
            "./simple-examples/rnnlm-0.2b/test\n",
            "./simple-examples/rnnlm-0.2b/train\n",
            "./simple-examples/rnnlm-0.2b/valid\n",
            "./simple-examples/rnnlm-0.2b/example.sh\n",
            "./simple-examples/rnnlm-0.2b/example.output\n",
            "./simple-examples/rnnlm-0.2b/COPYRIGHT.txt\n",
            "./simple-examples/1-train/\n",
            "./simple-examples/1-train/train.sh\n",
            "./simple-examples/1-train/test.sh\n",
            "./simple-examples/1-train/README\n",
            "./simple-examples/3-combination/\n",
            "./simple-examples/3-combination/train.sh\n",
            "./simple-examples/3-combination/test.sh\n",
            "./simple-examples/3-combination/README\n",
            "./simple-examples/2-nbest-rescore/\n",
            "./simple-examples/2-nbest-rescore/lattices/\n",
            "./simple-examples/2-nbest-rescore/lattices/AMI-3E0501_u3005_127040_127488.lat.gz\n",
            "./simple-examples/2-nbest-rescore/lattices/AMI-3E0501_u3005_127513_127835.lat.gz\n",
            "./simple-examples/2-nbest-rescore/lattices/AMI-3E0501_u3005_127865_128175.lat.gz\n",
            "./simple-examples/2-nbest-rescore/lattices/AMI-3E0501_u3005_128188_128447.lat.gz\n",
            "./simple-examples/2-nbest-rescore/lattices/AMI-3E0501_u3005_128490_129032.lat.gz\n",
            "./simple-examples/2-nbest-rescore/lattices/nbest.sh\n",
            "./simple-examples/2-nbest-rescore/lattices/nbest/\n",
            "./simple-examples/2-nbest-rescore/lattices/latlist\n",
            "./simple-examples/2-nbest-rescore/README\n",
            "./simple-examples/2-nbest-rescore/getbest.c\n",
            "./simple-examples/2-nbest-rescore/gettext.c\n",
            "./simple-examples/2-nbest-rescore/makenbest.c\n",
            "./simple-examples/2-nbest-rescore/makenbest\n",
            "./simple-examples/2-nbest-rescore/gettext\n",
            "./simple-examples/2-nbest-rescore/getbest\n",
            "./simple-examples/5-one-iter/\n",
            "./simple-examples/5-one-iter/test.sh\n",
            "./simple-examples/5-one-iter/train.sh\n",
            "./simple-examples/5-one-iter/README\n",
            "./simple-examples/6-recovery-during-training/\n",
            "./simple-examples/6-recovery-during-training/test.sh\n",
            "./simple-examples/6-recovery-during-training/train.sh\n",
            "./simple-examples/6-recovery-during-training/README\n",
            "./simple-examples/7-dynamic-evaluation/\n",
            "./simple-examples/7-dynamic-evaluation/test.sh\n",
            "./simple-examples/7-dynamic-evaluation/train.sh\n",
            "./simple-examples/7-dynamic-evaluation/README\n",
            "./simple-examples/temp/\n",
            "./simple-examples/8-direct/\n",
            "./simple-examples/8-direct/train.sh\n",
            "./simple-examples/8-direct/test.sh\n",
            "./simple-examples/8-direct/README\n",
            "./simple-examples/4-data-generation/\n",
            "./simple-examples/4-data-generation/train.sh\n",
            "./simple-examples/4-data-generation/test.sh\n",
            "./simple-examples/4-data-generation/README\n",
            "./simple-examples/9-char-based-lm/\n",
            "./simple-examples/9-char-based-lm/test.sh\n",
            "./simple-examples/9-char-based-lm/train.sh\n",
            "./simple-examples/9-char-based-lm/README\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GCWDBC_Jjl4o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#处理输入数据的类\n",
        "class PTBInput(object):\n",
        "  \n",
        "  def __init__(self, config, data, name=None):\n",
        "    self.batch_size = batch_size = config.batch_size\n",
        "    self.num_steps = num_steps = config.num_steps\n",
        "    self.epoch_size = ((len(data)//batch_size)-1) // num_steps\n",
        "    \n",
        "    self.input_data, self.targets = reader.ptb_producer(data, batch_size, num_steps, name=name)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YmVw2ZnFkKvM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义语言模型的类\n",
        "class PTBModel(object):\n",
        "  def __init__(self, is_training, config, input_):\n",
        "    self._input = input_\n",
        "    \n",
        "    batch_size = input_.batch_size\n",
        "    num_steps = input_.num_steps\n",
        "    size = config.hidden_size\n",
        "    vocab_size = config.vocab_size\n",
        "    \n",
        "    def lstm_cell():\n",
        "      return tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\n",
        "  \n",
        "    attn_cell = lstm_cell\n",
        "    if is_training and config.keep_prob < 1:\n",
        "      def attn_cell():\n",
        "        return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=config.keep_prob)\n",
        "    \n",
        "    #使用RNN的堆叠函数将前面狗找到额lstm_cell多层堆叠得到cell，堆叠次数为config中的num_layers\n",
        "    cell = tf.contrib.rnn.MultiRNNCell(\n",
        "        [attn_cell() for _ in range(config.num_layers)],\n",
        "        state_is_tuple=True\n",
        "    )\n",
        "  \n",
        "    self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "    \n",
        "    #创建网络的词嵌入embedding部分，embedding即为将one-hot的编码格式的单词转化为向量表达形式。\n",
        "    with tf.device('/cpu:0'):\n",
        "      embedding = tf.get_variable('embedding',[vocab_size, size], dtype=tf.float32)\n",
        "      inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
        "      \n",
        "    if is_training and config.keep_prob<1:\n",
        "      inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
        "    \n",
        "    #定义输出\n",
        "    outputs = []\n",
        "    state = self._initial_state\n",
        "    #使用variable_scope将接下来的操作的名称设为RNN\n",
        "    with tf.variable_scope('RNN'):\n",
        "      for time_step in range(num_steps):\n",
        "        if time_step>0: tf.get_variable_scope().reuse_variables()\n",
        "        (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
        "        outputs.append(cell_output)\n",
        "    \n",
        "    #将output的内容用tf.concat串接起来，用reshape将其转为一个很长的一维向量。\n",
        "    output = tf.reshape(tf.concat(outputs,1),[-1,size])\n",
        "    #softmax层，先定义权重softmax_w和偏置softmax_b，然后使用tf.matmul将输出output乘上权重并加上偏置得到logits，即网络最后的输出。\n",
        "    softmax_w = tf.get_variable('softmax_w', [size, vocab_size], dtype=tf.float32)\n",
        "    softmax_b = tf.get_variable('softmax_b', [vocab_size], dtype=tf.float32)\n",
        "    \n",
        "    #得到网络的最后输出\n",
        "    logits = tf.matmul(output, softmax_w)+softmax_b\n",
        "    \n",
        "    #定义loss\n",
        "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
        "        [logits],\n",
        "        [tf.reshape(input_.targets,[-1])],\n",
        "        [tf.ones([batch_size*num_steps], dtype=tf.float32)]\n",
        "    )\n",
        "    self._cost = cost = tf.reduce_sum(loss)/batch_size\n",
        "    self._final_state = state\n",
        "    \n",
        "    if not is_training:\n",
        "      return\n",
        "    \n",
        "    #定义学习速率的变量_lr,并将其设为不可训练\n",
        "    self._lr = tf.Variable(0.0, trainable=False)\n",
        "    tvars = tf.trainable_variables()\n",
        "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),config.max_grad_norm)\n",
        "    \n",
        "    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
        "    self._train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
        "                                              global_step=tf.contrib.framework.get_or_create_global_step())\n",
        "    \n",
        "    #设置_new_lr用以控制学习速率，同时定义操作_lr_update\n",
        "    self._new_lr = tf.placeholder(tf.float32, shape=[], name='new_learning_rate')\n",
        "    \n",
        "    #定义_lr_update，使用tf.assign将_new_lr的值赋给当前的学习速率_lr\n",
        "    self._lr_update = tf.assign(self._lr, self._new_lr)\n",
        "    \n",
        "  #定义assign_lr的函数，用来在外部控制模型的学习速率，方式是将学习速率值传入_new_lr这个placeholder,并执行update_lr操作完成对学习速率的修改\n",
        "  def assign_lr(self, session, lr_value):\n",
        "    session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
        "  \n",
        "  \n",
        "  #定义PTBModel类的一些property，python中的@property装饰器可以将返回变量设为只读，防止修改变量引发的问题\n",
        "  @property\n",
        "  def input(self):\n",
        "    return self._input\n",
        "  \n",
        "  @property\n",
        "  def initial_state(self):\n",
        "    return self._initial_state\n",
        "  \n",
        "  @property\n",
        "  def cost(self):\n",
        "    return self._cost\n",
        "  \n",
        "  @property\n",
        "  def final_state(self):\n",
        "    return self._final_state\n",
        "  \n",
        "  @property\n",
        "  def lr(self):\n",
        "    return self._lr\n",
        "  \n",
        "  @property\n",
        "  def train_op(self):\n",
        "    return self._train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VM-6o4xxthcj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义几种不同大小的模型的参数\n",
        "#首先是小模型的设置\n",
        "class SmallConfig(object):\n",
        "  init_scale = 0.1 #网络中权重值的初始scale\n",
        "  learning_rate = 1.0 #学习速率的初始值\n",
        "  max_grad_norm = 5 #梯度的最大范数\n",
        "  num_layers = 2 #LSTM可以堆叠的层数\n",
        "  num_steps = 20 #LSTM梯度反向传播的展开步数\n",
        "  hidden_size = 200 #LSTM内隐含节点数\n",
        "  max_epoch = 4 #初始学习速率可训练的epoch数\n",
        "  max_max_epoch = 13 #总共可训练的epoch数\n",
        "  keep_prob = 1.0 #dropout层保留节点的比例\n",
        "  lr_decay = 0.5 #学习速率衰减速度\n",
        "  batch_size = 20#每个batch中样本的数量\n",
        "  vocab_size = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9umEIDCCv9Mc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MediumConfig中型模型\n",
        "class MediumConfig(object):\n",
        "  init_scale = 0.05 #减小了init_state,即希望权重初值不要过大，小一些有利于温和的训练\n",
        "  learning_rate = 1.0\n",
        "  max_grad_norm = 5\n",
        "  num_layers = 2\n",
        "  num_steps = 35 #将梯度反向传播的展开步数从20提升到35\n",
        "  hidden_size = 650 #增大约3倍\n",
        "  max_epoch = 6\n",
        "  max_max_epoch = 39 #增大到3倍\n",
        "  keep_prob = 0.5 #设置为0.5\n",
        "  lr_decay = 0.8 #衰减速率增大\n",
        "  batch_size = 20\n",
        "  vocab_size = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vz7jvzRMxYrb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#LargeConfig大型模型\n",
        "class LargeConfig(object):\n",
        "  init_scale = 0.04 #进一步缩小了init_scale\n",
        "  learning_rate = 1.0\n",
        "  max_grad_norm = 10\n",
        "  num_layers = 2\n",
        "  num_steps = 35\n",
        "  hidden_size = 1500\n",
        "  max_epoch = 14\n",
        "  max_max_epoch = 55\n",
        "  heep_prob = 0.35\n",
        "  lr_decay = 1/1.15\n",
        "  batch_size = 20\n",
        "  vocab_size = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dolPp53Xx2wk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#测试用，参数都尽量使用最小值\n",
        "class TestConfig(object):\n",
        "  init_scale = 0.1\n",
        "  learning_rate = 1.0\n",
        "  max_grad_norm = 1\n",
        "  num_layers = 1\n",
        "  num_steps = 2\n",
        "  hidden_size = 2\n",
        "  max_epoch = 1\n",
        "  max_max_epoch = 1\n",
        "  keep_prob = 1.0\n",
        "  lr_decay = 0.5\n",
        "  batch_size = 20\n",
        "  vocab_size =10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qVJxPKPGylAQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_epoch(session, model, eval_op=None, verbose=False):\n",
        "  start_time = time.time()\n",
        "  costs = 0.0\n",
        "  iters = 0\n",
        "  state = session.run(model.initial_state)\n",
        "  \n",
        "  fetches = {\n",
        "      'cost':model.cost,\n",
        "      'final_state':model.final_state,\n",
        "  }\n",
        "  \n",
        "  if eval_op is not None:\n",
        "    fetches['eval_op'] = eval_op\n",
        "    \n",
        "  for step in range(model.input.epoch_size):\n",
        "    feed_dict = {}\n",
        "    for i, (c,h) in enumerate(model.initial_state):\n",
        "      feed_dict[c] = state[i].c\n",
        "      feed_dict[h] = state[i].h\n",
        "      \n",
        "    vals = session.run(fetches, feed_dict)\n",
        "    cost = vals['cost']\n",
        "    state = vals['final_state']\n",
        "    \n",
        "    costs += cost\n",
        "    iters += model.input.num_steps\n",
        "    \n",
        "    if verbose and step % (model.input.epoch_size // 10) == 10:\n",
        "      print('%.3f perplexity: %.3f speed: %.0f wps' %\n",
        "            (step*1.0/model.input.epoch_size, np.exp(costs/iters),\n",
        "            iters*model.input.batch_size/(time.time() - start_time))\n",
        "           )\n",
        "      \n",
        "  return np.exp(costs/iters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ysdf30NWz3zW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_data = reader.ptb_raw_data('simple-examples/data/')\n",
        "train_data, valid_data, test_data, _ = raw_data\n",
        "\n",
        "config = SmallConfig()\n",
        "eval_config = SmallConfig()\n",
        "eval_config.batch_size = 1\n",
        "eval_config.num_steps = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXgnhfnz0KWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3199
        },
        "outputId": "6405dd8a-9303-47d2-e436-78c54e6af24c"
      },
      "cell_type": "code",
      "source": [
        "#创建默认的graph\n",
        "with tf.Graph().as_default():\n",
        "  initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
        "  \n",
        "  with tf.name_scope('Train'):\n",
        "    train_input = PTBInput(config=config, data=train_data, name='TrainInput')\n",
        "    with tf.variable_scope('Model', reuse=None, initializer=initializer):\n",
        "      m=PTBModel(is_training=True, config=config, input_=train_input)\n",
        "      \n",
        "  with tf.name_scope('Valid'):\n",
        "    valid_input = PTBInput(config=config, data=valid_data, name='ValidInput')\n",
        "    \n",
        "    with tf.variable_scope('Model', reuse=True, initializer=initializer):\n",
        "      mvalid = PTBModel(is_training=False, config=config,input_=valid_input)\n",
        "      \n",
        "  with tf.name_scope('Test'):\n",
        "    test_input = PTBInput(config=eval_config, data=test_data, name='TestInput')\n",
        "    with tf.variable_scope('Model', reuse=True, initializer=initializer):\n",
        "      mtest = PTBModel(is_training=False, config=eval_config, input_=test_input)\n",
        "      \n",
        "  #创建训练的管理器\n",
        "  sv = tf.train.Supervisor()\n",
        "  with sv.managed_session() as session: #创建默认的session\n",
        "    for i in range(config.max_max_epoch):\n",
        "      lr_decay = config.lr_decay ** max(i+1-config.max_max_epoch, 0.0)\n",
        "      m.assign_lr(session, config.learning_rate * lr_decay)\n",
        "      \n",
        "      print('Epoch: %d Learning rate: %.3f' % (i+1, session.run(m.lr)))\n",
        "      train_perplexity = run_epoch(session, m, eval_op=m.train_op,verbose=True)\n",
        "      print('Epoch: %d Train Perplexity: %.3f' %(i+1, train_perplexity))\n",
        "      \n",
        "      valid_perplexity = run_epoch(session, mvalid)\n",
        "      print('Epoch: %d Valid Perplexity: %.3f' % (i+1, valid_perplexity))\n",
        "      \n",
        "    test_perplexity = run_epoch(session, mtest)\n",
        "    print('Test Perplexity: %.3f' % test_perplexity)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch: 1 Learning rate: 1.000\n",
            "0.004 perplexity: 5371.080 speed: 3878 wps\n",
            "0.104 perplexity: 827.934 speed: 6462 wps\n",
            "0.204 perplexity: 614.244 speed: 6540 wps\n",
            "0.304 perplexity: 497.417 speed: 6561 wps\n",
            "0.404 perplexity: 429.304 speed: 6584 wps\n",
            "0.504 perplexity: 384.868 speed: 6592 wps\n",
            "0.604 perplexity: 346.654 speed: 6604 wps\n",
            "0.703 perplexity: 320.583 speed: 6606 wps\n",
            "0.803 perplexity: 299.945 speed: 6614 wps\n",
            "0.903 perplexity: 280.891 speed: 6616 wps\n",
            "Epoch: 1 Train Perplexity: 266.654\n",
            "Epoch: 1 Valid Perplexity: 179.733\n",
            "Epoch: 2 Learning rate: 1.000\n",
            "0.004 perplexity: 209.546 speed: 6704 wps\n",
            "0.104 perplexity: 149.792 speed: 6659 wps\n",
            "0.204 perplexity: 157.204 speed: 6642 wps\n",
            "0.304 perplexity: 152.209 speed: 6629 wps\n",
            "0.404 perplexity: 149.319 speed: 6624 wps\n",
            "0.504 perplexity: 146.989 speed: 6624 wps\n",
            "0.604 perplexity: 142.326 speed: 6626 wps\n",
            "0.703 perplexity: 140.213 speed: 6628 wps\n",
            "0.803 perplexity: 138.266 speed: 6631 wps\n",
            "0.903 perplexity: 134.720 speed: 6631 wps\n",
            "Epoch: 2 Train Perplexity: 132.684\n",
            "Epoch: 2 Valid Perplexity: 142.535\n",
            "Epoch: 3 Learning rate: 1.000\n",
            "0.004 perplexity: 145.853 speed: 6734 wps\n",
            "0.104 perplexity: 104.654 speed: 6626 wps\n",
            "0.204 perplexity: 113.898 speed: 6632 wps\n",
            "0.304 perplexity: 110.975 speed: 6621 wps\n",
            "0.404 perplexity: 110.139 speed: 6618 wps\n",
            "0.504 perplexity: 109.407 speed: 6615 wps\n",
            "0.604 perplexity: 106.753 speed: 6619 wps\n",
            "0.703 perplexity: 106.139 speed: 6626 wps\n",
            "0.803 perplexity: 105.502 speed: 6626 wps\n",
            "0.903 perplexity: 103.314 speed: 6625 wps\n",
            "Epoch: 3 Train Perplexity: 102.354\n",
            "Epoch: 3 Valid Perplexity: 131.917\n",
            "Epoch: 4 Learning rate: 1.000\n",
            "0.004 perplexity: 117.349 speed: 6536 wps\n",
            "0.104 perplexity: 85.281 speed: 6613 wps\n",
            "0.204 perplexity: 93.789 speed: 6621 wps\n",
            "0.304 perplexity: 91.591 speed: 6645 wps\n",
            "0.404 perplexity: 91.206 speed: 6636 wps\n",
            "0.504 perplexity: 90.893 speed: 6645 wps\n",
            "0.604 perplexity: 89.008 speed: 6644 wps\n",
            "0.703 perplexity: 88.784 speed: 6640 wps\n",
            "0.803 perplexity: 88.538 speed: 6641 wps\n",
            "0.903 perplexity: 86.897 speed: 6641 wps\n",
            "Epoch: 4 Train Perplexity: 86.360\n",
            "Epoch: 4 Valid Perplexity: 127.256\n",
            "Epoch: 5 Learning rate: 1.000\n",
            "0.004 perplexity: 100.438 speed: 6843 wps\n",
            "0.104 perplexity: 74.003 speed: 6646 wps\n",
            "0.204 perplexity: 81.466 speed: 6646 wps\n",
            "0.304 perplexity: 79.605 speed: 6648 wps\n",
            "0.404 perplexity: 79.500 speed: 6643 wps\n",
            "0.504 perplexity: 79.368 speed: 6638 wps\n",
            "0.604 perplexity: 77.906 speed: 6650 wps\n",
            "0.703 perplexity: 77.913 speed: 6650 wps\n",
            "0.803 perplexity: 77.862 speed: 6650 wps\n",
            "0.903 perplexity: 76.598 speed: 6648 wps\n",
            "Epoch: 5 Train Perplexity: 76.272\n",
            "Epoch: 5 Valid Perplexity: 127.719\n",
            "Epoch: 6 Learning rate: 1.000\n",
            "0.004 perplexity: 88.963 speed: 6816 wps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0.104 perplexity: 66.818 speed: 6699 wps\n",
            "0.204 perplexity: 73.488 speed: 6678 wps\n",
            "0.304 perplexity: 71.889 speed: 6655 wps\n",
            "0.404 perplexity: 71.932 speed: 6658 wps\n",
            "0.504 perplexity: 71.889 speed: 6650 wps\n",
            "0.604 perplexity: 70.644 speed: 6649 wps\n",
            "0.703 perplexity: 70.742 speed: 6649 wps\n",
            "0.803 perplexity: 70.799 speed: 6645 wps\n",
            "0.903 perplexity: 69.746 speed: 6645 wps\n",
            "Epoch: 6 Train Perplexity: 69.497\n",
            "Epoch: 6 Valid Perplexity: 128.984\n",
            "Epoch: 7 Learning rate: 1.000\n",
            "0.004 perplexity: 82.229 speed: 6665 wps\n",
            "0.104 perplexity: 61.542 speed: 6669 wps\n",
            "0.204 perplexity: 67.640 speed: 6658 wps\n",
            "0.304 perplexity: 66.170 speed: 6637 wps\n",
            "0.404 perplexity: 66.273 speed: 6643 wps\n",
            "0.504 perplexity: 66.369 speed: 6647 wps\n",
            "0.604 perplexity: 65.325 speed: 6646 wps\n",
            "0.703 perplexity: 65.483 speed: 6639 wps\n",
            "0.803 perplexity: 65.595 speed: 6638 wps\n",
            "0.903 perplexity: 64.652 speed: 6637 wps\n",
            "Epoch: 7 Train Perplexity: 64.517\n",
            "Epoch: 7 Valid Perplexity: 129.086\n",
            "Epoch: 8 Learning rate: 1.000\n",
            "0.004 perplexity: 76.910 speed: 6689 wps\n",
            "0.104 perplexity: 57.768 speed: 6636 wps\n",
            "0.204 perplexity: 63.463 speed: 6660 wps\n",
            "0.304 perplexity: 62.119 speed: 6667 wps\n",
            "0.404 perplexity: 62.217 speed: 6667 wps\n",
            "0.504 perplexity: 62.311 speed: 6654 wps\n",
            "0.604 perplexity: 61.389 speed: 6657 wps\n",
            "0.703 perplexity: 61.618 speed: 6647 wps\n",
            "0.803 perplexity: 61.762 speed: 6646 wps\n",
            "0.903 perplexity: 60.899 speed: 6650 wps\n",
            "Epoch: 8 Train Perplexity: 60.783\n",
            "Epoch: 8 Valid Perplexity: 131.235\n",
            "Epoch: 9 Learning rate: 1.000\n",
            "0.004 perplexity: 72.627 speed: 6402 wps\n",
            "0.104 perplexity: 54.837 speed: 6600 wps\n",
            "0.204 perplexity: 60.244 speed: 6652 wps\n",
            "0.304 perplexity: 58.993 speed: 6657 wps\n",
            "0.404 perplexity: 59.170 speed: 6642 wps\n",
            "0.504 perplexity: 59.350 speed: 6639 wps\n",
            "0.604 perplexity: 58.503 speed: 6644 wps\n",
            "0.703 perplexity: 58.682 speed: 6646 wps\n",
            "0.803 perplexity: 58.829 speed: 6640 wps\n",
            "0.903 perplexity: 58.029 speed: 6641 wps\n",
            "Epoch: 9 Train Perplexity: 57.939\n",
            "Epoch: 9 Valid Perplexity: 133.448\n",
            "Epoch: 10 Learning rate: 1.000\n",
            "0.004 perplexity: 69.111 speed: 6365 wps\n",
            "0.104 perplexity: 52.273 speed: 6618 wps\n",
            "0.204 perplexity: 57.416 speed: 6643 wps\n",
            "0.304 perplexity: 56.271 speed: 6628 wps\n",
            "0.404 perplexity: 56.433 speed: 6630 wps\n",
            "0.504 perplexity: 56.617 speed: 6624 wps\n",
            "0.604 perplexity: 55.850 speed: 6628 wps\n",
            "0.703 perplexity: 56.039 speed: 6629 wps\n",
            "0.803 perplexity: 56.177 speed: 6627 wps\n",
            "0.903 perplexity: 55.491 speed: 6627 wps\n",
            "Epoch: 10 Train Perplexity: 55.424\n",
            "Epoch: 10 Valid Perplexity: 135.516\n",
            "Epoch: 11 Learning rate: 1.000\n",
            "0.004 perplexity: 67.377 speed: 6216 wps\n",
            "0.104 perplexity: 50.390 speed: 6608 wps\n",
            "0.204 perplexity: 55.295 speed: 6631 wps\n",
            "0.304 perplexity: 54.137 speed: 6656 wps\n",
            "0.404 perplexity: 54.235 speed: 6652 wps\n",
            "0.504 perplexity: 54.403 speed: 6652 wps\n",
            "0.604 perplexity: 53.694 speed: 6657 wps\n",
            "0.703 perplexity: 53.902 speed: 6659 wps\n",
            "0.803 perplexity: 54.018 speed: 6658 wps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0.903 perplexity: 53.374 speed: 6654 wps\n",
            "Epoch: 11 Train Perplexity: 53.317\n",
            "Epoch: 11 Valid Perplexity: 135.591\n",
            "Epoch: 12 Learning rate: 1.000\n",
            "0.004 perplexity: 63.289 speed: 6464 wps\n",
            "0.104 perplexity: 48.463 speed: 6604 wps\n",
            "0.204 perplexity: 53.098 speed: 6634 wps\n",
            "0.304 perplexity: 52.186 speed: 6631 wps\n",
            "0.404 perplexity: 52.317 speed: 6633 wps\n",
            "0.504 perplexity: 52.498 speed: 6632 wps\n",
            "0.604 perplexity: 51.822 speed: 6624 wps\n",
            "0.703 perplexity: 52.033 speed: 6622 wps\n",
            "0.803 perplexity: 52.179 speed: 6623 wps\n",
            "0.903 perplexity: 51.532 speed: 6615 wps\n",
            "Epoch: 12 Train Perplexity: 51.503\n",
            "Epoch: 12 Valid Perplexity: 138.726\n",
            "Epoch: 13 Learning rate: 1.000\n",
            "0.004 perplexity: 61.245 speed: 6633 wps\n",
            "0.104 perplexity: 47.054 speed: 6601 wps\n",
            "0.204 perplexity: 51.421 speed: 6611 wps\n",
            "0.304 perplexity: 50.498 speed: 6601 wps\n",
            "0.404 perplexity: 50.607 speed: 6615 wps\n",
            "0.504 perplexity: 50.792 speed: 6616 wps\n",
            "0.604 perplexity: 50.161 speed: 6615 wps\n",
            "0.703 perplexity: 50.449 speed: 6616 wps\n",
            "0.803 perplexity: 50.635 speed: 6619 wps\n",
            "0.903 perplexity: 50.039 speed: 6616 wps\n",
            "Epoch: 13 Train Perplexity: 50.017\n",
            "Epoch: 13 Valid Perplexity: 139.800\n",
            "Test Perplexity: 135.394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_KuWqB3W6-mr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "在本节实现了一个基于LSTM的语言模型，LSTM在处理文本等时序数据时，LSTM可以存储状态，并依靠状态对当前的输入进行处理分析和预测。RNN和LSTM赋予了神经网络记录和存储过往信息的能力，可以模仿人类的一些简单的记忆和推理功能。\n",
        "\n",
        "## 注意力机制\n",
        "目前，注意力机制是RNN和NLP领域研究的热点。这种机制让机器可以更好的模拟人脑的功能。在图像标题生成任务中，包含注意力机制的RNN可以对某一区域的图像进行分析，并生成对应的文字描述。\n",
        "\n",
        "可阅读论文[Show,Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "mvGlWlVO5fxf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}