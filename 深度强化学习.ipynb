{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "深度强化学习.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/JozeeLin/google-tensorflow-exercise/blob/master/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "z2ZFvF6shpmE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "强化学习(Reinforcement Learning)是机器学习的一个重要分支，主要用来解决连续决策的问题。强化学习可以在复杂的、不确定的环境中学习如何实现我们设定的目标。\n",
        "\n",
        "强化学习的应用场景非常广，几乎包括了所有需要做一系列决策的问题，比如控制机器人的电机让它执行特定任务，给商品定价或者库存管理、玩视频游戏或棋牌游戏等。\n",
        "\n",
        "强化学习也可以应用到有序列输出的问题中，因为它可以针对一系列变化的环境状态，输出一系列对应的行动。\n",
        "\n",
        "一个强化学习问题包含三个主要概念，即环境状态(Environment State)、行动(Action)和奖励(Reward),而强化学习的目标就是获得最多的累积奖励。\n",
        "\n",
        "google DeepMind结合强化学习与深度学习，提出DQN，它可以自动玩Atari2600系列的游戏，并取得了超过人类的水平。\n",
        "\n",
        "而deepmind的alphaGo结合了策略网络(Policy Network)、估值网络(value network,即DQN)与蒙特卡洛搜索树(Monte Carlo Tree Search),实现了具有超高水平的围棋对战程序。\n",
        "\n",
        "**无人驾驶**是一个非常复杂、非常困难的强化学习任务，在深度学习出现之前，几乎不可能实现。无人驾驶汽车通过各种传感器获取**环境信息**，然后通过深度学习模型中的CNN,RNN等环境信息进行处理、抽象和转化，再结合强化学习算法框架预测出最应该执行的动作，来实现自动驾驶。无人驾驶汽车每次执行的动作，都会让它到目的地的路程更短，这就是每次行动的**奖励**。\n",
        "\n",
        "深度强化学习的另一个重要应用是操控复杂的机械装置。一般情况下，我们需要给机械装置编写逻辑非常复杂的控制代码来让他们执行具体的操作。\n",
        "\n",
        "深度强化学习模型中前几层可使用卷积网络，然后可以使用卷积网络对摄像头捕获的图像进行处理和分析，让模型能“看见”环境并识别出物体位置，再通过强化学习框架，\n",
        "\n",
        "学习如何通过一系列动作来最高效的拾取物体。因此，我们可以通过深度强化学习来让模型学会自动驾驶直升飞机。\n",
        "\n",
        "同时，我们也可以使用深度强化学习自动玩游戏，用DQN可学习自动玩Flappy Bird.DQN前几层通常也是卷积层，因此具有了对游戏图像像素直接进行学习的能力。**前几层卷积可理解和识别游戏图像中的物体，后层的神经网络则对Action的期望价值进行学习，结合这两个部分，可以得到能根据游戏像素自动玩Flappy Bird的强化学习策略**。\n",
        "\n",
        "在AlphaGo中使用了快速走子(Fast Rollout)、策略网络、估值网络和蒙特卡洛搜索树等技术。\n",
        "\n",
        "Policy-Based(Policy Gradients)和Value-Based(或者Q-Learning)是强化学习中最重要的两类方法，其主要区别在于Policy-Based的方法直接预测在某个环境状态下应该采取的Action，而Value Based的方法则预测某个环境状态下所有Action的期望价值(Q值)，之后可以通过选择Q值最高的Action执行策略。\n",
        "\n",
        "## 策略网络\n",
        "所谓策略网络，即建立一个神经网络模型，它可以通过观察环境状态，直接预测出目前最应该执行的策略Policy，执行这个策略可以获得最大的期望收益。\n",
        "\n",
        "Policy Based的方法相比于Value-Based，有更好的收敛性(通常可以保证收敛到局部最优，且不会发散)，同时对高维或者连续值得Action非常高效(训练和输出结果都更高效)。同时能学习出带有随机性的策略。\n",
        "\n",
        "使用Gym辅助进行策略网络的训练。Gym是OpenAI推出的开源的强化学习的环境生成工具。它的主要作用是为研究者和开发者提供一个方便的强化学习任务环境，例如文字游戏、棋类游戏、视频图像游戏等，并且让用户可以和其他人的强化学习算法进行效率、性能上的比较。\n",
        "\n",
        "## 强化学习研究的限制\n",
        "强化研究之前主要受制于两个因素：\n",
        "- 缺乏高质量的BenchMark，对于图像识别、监督学习等问题\n",
        "- 我们没有一个通用的环境标准、强化学习的相关论文很难进行横向比较，不同任务使用的环境定义、reward函数、可用的Action都会有区别，而且不同任务的难度可能差异非常大。\n",
        "\n",
        "Gym则非常好的解决了这两个问题，提供了大量的标准化的环境，可以用来公平的横向对比强化学习模型的性能。Gym的用户可以上传模型效果和训练日志到OpenAI Gym Service的接口，\n",
        "\n",
        "随后可以参与某个任务的排名，和其他研究者比较模型的效果，并分享算法的思路给其他研究者。\n",
        "\n",
        "## Gym\n",
        "**在Gym中，有两个核心的概念，一个是Environment，致我们的任务或者问题，另一个就是Agent，即我们编写的策略或算法。**Agent会将执行的Action传给Environment，\n",
        "\n",
        "Environment接收某个Action后，再将结果Observation(即环境状态)和Reward返回给Agent。Gym中提供了完整的Environment的接口，而Agent则是完全由用户编写。\n",
        "\n",
        "目前，Gym一共包含了几个大类的环境，分别是Algorithmic(算法)、Atari游戏(使用了Arcade Learning Environment)、\n",
        "\n",
        "Board Games(棋牌类游戏，其中围棋包含了9x9和19x19两种规模，目前使用的对抗程序为pachi)、Box2D(二维的物理引擎)、Classic Control(经典的控制类问题)、\n",
        "\n",
        "MuJoCo(另一个高效的物理引擎，可以实现非常细节的物理模拟，包括碰撞，可以用来控制2D或者3D的机器人执行一些任务操作)，以及Toy Text(文本类型)的任务。\n",
        "\n",
        "## 本例说明\n",
        "下面使用TensorFlow创建一个基于策略网络的Agent来解决CartPole问题。本节代码主要来自DeepRL-Agent的开源实现"
      ]
    },
    {
      "metadata": {
        "id": "W3q2ehwihi8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "f6a89c69-ee58-416e-b39d-cd1aa2bbc1ea"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/50/ed4a03d2be47ffd043be2ee514f329ce45d98a30fe2d1b9c61dea5a9d861/gym-0.10.5.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.3)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.4.16)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/cb/14/71/f4ab006b1e6ff75c2b54985c2f98d0644fffe9c1dddc670925\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.5 pyglet-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D5LScGqy1fEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3074
        },
        "outputId": "3611e4d7-2c7e-4cd6-9b4f-ceab4e9b8b93"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install python-opengl"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2\n",
            "  libdrm-radeon1 libdrm2 libelf1 libgl1-mesa-dri libgl1-mesa-glx libglapi-mesa\n",
            "  libglu1-mesa libllvm5.0 libpciaccess0 libsensors4 libtxc-dxtn-s2tc\n",
            "  libx11-xcb1 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0 libxcb-present0\n",
            "  libxcb-sync1 libxdamage1 libxfixes3 libxi6 libxshmfence1 libxxf86vm1\n",
            "Suggested packages:\n",
            "  pciutils lm-sensors python-numpy libgle3\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2\n",
            "  libdrm-radeon1 libdrm2 libelf1 libgl1-mesa-dri libgl1-mesa-glx libglapi-mesa\n",
            "  libglu1-mesa libllvm5.0 libpciaccess0 libsensors4 libtxc-dxtn-s2tc\n",
            "  libx11-xcb1 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0 libxcb-present0\n",
            "  libxcb-sync1 libxdamage1 libxfixes3 libxi6 libxshmfence1 libxxf86vm1\n",
            "  python-opengl\n",
            "0 upgraded, 28 newly installed, 0 to remove and 0 not upgraded.\n",
            "Need to get 20.7 MB of archives.\n",
            "After this operation, 194 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu artful/main amd64 libxshmfence1 amd64 1.2-1 [5,042 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu artful/main amd64 libxxf86vm1 amd64 1:1.1.4-1 [10.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu artful/main amd64 libelf1 amd64 0.170-0.1 [44.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu artful/main amd64 libdrm-common all 2.4.83-1 [4,938 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu artful/main amd64 libdrm2 amd64 2.4.83-1 [30.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu artful-updates/main amd64 libglapi-mesa amd64 17.2.8-0ubuntu0~17.10.1 [22.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu artful/main amd64 libx11-xcb1 amd64 2:1.6.4-3 [9,626 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu artful/main amd64 libxcb-dri2-0 amd64 1.12-1ubuntu1 [6,838 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu artful/main amd64 libxcb-dri3-0 amd64 1.12-1ubuntu1 [5,156 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu artful/main amd64 libxcb-glx0 amd64 1.12-1ubuntu1 [22.3 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu artful/main amd64 libxcb-present0 amd64 1.12-1ubuntu1 [5,436 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu artful/main amd64 libxcb-sync1 amd64 1.12-1ubuntu1 [8,746 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu artful/main amd64 libxdamage1 amd64 1:1.1.4-3 [6,934 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu artful/main amd64 libxfixes3 amd64 1:5.0.3-1 [10.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu artful/main amd64 libdrm-amdgpu1 amd64 2.4.83-1 [18.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu artful/main amd64 libpciaccess0 amd64 0.13.4-1ubuntu1 [17.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu artful/main amd64 libdrm-intel1 amd64 2.4.83-1 [59.7 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu artful/main amd64 libdrm-nouveau2 amd64 2.4.83-1 [16.4 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu artful/main amd64 libdrm-radeon1 amd64 2.4.83-1 [21.6 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu artful/main amd64 libllvm5.0 amd64 1:5.0-3 [13.7 MB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu artful/main amd64 libsensors4 amd64 1:3.4.0-4 [28.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu artful-updates/main amd64 libgl1-mesa-dri amd64 17.2.8-0ubuntu0~17.10.1 [5,707 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu artful-updates/main amd64 libgl1-mesa-glx amd64 17.2.8-0ubuntu0~17.10.1 [130 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu artful/main amd64 libxi6 amd64 2:1.7.9-1 [29.2 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu artful/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu artful/main amd64 libglu1-mesa amd64 9.0.0-2.1build1 [168 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu artful/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu artful/main amd64 libtxc-dxtn-s2tc amd64 1.0+git20151227-2 [48.4 kB]\n",
            "Fetched 20.7 MB in 1s (12.6 MB/s)\n",
            "Selecting previously unselected package libxshmfence1:amd64.\n",
            "(Reading database ... 18298 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libxshmfence1_1.2-1_amd64.deb ...\n",
            "Unpacking libxshmfence1:amd64 (1.2-1) ...\n",
            "Selecting previously unselected package libxxf86vm1:amd64.\n",
            "Preparing to unpack .../01-libxxf86vm1_1%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86vm1:amd64 (1:1.1.4-1) ...\n",
            "Selecting previously unselected package libelf1:amd64.\n",
            "Preparing to unpack .../02-libelf1_0.170-0.1_amd64.deb ...\n",
            "Unpacking libelf1:amd64 (0.170-0.1) ...\n",
            "Selecting previously unselected package libdrm-common.\n",
            "Preparing to unpack .../03-libdrm-common_2.4.83-1_all.deb ...\n",
            "Unpacking libdrm-common (2.4.83-1) ...\n",
            "Selecting previously unselected package libdrm2:amd64.\n",
            "Preparing to unpack .../04-libdrm2_2.4.83-1_amd64.deb ...\n",
            "Unpacking libdrm2:amd64 (2.4.83-1) ...\n",
            "Selecting previously unselected package libglapi-mesa:amd64.\n",
            "Preparing to unpack .../05-libglapi-mesa_17.2.8-0ubuntu0~17.10.1_amd64.deb ...\n",
            "Unpacking libglapi-mesa:amd64 (17.2.8-0ubuntu0~17.10.1) ...\n",
            "Selecting previously unselected package libx11-xcb1:amd64.\n",
            "Preparing to unpack .../06-libx11-xcb1_2%3a1.6.4-3_amd64.deb ...\n",
            "Unpacking libx11-xcb1:amd64 (2:1.6.4-3) ...\n",
            "Selecting previously unselected package libxcb-dri2-0:amd64.\n",
            "Preparing to unpack .../07-libxcb-dri2-0_1.12-1ubuntu1_amd64.deb ...\n",
            "Unpacking libxcb-dri2-0:amd64 (1.12-1ubuntu1) ...\n",
            "Selecting previously unselected package libxcb-dri3-0:amd64.\n",
            "Preparing to unpack .../08-libxcb-dri3-0_1.12-1ubuntu1_amd64.deb ...\n",
            "Unpacking libxcb-dri3-0:amd64 (1.12-1ubuntu1) ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package libxcb-glx0:amd64.\r\n",
            "Preparing to unpack .../09-libxcb-glx0_1.12-1ubuntu1_amd64.deb ...\r\n",
            "Unpacking libxcb-glx0:amd64 (1.12-1ubuntu1) ...\n",
            "Selecting previously unselected package libxcb-present0:amd64.\n",
            "Preparing to unpack .../10-libxcb-present0_1.12-1ubuntu1_amd64.deb ...\n",
            "Unpacking libxcb-present0:amd64 (1.12-1ubuntu1) ...\n",
            "Selecting previously unselected package libxcb-sync1:amd64.\n",
            "Preparing to unpack .../11-libxcb-sync1_1.12-1ubuntu1_amd64.deb ...\n",
            "Unpacking libxcb-sync1:amd64 (1.12-1ubuntu1) ...\n",
            "Selecting previously unselected package libxdamage1:amd64.\n",
            "Preparing to unpack .../12-libxdamage1_1%3a1.1.4-3_amd64.deb ...\n",
            "Unpacking libxdamage1:amd64 (1:1.1.4-3) ...\n",
            "Selecting previously unselected package libxfixes3:amd64.\n",
            "Preparing to unpack .../13-libxfixes3_1%3a5.0.3-1_amd64.deb ...\n",
            "Unpacking libxfixes3:amd64 (1:5.0.3-1) ...\n",
            "Selecting previously unselected package libdrm-amdgpu1:amd64.\n",
            "Preparing to unpack .../14-libdrm-amdgpu1_2.4.83-1_amd64.deb ...\n",
            "Unpacking libdrm-amdgpu1:amd64 (2.4.83-1) ...\n",
            "Selecting previously unselected package libpciaccess0:amd64.\n",
            "Preparing to unpack .../15-libpciaccess0_0.13.4-1ubuntu1_amd64.deb ...\n",
            "Unpacking libpciaccess0:amd64 (0.13.4-1ubuntu1) ...\n",
            "Selecting previously unselected package libdrm-intel1:amd64.\n",
            "Preparing to unpack .../16-libdrm-intel1_2.4.83-1_amd64.deb ...\n",
            "Unpacking libdrm-intel1:amd64 (2.4.83-1) ...\n",
            "Selecting previously unselected package libdrm-nouveau2:amd64.\n",
            "Preparing to unpack .../17-libdrm-nouveau2_2.4.83-1_amd64.deb ...\n",
            "Unpacking libdrm-nouveau2:amd64 (2.4.83-1) ...\n",
            "Selecting previously unselected package libdrm-radeon1:amd64.\n",
            "Preparing to unpack .../18-libdrm-radeon1_2.4.83-1_amd64.deb ...\n",
            "Unpacking libdrm-radeon1:amd64 (2.4.83-1) ...\n",
            "Selecting previously unselected package libllvm5.0:amd64.\n",
            "Preparing to unpack .../19-libllvm5.0_1%3a5.0-3_amd64.deb ...\n",
            "Unpacking libllvm5.0:amd64 (1:5.0-3) ...\n",
            "Selecting previously unselected package libsensors4:amd64.\n",
            "Preparing to unpack .../20-libsensors4_1%3a3.4.0-4_amd64.deb ...\n",
            "Unpacking libsensors4:amd64 (1:3.4.0-4) ...\n",
            "Selecting previously unselected package libgl1-mesa-dri:amd64.\n",
            "Preparing to unpack .../21-libgl1-mesa-dri_17.2.8-0ubuntu0~17.10.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dri:amd64 (17.2.8-0ubuntu0~17.10.1) ...\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "Preparing to unpack .../22-libgl1-mesa-glx_17.2.8-0ubuntu0~17.10.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (17.2.8-0ubuntu0~17.10.1) ...\n",
            "Selecting previously unselected package libxi6:amd64.\n",
            "Preparing to unpack .../23-libxi6_2%3a1.7.9-1_amd64.deb ...\n",
            "Unpacking libxi6:amd64 (2:1.7.9-1) ...\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "Preparing to unpack .../24-freeglut3_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../25-libglu1-mesa_9.0.0-2.1build1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.0-2.1build1) ...\n",
            "Selecting previously unselected package python-opengl.\n",
            "Preparing to unpack .../26-python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package libtxc-dxtn-s2tc:amd64.\n",
            "Preparing to unpack .../27-libtxc-dxtn-s2tc_1.0+git20151227-2_amd64.deb ...\n",
            "Unpacking libtxc-dxtn-s2tc:amd64 (1.0+git20151227-2) ...\n",
            "Setting up libxi6:amd64 (2:1.7.9-1) ...\n",
            "Setting up libxcb-present0:amd64 (1.12-1ubuntu1) ...\n",
            "Setting up libxcb-dri2-0:amd64 (1.12-1ubuntu1) ...\n",
            "Setting up libxcb-dri3-0:amd64 (1.12-1ubuntu1) ...\n",
            "Setting up libxcb-glx0:amd64 (1.12-1ubuntu1) ...\n",
            "Setting up libxdamage1:amd64 (1:1.1.4-3) ...\n",
            "Setting up libxfixes3:amd64 (1:5.0.3-1) ...\n",
            "Setting up libelf1:amd64 (0.170-0.1) ...\n",
            "Setting up libxshmfence1:amd64 (1.2-1) ...\n",
            "Setting up libllvm5.0:amd64 (1:5.0-3) ...\n",
            "Setting up libtxc-dxtn-s2tc:amd64 (1.0+git20151227-2) ...\n",
            "update-alternatives: using /usr/lib/x86_64-linux-gnu/s2tc/libtxc_dxtn.so to provide /usr/lib/x86_64-linux-gnu/libtxc_dxtn.so (libtxc-dxtn-x86_64-linux-gnu) in auto mode\n",
            "Setting up libglapi-mesa:amd64 (17.2.8-0ubuntu0~17.10.1) ...\n",
            "Setting up libdrm-common (2.4.83-1) ...\n",
            "Setting up libxcb-sync1:amd64 (1.12-1ubuntu1) ...\n",
            "Setting up libx11-xcb1:amd64 (2:1.6.4-3) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up libpciaccess0:amd64 (0.13.4-1ubuntu1) ...\n",
            "Setting up libsensors4:amd64 (1:3.4.0-4) ...\n",
            "Setting up libxxf86vm1:amd64 (1:1.1.4-1) ...\n",
            "Setting up libdrm2:amd64 (2.4.83-1) ...\n",
            "Setting up libdrm-intel1:amd64 (2.4.83-1) ...\n",
            "Setting up libdrm-radeon1:amd64 (2.4.83-1) ...\n",
            "Setting up libdrm-nouveau2:amd64 (2.4.83-1) ...\n",
            "Setting up libdrm-amdgpu1:amd64 (2.4.83-1) ...\n",
            "Setting up libgl1-mesa-dri:amd64 (17.2.8-0ubuntu0~17.10.1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (17.2.8-0ubuntu0~17.10.1) ...\n",
            "update-alternatives: using /usr/lib/x86_64-linux-gnu/mesa/ld.so.conf to provide /etc/ld.so.conf.d/x86_64-linux-gnu_GL.conf (x86_64-linux-gnu_gl_conf) in auto mode\n",
            "Setting up libglu1-mesa:amd64 (9.0.0-2.1build1) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-3) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vxi9mNnFx7Zm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe5ce65b-dc4e-4bf4-8258-757234e1aa2e"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "env = gym.make('CartPole-v0') #创建CartPole问题的环境env"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hXzBMbg3Jl_w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import itertools\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0gCa-txGyJUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "03961c0d-543d-4027-86dd-b625bf599d83"
      },
      "cell_type": "code",
      "source": [
        "#测试在CartPole环境中使用随机Action的表现，作为接下来对比的baseline。\n",
        "\n",
        "env.reset()\n",
        "random_episodes = 0\n",
        "reward_sum = 0\n",
        "while random_episodes < 10:\n",
        "  #env.render()\n",
        "  observation,reward, done, _ = env.step(np.random.randint(0,2))\n",
        "  reward_sum += reward\n",
        "  if done:\n",
        "    random_episodes += 1\n",
        "    print('Reward for this episode was:', reward_sum)\n",
        "    reward_sum = 0\n",
        "    env.reset()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward for this episode was: 16.0\n",
            "Reward for this episode was: 9.0\n",
            "Reward for this episode was: 66.0\n",
            "Reward for this episode was: 11.0\n",
            "Reward for this episode was: 24.0\n",
            "Reward for this episode was: 15.0\n",
            "Reward for this episode was: 10.0\n",
            "Reward for this episode was: 40.0\n",
            "Reward for this episode was: 31.0\n",
            "Reward for this episode was: 64.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J2Y7Pajj1NpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#我们的策略网络使用简单的带有一个隐含层MLP\n",
        "H = 50#隐含节点\n",
        "batch_size = 25\n",
        "learning_rate=1e-1#学习速率\n",
        "D = 4#环境信息observation的维度D为4\n",
        "gamma = 0.99 #gamma即Reward的discount比例设为0.99\n",
        "\n",
        "observations = tf.placeholder(tf.float32, [None, D], name='input_x')\n",
        "W1 = tf.get_variable('W1', shape=[D,H],\n",
        "                    initializer=tf.contrib.layers.xavier_initializer())\n",
        "layer1 = tf.nn.relu(tf.matmul(observations, W1))\n",
        "W2 = tf.get_variable('W2', shape=[H,1],\n",
        "                    initializer=tf.contrib.layers.xavier_initializer())\n",
        "score = tf.matmul(layer1, W2)\n",
        "probability = tf.nn.sigmoid(score)\n",
        "\n",
        "tvars = tf.trainable_variables()\n",
        "\n",
        "input_y = tf.placeholder(tf.float32, [None,1], name='input_y')#人工设置的虚拟label的placeholder——input_y\n",
        "advantages = tf.placeholder(tf.float32, name='reward_signal') #每个action的潜在价值的placeholder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HvVhfvfo7uMN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 模型的优化器使用Adam算法。我们分别设置两层神经网络参数的梯度的placeholder——W1Grad和W2Grad，并使用adam.apply_gradients定义我们更新模型参数的操作updateGrads。\n",
        "#之后计算参数的梯度，当积累到一定样本量的梯度，就传入W1Grad和W2Grad，并执行updateGrads更新模型参数。\n",
        "\n",
        "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "W1Grad = tf.placeholder(tf.float32, name='batch_grad1')\n",
        "W2Grad = tf.placeholder(tf.float32, name='batch_grad2')\n",
        "batchGrad = [W1Grad, W2Grad]\n",
        "updateGrads = adam.apply_gradients(zip(batchGrad, tvars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-3KMij9I9-dj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义函数discount_rewards,用来估算每一个Action对应的潜在价值discount_r\n",
        "def discount_rewards(r):\n",
        "  discounted_r = np.zeros_like(r)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(r.size)):\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "    \n",
        "  return discounted_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4JgZhbN7_7mm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loglik = tf.log(input_y*(input_y - probability)+\n",
        "               (1-input_y)*(input_y+probability))\n",
        "loss = -tf.reduce_mean(loglik * advantages)\n",
        "\n",
        "newGrads = tf.gradients(loss, tvars) #使用tf.gradients求解模型参数关于loss的梯度"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7IUyQeBrBzG-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xs,ys,drs = [],[],[]\n",
        "reward_sum = 0\n",
        "episode_number = 1\n",
        "total_episodes = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UNzyJrnDB9Pa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7290
        },
        "outputId": "57d2ec02-33d3-48ac-e02c-d7e27d923669"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  rendering = False\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess.run(init)\n",
        "  observation = env.reset()\n",
        "  \n",
        "  gradBuffer = sess.run(tvars)\n",
        "  for ix, grad in enumerate(gradBuffer):\n",
        "    gradBuffer[ix] = grad*0\n",
        "    \n",
        "  while episode_number <= total_episodes:\n",
        "    if reward_sum/batch_size > 100 or rendering == True:\n",
        "      #env.render()\n",
        "      rendering = True\n",
        "      \n",
        "    x = np.reshape(observation,[1,D])\n",
        "    \n",
        "    tfprob = sess.run(probability, feed_dict={observations:x})\n",
        "    action = 1 if np.random.uniform() < tfprob else 0\n",
        "    \n",
        "    xs.append(x)\n",
        "    y = 1-action\n",
        "    ys.append(y)\n",
        "    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "    \n",
        "    drs.append(reward)\n",
        "    \n",
        "    if done:\n",
        "      episode_number += 1\n",
        "      epx = np.vstack(xs)\n",
        "      epy = np.vstack(ys)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,ys,drs = [],[],[]\n",
        "      \n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "      \n",
        "      tGrad = sess.run(newGrads, feed_dict={observations:epx,\n",
        "                                           input_y:epy, advantages:discounted_epr})\n",
        "      \n",
        "      for ix, grad in enumerate(tGrad):\n",
        "        gradBuffer[ix] += grad\n",
        "        \n",
        "      if episode_number % batch_size == 0:\n",
        "        sess.run(updateGrads, feed_dict={W1Grad:gradBuffer[0],\n",
        "                                        W2Grad:gradBuffer[1]})\n",
        "        \n",
        "        for ix, grad in enumerate(gradBuffer):\n",
        "          gradBuffer[ix] = grad * 0\n",
        "          \n",
        "        print('Average reward for episode %d : %f.' % \\\n",
        "             (episode_number, reward_sum/batch_size))\n",
        "        \n",
        "        if reward_sum/batch_size>200:\n",
        "          print('Task solved in', episode_number, 'episodes!')\n",
        "          break\n",
        "          \n",
        "        reward_sum = 0\n",
        "      observation = env.reset()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average reward for episode 25 : 24.680000.\n",
            "Average reward for episode 50 : 48.440000.\n",
            "Average reward for episode 75 : 66.040000.\n",
            "Average reward for episode 100 : 73.320000.\n",
            "Average reward for episode 125 : 115.800000.\n",
            "Average reward for episode 150 : 147.120000.\n",
            "Average reward for episode 175 : 194.520000.\n",
            "Average reward for episode 200 : 199.240000.\n",
            "Average reward for episode 225 : 187.840000.\n",
            "Average reward for episode 250 : 196.840000.\n",
            "Average reward for episode 275 : 196.520000.\n",
            "Average reward for episode 300 : 200.000000.\n",
            "Average reward for episode 325 : 200.000000.\n",
            "Average reward for episode 350 : 200.000000.\n",
            "Average reward for episode 375 : 200.000000.\n",
            "Average reward for episode 400 : 200.000000.\n",
            "Average reward for episode 425 : 200.000000.\n",
            "Average reward for episode 450 : 200.000000.\n",
            "Average reward for episode 475 : 200.000000.\n",
            "Average reward for episode 500 : 200.000000.\n",
            "Average reward for episode 525 : 200.000000.\n",
            "Average reward for episode 550 : 200.000000.\n",
            "Average reward for episode 575 : 200.000000.\n",
            "Average reward for episode 600 : 200.000000.\n",
            "Average reward for episode 625 : 200.000000.\n",
            "Average reward for episode 650 : 200.000000.\n",
            "Average reward for episode 675 : 200.000000.\n",
            "Average reward for episode 700 : 200.000000.\n",
            "Average reward for episode 725 : 200.000000.\n",
            "Average reward for episode 750 : 200.000000.\n",
            "Average reward for episode 775 : 200.000000.\n",
            "Average reward for episode 800 : 200.000000.\n",
            "Average reward for episode 825 : 200.000000.\n",
            "Average reward for episode 850 : 200.000000.\n",
            "Average reward for episode 875 : 200.000000.\n",
            "Average reward for episode 900 : 200.000000.\n",
            "Average reward for episode 925 : 200.000000.\n",
            "Average reward for episode 950 : 200.000000.\n",
            "Average reward for episode 975 : 200.000000.\n",
            "Average reward for episode 1000 : 200.000000.\n",
            "Average reward for episode 1025 : 200.000000.\n",
            "Average reward for episode 1050 : 200.000000.\n",
            "Average reward for episode 1075 : 200.000000.\n",
            "Average reward for episode 1100 : 200.000000.\n",
            "Average reward for episode 1125 : 200.000000.\n",
            "Average reward for episode 1150 : 200.000000.\n",
            "Average reward for episode 1175 : 200.000000.\n",
            "Average reward for episode 1200 : 200.000000.\n",
            "Average reward for episode 1225 : 199.480000.\n",
            "Average reward for episode 1250 : 200.000000.\n",
            "Average reward for episode 1275 : 200.000000.\n",
            "Average reward for episode 1300 : 200.000000.\n",
            "Average reward for episode 1325 : 197.400000.\n",
            "Average reward for episode 1350 : 199.280000.\n",
            "Average reward for episode 1375 : 193.280000.\n",
            "Average reward for episode 1400 : 181.080000.\n",
            "Average reward for episode 1425 : 181.040000.\n",
            "Average reward for episode 1450 : 186.720000.\n",
            "Average reward for episode 1475 : 174.000000.\n",
            "Average reward for episode 1500 : 156.960000.\n",
            "Average reward for episode 1525 : 159.880000.\n",
            "Average reward for episode 1550 : 169.360000.\n",
            "Average reward for episode 1575 : 164.880000.\n",
            "Average reward for episode 1600 : 167.560000.\n",
            "Average reward for episode 1625 : 157.640000.\n",
            "Average reward for episode 1650 : 168.480000.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average reward for episode 1675 : 191.880000.\n",
            "Average reward for episode 1700 : 187.760000.\n",
            "Average reward for episode 1725 : 191.960000.\n",
            "Average reward for episode 1750 : 193.040000.\n",
            "Average reward for episode 1775 : 198.440000.\n",
            "Average reward for episode 1800 : 198.800000.\n",
            "Average reward for episode 1825 : 200.000000.\n",
            "Average reward for episode 1850 : 199.880000.\n",
            "Average reward for episode 1875 : 200.000000.\n",
            "Average reward for episode 1900 : 200.000000.\n",
            "Average reward for episode 1925 : 200.000000.\n",
            "Average reward for episode 1950 : 200.000000.\n",
            "Average reward for episode 1975 : 200.000000.\n",
            "Average reward for episode 2000 : 200.000000.\n",
            "Average reward for episode 2025 : 200.000000.\n",
            "Average reward for episode 2050 : 200.000000.\n",
            "Average reward for episode 2075 : 200.000000.\n",
            "Average reward for episode 2100 : 200.000000.\n",
            "Average reward for episode 2125 : 200.000000.\n",
            "Average reward for episode 2150 : 200.000000.\n",
            "Average reward for episode 2175 : 200.000000.\n",
            "Average reward for episode 2200 : 200.000000.\n",
            "Average reward for episode 2225 : 200.000000.\n",
            "Average reward for episode 2250 : 200.000000.\n",
            "Average reward for episode 2275 : 199.880000.\n",
            "Average reward for episode 2300 : 200.000000.\n",
            "Average reward for episode 2325 : 198.520000.\n",
            "Average reward for episode 2350 : 200.000000.\n",
            "Average reward for episode 2375 : 200.000000.\n",
            "Average reward for episode 2400 : 200.000000.\n",
            "Average reward for episode 2425 : 200.000000.\n",
            "Average reward for episode 2450 : 200.000000.\n",
            "Average reward for episode 2475 : 198.800000.\n",
            "Average reward for episode 2500 : 200.000000.\n",
            "Average reward for episode 2525 : 200.000000.\n",
            "Average reward for episode 2550 : 199.960000.\n",
            "Average reward for episode 2575 : 200.000000.\n",
            "Average reward for episode 2600 : 200.000000.\n",
            "Average reward for episode 2625 : 200.000000.\n",
            "Average reward for episode 2650 : 200.000000.\n",
            "Average reward for episode 2675 : 200.000000.\n",
            "Average reward for episode 2700 : 200.000000.\n",
            "Average reward for episode 2725 : 200.000000.\n",
            "Average reward for episode 2750 : 200.000000.\n",
            "Average reward for episode 2775 : 199.560000.\n",
            "Average reward for episode 2800 : 200.000000.\n",
            "Average reward for episode 2825 : 200.000000.\n",
            "Average reward for episode 2850 : 200.000000.\n",
            "Average reward for episode 2875 : 197.720000.\n",
            "Average reward for episode 2900 : 199.560000.\n",
            "Average reward for episode 2925 : 200.000000.\n",
            "Average reward for episode 2950 : 199.840000.\n",
            "Average reward for episode 2975 : 200.000000.\n",
            "Average reward for episode 3000 : 200.000000.\n",
            "Average reward for episode 3025 : 200.000000.\n",
            "Average reward for episode 3050 : 199.760000.\n",
            "Average reward for episode 3075 : 200.000000.\n",
            "Average reward for episode 3100 : 200.000000.\n",
            "Average reward for episode 3125 : 200.000000.\n",
            "Average reward for episode 3150 : 200.000000.\n",
            "Average reward for episode 3175 : 200.000000.\n",
            "Average reward for episode 3200 : 200.000000.\n",
            "Average reward for episode 3225 : 200.000000.\n",
            "Average reward for episode 3250 : 200.000000.\n",
            "Average reward for episode 3275 : 200.000000.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average reward for episode 3300 : 200.000000.\n",
            "Average reward for episode 3325 : 200.000000.\n",
            "Average reward for episode 3350 : 200.000000.\n",
            "Average reward for episode 3375 : 200.000000.\n",
            "Average reward for episode 3400 : 200.000000.\n",
            "Average reward for episode 3425 : 200.000000.\n",
            "Average reward for episode 3450 : 200.000000.\n",
            "Average reward for episode 3475 : 200.000000.\n",
            "Average reward for episode 3500 : 200.000000.\n",
            "Average reward for episode 3525 : 200.000000.\n",
            "Average reward for episode 3550 : 200.000000.\n",
            "Average reward for episode 3575 : 200.000000.\n",
            "Average reward for episode 3600 : 200.000000.\n",
            "Average reward for episode 3625 : 200.000000.\n",
            "Average reward for episode 3650 : 200.000000.\n",
            "Average reward for episode 3675 : 200.000000.\n",
            "Average reward for episode 3700 : 200.000000.\n",
            "Average reward for episode 3725 : 200.000000.\n",
            "Average reward for episode 3750 : 200.000000.\n",
            "Average reward for episode 3775 : 200.000000.\n",
            "Average reward for episode 3800 : 200.000000.\n",
            "Average reward for episode 3825 : 200.000000.\n",
            "Average reward for episode 3850 : 200.000000.\n",
            "Average reward for episode 3875 : 200.000000.\n",
            "Average reward for episode 3900 : 200.000000.\n",
            "Average reward for episode 3925 : 200.000000.\n",
            "Average reward for episode 3950 : 200.000000.\n",
            "Average reward for episode 3975 : 200.000000.\n",
            "Average reward for episode 4000 : 200.000000.\n",
            "Average reward for episode 4025 : 200.000000.\n",
            "Average reward for episode 4050 : 200.000000.\n",
            "Average reward for episode 4075 : 200.000000.\n",
            "Average reward for episode 4100 : 200.000000.\n",
            "Average reward for episode 4125 : 200.000000.\n",
            "Average reward for episode 4150 : 200.000000.\n",
            "Average reward for episode 4175 : 200.000000.\n",
            "Average reward for episode 4200 : 200.000000.\n",
            "Average reward for episode 4225 : 200.000000.\n",
            "Average reward for episode 4250 : 200.000000.\n",
            "Average reward for episode 4275 : 200.000000.\n",
            "Average reward for episode 4300 : 200.000000.\n",
            "Average reward for episode 4325 : 200.000000.\n",
            "Average reward for episode 4350 : 200.000000.\n",
            "Average reward for episode 4375 : 200.000000.\n",
            "Average reward for episode 4400 : 200.000000.\n",
            "Average reward for episode 4425 : 200.000000.\n",
            "Average reward for episode 4450 : 200.000000.\n",
            "Average reward for episode 4475 : 200.000000.\n",
            "Average reward for episode 4500 : 200.000000.\n",
            "Average reward for episode 4525 : 200.000000.\n",
            "Average reward for episode 4550 : 200.000000.\n",
            "Average reward for episode 4575 : 200.000000.\n",
            "Average reward for episode 4600 : 200.000000.\n",
            "Average reward for episode 4625 : 200.000000.\n",
            "Average reward for episode 4650 : 200.000000.\n",
            "Average reward for episode 4675 : 200.000000.\n",
            "Average reward for episode 4700 : 200.000000.\n",
            "Average reward for episode 4725 : 200.000000.\n",
            "Average reward for episode 4750 : 200.000000.\n",
            "Average reward for episode 4775 : 200.000000.\n",
            "Average reward for episode 4800 : 200.000000.\n",
            "Average reward for episode 4825 : 200.000000.\n",
            "Average reward for episode 4850 : 200.000000.\n",
            "Average reward for episode 4875 : 200.000000.\n",
            "Average reward for episode 4900 : 200.000000.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average reward for episode 4925 : 200.000000.\n",
            "Average reward for episode 4950 : 200.000000.\n",
            "Average reward for episode 4975 : 200.000000.\n",
            "Average reward for episode 5000 : 200.000000.\n",
            "Average reward for episode 5025 : 200.000000.\n",
            "Average reward for episode 5050 : 200.000000.\n",
            "Average reward for episode 5075 : 198.880000.\n",
            "Average reward for episode 5100 : 198.760000.\n",
            "Average reward for episode 5125 : 193.440000.\n",
            "Average reward for episode 5150 : 185.640000.\n",
            "Average reward for episode 5175 : 195.720000.\n",
            "Average reward for episode 5200 : 185.160000.\n",
            "Average reward for episode 5225 : 183.920000.\n",
            "Average reward for episode 5250 : 187.920000.\n",
            "Average reward for episode 5275 : 177.760000.\n",
            "Average reward for episode 5300 : 182.840000.\n",
            "Average reward for episode 5325 : 188.080000.\n",
            "Average reward for episode 5350 : 181.160000.\n",
            "Average reward for episode 5375 : 186.480000.\n",
            "Average reward for episode 5400 : 181.760000.\n",
            "Average reward for episode 5425 : 180.720000.\n",
            "Average reward for episode 5450 : 189.040000.\n",
            "Average reward for episode 5475 : 170.560000.\n",
            "Average reward for episode 5500 : 181.080000.\n",
            "Average reward for episode 5525 : 188.080000.\n",
            "Average reward for episode 5550 : 188.800000.\n",
            "Average reward for episode 5575 : 196.680000.\n",
            "Average reward for episode 5600 : 198.240000.\n",
            "Average reward for episode 5625 : 197.680000.\n",
            "Average reward for episode 5650 : 199.080000.\n",
            "Average reward for episode 5675 : 199.040000.\n",
            "Average reward for episode 5700 : 199.240000.\n",
            "Average reward for episode 5725 : 197.800000.\n",
            "Average reward for episode 5750 : 196.480000.\n",
            "Average reward for episode 5775 : 188.320000.\n",
            "Average reward for episode 5800 : 196.840000.\n",
            "Average reward for episode 5825 : 192.640000.\n",
            "Average reward for episode 5850 : 191.520000.\n",
            "Average reward for episode 5875 : 194.440000.\n",
            "Average reward for episode 5900 : 192.600000.\n",
            "Average reward for episode 5925 : 194.880000.\n",
            "Average reward for episode 5950 : 183.480000.\n",
            "Average reward for episode 5975 : 186.920000.\n",
            "Average reward for episode 6000 : 175.040000.\n",
            "Average reward for episode 6025 : 184.000000.\n",
            "Average reward for episode 6050 : 185.280000.\n",
            "Average reward for episode 6075 : 180.280000.\n",
            "Average reward for episode 6100 : 187.920000.\n",
            "Average reward for episode 6125 : 179.960000.\n",
            "Average reward for episode 6150 : 189.880000.\n",
            "Average reward for episode 6175 : 192.280000.\n",
            "Average reward for episode 6200 : 195.840000.\n",
            "Average reward for episode 6225 : 197.680000.\n",
            "Average reward for episode 6250 : 200.000000.\n",
            "Average reward for episode 6275 : 200.000000.\n",
            "Average reward for episode 6300 : 200.000000.\n",
            "Average reward for episode 6325 : 200.000000.\n",
            "Average reward for episode 6350 : 200.000000.\n",
            "Average reward for episode 6375 : 200.000000.\n",
            "Average reward for episode 6400 : 200.000000.\n",
            "Average reward for episode 6425 : 200.000000.\n",
            "Average reward for episode 6450 : 200.000000.\n",
            "Average reward for episode 6475 : 200.000000.\n",
            "Average reward for episode 6500 : 200.000000.\n",
            "Average reward for episode 6525 : 200.000000.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average reward for episode 6550 : 160.840000.\n",
            "Average reward for episode 6575 : 200.000000.\n",
            "Average reward for episode 6600 : 200.000000.\n",
            "Average reward for episode 6625 : 200.000000.\n",
            "Average reward for episode 6650 : 200.000000.\n",
            "Average reward for episode 6675 : 200.000000.\n",
            "Average reward for episode 6700 : 200.000000.\n",
            "Average reward for episode 6725 : 200.000000.\n",
            "Average reward for episode 6750 : 200.000000.\n",
            "Average reward for episode 6775 : 200.000000.\n",
            "Average reward for episode 6800 : 199.200000.\n",
            "Average reward for episode 6825 : 196.840000.\n",
            "Average reward for episode 6850 : 195.240000.\n",
            "Average reward for episode 6875 : 196.480000.\n",
            "Average reward for episode 6900 : 190.400000.\n",
            "Average reward for episode 6925 : 192.640000.\n",
            "Average reward for episode 6950 : 190.320000.\n",
            "Average reward for episode 6975 : 191.480000.\n",
            "Average reward for episode 7000 : 185.760000.\n",
            "Average reward for episode 7025 : 189.520000.\n",
            "Average reward for episode 7050 : 179.880000.\n",
            "Average reward for episode 7075 : 180.800000.\n",
            "Average reward for episode 7100 : 191.320000.\n",
            "Average reward for episode 7125 : 173.360000.\n",
            "Average reward for episode 7150 : 189.520000.\n",
            "Average reward for episode 7175 : 188.720000.\n",
            "Average reward for episode 7200 : 171.480000.\n",
            "Average reward for episode 7225 : 183.200000.\n",
            "Average reward for episode 7250 : 174.800000.\n",
            "Average reward for episode 7275 : 181.880000.\n",
            "Average reward for episode 7300 : 175.640000.\n",
            "Average reward for episode 7325 : 187.040000.\n",
            "Average reward for episode 7350 : 183.320000.\n",
            "Average reward for episode 7375 : 193.480000.\n",
            "Average reward for episode 7400 : 187.920000.\n",
            "Average reward for episode 7425 : 187.040000.\n",
            "Average reward for episode 7450 : 195.360000.\n",
            "Average reward for episode 7475 : 194.720000.\n",
            "Average reward for episode 7500 : 196.880000.\n",
            "Average reward for episode 7525 : 199.280000.\n",
            "Average reward for episode 7550 : 200.000000.\n",
            "Average reward for episode 7575 : 200.000000.\n",
            "Average reward for episode 7600 : 200.000000.\n",
            "Average reward for episode 7625 : 200.000000.\n",
            "Average reward for episode 7650 : 200.000000.\n",
            "Average reward for episode 7675 : 200.000000.\n",
            "Average reward for episode 7700 : 200.000000.\n",
            "Average reward for episode 7725 : 200.000000.\n",
            "Average reward for episode 7750 : 200.000000.\n",
            "Average reward for episode 7775 : 200.000000.\n",
            "Average reward for episode 7800 : 200.000000.\n",
            "Average reward for episode 7825 : 200.000000.\n",
            "Average reward for episode 7850 : 200.000000.\n",
            "Average reward for episode 7875 : 200.000000.\n",
            "Average reward for episode 7900 : 200.000000.\n",
            "Average reward for episode 7925 : 200.000000.\n",
            "Average reward for episode 7950 : 200.000000.\n",
            "Average reward for episode 7975 : 200.000000.\n",
            "Average reward for episode 8000 : 200.000000.\n",
            "Average reward for episode 8025 : 200.000000.\n",
            "Average reward for episode 8050 : 200.000000.\n",
            "Average reward for episode 8075 : 200.000000.\n",
            "Average reward for episode 8100 : 200.000000.\n",
            "Average reward for episode 8125 : 200.000000.\n",
            "Average reward for episode 8150 : 200.000000.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average reward for episode 8175 : 200.000000.\n",
            "Average reward for episode 8200 : 200.000000.\n",
            "Average reward for episode 8225 : 200.000000.\n",
            "Average reward for episode 8250 : 200.000000.\n",
            "Average reward for episode 8275 : 200.000000.\n",
            "Average reward for episode 8300 : 200.000000.\n",
            "Average reward for episode 8325 : 200.000000.\n",
            "Average reward for episode 8350 : 200.000000.\n",
            "Average reward for episode 8375 : 200.000000.\n",
            "Average reward for episode 8400 : 200.000000.\n",
            "Average reward for episode 8425 : 200.000000.\n",
            "Average reward for episode 8450 : 200.000000.\n",
            "Average reward for episode 8475 : 200.000000.\n",
            "Average reward for episode 8500 : 200.000000.\n",
            "Average reward for episode 8525 : 200.000000.\n",
            "Average reward for episode 8550 : 200.000000.\n",
            "Average reward for episode 8575 : 200.000000.\n",
            "Average reward for episode 8600 : 200.000000.\n",
            "Average reward for episode 8625 : 200.000000.\n",
            "Average reward for episode 8650 : 200.000000.\n",
            "Average reward for episode 8675 : 200.000000.\n",
            "Average reward for episode 8700 : 200.000000.\n",
            "Average reward for episode 8725 : 200.000000.\n",
            "Average reward for episode 8750 : 200.000000.\n",
            "Average reward for episode 8775 : 200.000000.\n",
            "Average reward for episode 8800 : 200.000000.\n",
            "Average reward for episode 8825 : 200.000000.\n",
            "Average reward for episode 8850 : 200.000000.\n",
            "Average reward for episode 8875 : 200.000000.\n",
            "Average reward for episode 8900 : 200.000000.\n",
            "Average reward for episode 8925 : 200.000000.\n",
            "Average reward for episode 8950 : 200.000000.\n",
            "Average reward for episode 8975 : 200.000000.\n",
            "Average reward for episode 9000 : 200.000000.\n",
            "Average reward for episode 9025 : 200.000000.\n",
            "Average reward for episode 9050 : 200.000000.\n",
            "Average reward for episode 9075 : 200.000000.\n",
            "Average reward for episode 9100 : 200.000000.\n",
            "Average reward for episode 9125 : 200.000000.\n",
            "Average reward for episode 9150 : 200.000000.\n",
            "Average reward for episode 9175 : 200.000000.\n",
            "Average reward for episode 9200 : 200.000000.\n",
            "Average reward for episode 9225 : 200.000000.\n",
            "Average reward for episode 9250 : 200.000000.\n",
            "Average reward for episode 9275 : 200.000000.\n",
            "Average reward for episode 9300 : 200.000000.\n",
            "Average reward for episode 9325 : 200.000000.\n",
            "Average reward for episode 9350 : 200.000000.\n",
            "Average reward for episode 9375 : 200.000000.\n",
            "Average reward for episode 9400 : 200.000000.\n",
            "Average reward for episode 9425 : 200.000000.\n",
            "Average reward for episode 9450 : 200.000000.\n",
            "Average reward for episode 9475 : 200.000000.\n",
            "Average reward for episode 9500 : 200.000000.\n",
            "Average reward for episode 9525 : 200.000000.\n",
            "Average reward for episode 9550 : 200.000000.\n",
            "Average reward for episode 9575 : 200.000000.\n",
            "Average reward for episode 9600 : 200.000000.\n",
            "Average reward for episode 9625 : 200.000000.\n",
            "Average reward for episode 9650 : 200.000000.\n",
            "Average reward for episode 9675 : 200.000000.\n",
            "Average reward for episode 9700 : 200.000000.\n",
            "Average reward for episode 9725 : 200.000000.\n",
            "Average reward for episode 9750 : 200.000000.\n",
            "Average reward for episode 9775 : 200.000000.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average reward for episode 9800 : 200.000000.\n",
            "Average reward for episode 9825 : 200.000000.\n",
            "Average reward for episode 9850 : 200.000000.\n",
            "Average reward for episode 9875 : 200.000000.\n",
            "Average reward for episode 9900 : 200.000000.\n",
            "Average reward for episode 9925 : 200.000000.\n",
            "Average reward for episode 9950 : 200.000000.\n",
            "Average reward for episode 9975 : 200.000000.\n",
            "Average reward for episode 10000 : 200.000000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LK3Lq8B9FGI3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 实现估值网络\n",
        "\n",
        "下面逐一介绍目前state of the art的DQN中的一些Trick\n",
        "\n",
        "### 第一个trick：引入卷积层\n",
        "不再是输入一些数值类的特征让模型学习，而是直接让模型通过Atari这类游戏的视频图像了解环境信息并学习策略。这就要求DQN具有图像识别的能力，因此引入卷积网络。\n",
        "\n",
        "### 第二个trick：Experience Replay\n",
        "因为深度学习需要大量的样本，所以传统的Q-Learning的online update的方法(逐一对新样本学习的方式)可能不太适合DQN。为了对图像进行反复利用，我们引入一种称为Experience Replay技术，\n",
        "\n",
        "它的主要思想是存储Agent的experience(即样本)，并且每次训练时随机抽取一部分样本供给网络学习。这样我们能比较稳定的完成学习任务，避免只是学习最新接触到的样本，\n",
        "\n",
        "而是综合地、反复地利用过往的大量样本进行学习。通过创建一个用来存储Experience的缓存buffer，它里面可以存储一定量的比较新的样本，来进行代码实现。让容量满了以后，\n",
        "\n",
        "会用新样本替换最旧的样本，这可以保证大部分样本有相近的概率被抽到，如果不替换旧的，那么旧样本被抽取到的概率会比新样本的高。\n",
        "\n",
        "### 第三个trick:使用第二个DQN网络来辅助训练\n",
        "辅助网络一般称为target DQN，它的意义是辅助我们计算目标Q值。之所以拆分成两个网络，一个用来制造学习目标，一个用来进行实际训练，原因很简单，是为了让Q-Learning训练的目标保持平稳。\n",
        "\n",
        "### 第四个trick：Double DQN\n",
        "参考论文[Deep Reinforcement Learning with Double Q-Learning](https://arxiv.org/pdf/1509.06461.pdf)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vIqM6u8pEX_J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#创建环境内物体对象的class\n",
        "\n",
        "class gameOb():\n",
        "  def __init__(self, coordinates, size, intensity, channel, reward, name):\n",
        "    self.x = coordinates[0]\n",
        "    self.y = coordinates[1]\n",
        "    self.size = size\n",
        "    self.intensity = intensity\n",
        "    self.channel = channel\n",
        "    self.reward = reward\n",
        "    self.name = name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egeNu5u7KTLO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#创建GridWorld环境的class\n",
        "\n",
        "class gameEnv():\n",
        "  def __init__(self, size):\n",
        "    self.sizeX = size\n",
        "    self.sizeY = size\n",
        "    self.actions = 4\n",
        "    self.objects = []\n",
        "    a = self.reset()\n",
        "    plt.imshow(a, interpolation='nearest')\n",
        "    \n",
        "  def reset(self):\n",
        "    self.objects = []\n",
        "    hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
        "    self.objects.append(hero)\n",
        "    \n",
        "    goal = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
        "    self.objects.append(goal)\n",
        "    hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
        "    self.objects.append(hole)\n",
        "    \n",
        "    goal2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
        "    self.objects.append(goal2)\n",
        "    hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
        "    self.objects.append(hole2)\n",
        "    \n",
        "    goal3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
        "    self.objects.append(goal3)\n",
        "    \n",
        "    goal4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
        "    self.objects.append(goal4)\n",
        "    \n",
        "    state = self.renderEnv()\n",
        "    \n",
        "    self.state = state\n",
        "    \n",
        "    return state\n",
        "  \n",
        "  #实现移动英雄角色的方法\n",
        "  def moveChar(self,direction):\n",
        "    hero = self.objects[0]\n",
        "    heroX = hero.x\n",
        "    heroY = hero.y\n",
        "    if direction == 0 and hero.y >= 1:\n",
        "      hero.y -= 1\n",
        "      \n",
        "    if direction==1 and hero.y<=self.sizeY-2:\n",
        "      hero.y += 1\n",
        "      \n",
        "    if direction==2 and hero.x>=1:\n",
        "      hero.x -= 1\n",
        "      \n",
        "    if direction==3 and hero.x<=self.sizeX-2:\n",
        "      hero.x += 1\n",
        "      \n",
        "    self.objects[0] = hero\n",
        "    \n",
        "    \n",
        "  def newPosition(self):\n",
        "    iterables = [range(self.sizeX), range(self.sizeY)]\n",
        "    points = []\n",
        "    for t in itertools.product(*iterables):\n",
        "      points.append(t)\n",
        "      \n",
        "    currentPositions = []\n",
        "    \n",
        "    for objectA in self.objects:\n",
        "      if (objectA.x,objectA.y) not in currentPositions:\n",
        "        currentPositions.append((objectA.x,objectA.y))\n",
        "        \n",
        "    for pos in currentPositions:\n",
        "      points.remove(pos)\n",
        "      \n",
        "    location = np.random.choice(range(len(points)), replace=False)\n",
        "    \n",
        "    return points[location]\n",
        "  \n",
        "  def checkGoal(self):\n",
        "    others = []\n",
        "    for obj in self.objects:\n",
        "      if obj.name == 'hero':\n",
        "        hero = obj\n",
        "      else:\n",
        "        others.append(obj)\n",
        "        \n",
        "    for other in others:\n",
        "      if hero.x == other.x and hero.y == other.y:\n",
        "        self.objects.remove(other)\n",
        "        if other.reward == 1:\n",
        "          self.objects.append(gameOb(self.newPosition(), 1,1,1,1,'goal'))\n",
        "          \n",
        "        else:\n",
        "          self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n",
        "          \n",
        "        return other.reward,False\n",
        "    return 0.0, False\n",
        "  \n",
        "  def renderEnv(self):\n",
        "    a = np.ones([self.sizeY+2, self.sizeX+2,3])\n",
        "    a[1:-1,1:-1,:] = 0\n",
        "    \n",
        "    hero = None\n",
        "    for item in self.objects:\n",
        "      a[item.y+1:item.y+item.size+1, item.x+1:item.x+item.size+1, item.channel] = item.intensity\n",
        "      \n",
        "    b = scipy.misc.imresize(a[:,:,0],[84,84,1],interp='nearest')\n",
        "    c = scipy.misc.imresize(a[:,:,1],[84,84,1],interp='nearest')\n",
        "    d = scipy.misc.imresize(a[:,:,2],[84,84,1],interp='nearest')\n",
        "    a = np.stack([b,c,d],axis=2)\n",
        "    return a\n",
        "  \n",
        "  def step(self, action):\n",
        "    self.moveChar(action)\n",
        "    reward,done = self.checkGoal()\n",
        "    state = self.renderEnv()\n",
        "    return state, reward, done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QN_teRcQDVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "8b6f8658-e47d-4b08-cabf-d165bd24f7b7"
      },
      "cell_type": "code",
      "source": [
        "env = gameEnv(size=5)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if issubdtype(ts, int):\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  elif issubdtype(type(size), float):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8961d72b00>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vWvHujSnQFXN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Qnetwork():\n",
        "  def __init__(self, h_size):\n",
        "    self.scalarInput = tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
        "    self.imageIn = tf.reshape(self.scalarInput, shape=[-1,84,84,3])\n",
        "    \n",
        "    self.conv1 = tf.contrib.layers.convolution2d(\n",
        "        inputs=self.imageIn,num_outputs=32,\n",
        "        kernel_size=[8,8],stride=[4,4],\n",
        "        padding='VALID',biases_initializer=None\n",
        "    )\n",
        "    \n",
        "    self.conv2 = tf.contrib.layers.convolution2d(\n",
        "        inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],\n",
        "        padding='VALID',biases_initializer=None\n",
        "    )\n",
        "    \n",
        "    self.conv3 = tf.contrib.layers.convolution2d(\n",
        "        inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],\n",
        "        padding='VALID',biases_initializer=None\n",
        "    )\n",
        "    \n",
        "    self.conv4 = tf.contrib.layers.convolution2d(\n",
        "        inputs=self.conv3,num_outputs=512,kernel_size=[7,7],stride=[1,1],\n",
        "        padding='VALID',biases_initializer=None\n",
        "    )\n",
        "    \n",
        "    self.streamAC, self.streamVC = tf.split(self.conv4,2,3)\n",
        "    self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
        "    self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
        "    self.AW = tf.Variable(tf.random_normal([h_size//2, env.actions]))\n",
        "    self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
        "    self.Advantage = tf.matmul(self.streamA, self.AW)\n",
        "    self.Value = tf.matmul(self.streamV, self.VW)\n",
        "    \n",
        "    self.Qout = self.Value+tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, reduction_indices=1,keep_dims=True))\n",
        "    \n",
        "    self.predict = tf.argmax(self.Qout,1)\n",
        "    \n",
        "    self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
        "    self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
        "    self.actions_onehot = tf.one_hot(self.actions, env.actions,dtype=tf.float32)\n",
        "    \n",
        "    self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot),reduction_indices=1)\n",
        "    \n",
        "    self.td_error = tf.square(self.targetQ-self.Q)\n",
        "    self.loss = tf.reduce_mean(self.td_error)\n",
        "    self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "    \n",
        "    self.updateModel = self.trainer.minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z-xYPsHETfwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class experience_buffer():\n",
        "  def __init__(self, buffer_size=50000):\n",
        "    self.buffer = []\n",
        "    self.buffer_size = buffer_size\n",
        "    \n",
        "  def add(self, experience):\n",
        "    if len(self.buffer)+len(experience)>=self.buffer_size:\n",
        "      self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
        "      \n",
        "    self.buffer.extend(experience)\n",
        "    \n",
        "  def sample(self, size):\n",
        "    return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zxOYxb-eMalo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def processState(states):\n",
        "  return np.reshape(states, [21168])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q14xb06MWFnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def updateTargetGraph(tfVars, tau):\n",
        "  total_vars = len(tfVars)\n",
        "  op_holder = []\n",
        "  for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
        "    op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau)+((1-tau)*tfVars[idx+total_vars//2].value())))\n",
        "      \n",
        "  return op_holder\n",
        "  \n",
        "def updateTarget(op_holder,sess):\n",
        "  for op in op_holder:\n",
        "    sess.run(op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SX9qMkYTU09s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "update_freq = 4\n",
        "y = .99\n",
        "startE = 1\n",
        "endE = 0.1\n",
        "anneling_steps = 10000.\n",
        "num_episodes = 10000\n",
        "pre_train_steps = 10000\n",
        "max_epLength = 50\n",
        "load_model = False\n",
        "path = './dqn'\n",
        "h_size = 512\n",
        "tau = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KF2Uu1swVI0z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mainQN = Qnetwork(h_size)\n",
        "targetQN = Qnetwork(h_size)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "trainables = tf.trainable_variables()\n",
        "targetOps = updateTargetGraph(trainables, tau)\n",
        "\n",
        "myBuffer = experience_buffer()\n",
        "\n",
        "e = startE\n",
        "\n",
        "stepDrop = (startE-endE)/anneling_steps\n",
        "\n",
        "rList = []\n",
        "total_steps = 0\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_uicL1u3Vdue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1728
        },
        "outputId": "9b615f91-d353-4aae-ed46-fd86f3dea82e"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  if load_model == True:\n",
        "    print('Loading Model...')\n",
        "    ckpt = tf.train.get_checkpoint_state(path)\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    \n",
        "  sess.run(init)\n",
        "  updateTarget(targetOps, sess)\n",
        "  for i in range(num_episodes+1):\n",
        "    episodeBuffer = experience_buffer()\n",
        "    s = env.reset()\n",
        "    s = processState(s)\n",
        "    d = False\n",
        "    rAll = 0\n",
        "    j = 0\n",
        "    \n",
        "    while j<max_epLength:\n",
        "      j+=1\n",
        "      if np.random.rand(1) < e or total_steps<pre_train_steps:\n",
        "        a = np.random.randint(0,4)\n",
        "        \n",
        "      else:\n",
        "        a = sess.run(mainQN.predict, feed_dict={mainQN.scalarInput:[s]})[0]\n",
        "        \n",
        "      s1,r,d = env.step(a)\n",
        "      s1 = processState(s1)\n",
        "      total_steps += 1\n",
        "      episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
        "      \n",
        "      if total_steps > pre_train_steps:\n",
        "        if e>endE:\n",
        "          e -= stepDrop\n",
        "          \n",
        "        if total_steps % (update_freq)==0:\n",
        "          trainBatch = myBuffer.sample(batch_size)\n",
        "          A = sess.run(mainQN.predict, feed_dict={\n",
        "              mainQN.scalarInput:np.vstack(trainBatch[:,3])\n",
        "          })\n",
        "          \n",
        "          Q = sess.run(targetQN.Qout, feed_dict={\n",
        "              targetQN.scalarInput:np.vstack(trainBatch[:,3])\n",
        "          })\n",
        "          doubleQ = Q[range(batch_size),A]\n",
        "          targetQ = trainBatch[:,2]+y*doubleQ\n",
        "          _ = sess.run(mainQN.updateModel, feed_dict={\n",
        "              mainQN.scalarInput:np.vstack(trainBatch[:,0]),\n",
        "              mainQN.targetQ:targetQ,\n",
        "              mainQ.//]\n",
        "              \n",
        "              P8IKHNE2N.actions:trainBatch[:,1]\n",
        "          })\n",
        "          \n",
        "          updateTarget(targetOps, sess)\n",
        "          \n",
        "      rAll += r\n",
        "      s = s1\n",
        "      \n",
        "      if d==True:\n",
        "        break\n",
        "        \n",
        "    myBuffer.add(episodeBuffer.buffer)\n",
        "    rList.append(rAll)\n",
        "    if i>0 and i%25==0:\n",
        "      print('episode',i,', average reward of last 25 episode',np.mean(rList[-25:]))\n",
        "      \n",
        "    if i>0 and i%1000==0:\n",
        "      saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
        "      print('Saved Model')\n",
        "      \n",
        "  saver.save(sess,path+'/model-'+str(i)+'.cptk')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if issubdtype(ts, int):\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  elif issubdtype(type(size), float):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode 25 , average reward of last 25 episode 2.08\n",
            "episode 50 , average reward of last 25 episode 1.52\n",
            "episode 75 , average reward of last 25 episode 1.8\n",
            "episode 100 , average reward of last 25 episode 0.76\n",
            "episode 125 , average reward of last 25 episode 0.96\n",
            "episode 150 , average reward of last 25 episode 1.08\n",
            "episode 175 , average reward of last 25 episode 0.92\n",
            "episode 200 , average reward of last 25 episode 0.6\n",
            "episode 225 , average reward of last 25 episode 0.52\n",
            "episode 250 , average reward of last 25 episode 0.36\n",
            "episode 275 , average reward of last 25 episode 0.48\n",
            "episode 300 , average reward of last 25 episode 0.56\n",
            "episode 325 , average reward of last 25 episode 0.48\n",
            "episode 350 , average reward of last 25 episode 0.32\n",
            "episode 375 , average reward of last 25 episode 0.0\n",
            "episode 400 , average reward of last 25 episode 0.12\n",
            "episode 425 , average reward of last 25 episode 0.44\n",
            "episode 450 , average reward of last 25 episode 0.68\n",
            "episode 475 , average reward of last 25 episode 0.64\n",
            "episode 500 , average reward of last 25 episode 0.32\n",
            "episode 525 , average reward of last 25 episode 0.48\n",
            "episode 550 , average reward of last 25 episode 0.08\n",
            "episode 575 , average reward of last 25 episode 0.44\n",
            "episode 600 , average reward of last 25 episode 0.52\n",
            "episode 625 , average reward of last 25 episode 0.4\n",
            "episode 650 , average reward of last 25 episode 0.52\n",
            "episode 675 , average reward of last 25 episode 0.0\n",
            "episode 700 , average reward of last 25 episode 0.44\n",
            "episode 725 , average reward of last 25 episode 0.12\n",
            "episode 750 , average reward of last 25 episode 0.4\n",
            "episode 775 , average reward of last 25 episode 0.16\n",
            "episode 800 , average reward of last 25 episode 0.36\n",
            "episode 825 , average reward of last 25 episode 0.4\n",
            "episode 850 , average reward of last 25 episode 0.2\n",
            "episode 875 , average reward of last 25 episode 0.44\n",
            "episode 900 , average reward of last 25 episode 0.68\n",
            "episode 925 , average reward of last 25 episode 0.68\n",
            "episode 950 , average reward of last 25 episode 0.52\n",
            "episode 975 , average reward of last 25 episode 0.6\n",
            "episode 1000 , average reward of last 25 episode 0.32\n",
            "Saved Model\n",
            "episode 1025 , average reward of last 25 episode 0.24\n",
            "episode 1050 , average reward of last 25 episode 0.44\n",
            "episode 1075 , average reward of last 25 episode 0.28\n",
            "episode 1100 , average reward of last 25 episode 0.36\n",
            "episode 1125 , average reward of last 25 episode 0.68\n",
            "episode 1150 , average reward of last 25 episode 0.16\n",
            "episode 1175 , average reward of last 25 episode 0.28\n",
            "episode 1200 , average reward of last 25 episode 0.92\n",
            "episode 1225 , average reward of last 25 episode 0.24\n",
            "episode 1250 , average reward of last 25 episode 0.28\n",
            "episode 1275 , average reward of last 25 episode 0.28\n",
            "episode 1300 , average reward of last 25 episode 0.48\n",
            "episode 1325 , average reward of last 25 episode 0.24\n",
            "episode 1350 , average reward of last 25 episode 0.0\n",
            "episode 1375 , average reward of last 25 episode 0.4\n",
            "episode 1400 , average reward of last 25 episode 0.12\n",
            "episode 1425 , average reward of last 25 episode 0.76\n",
            "episode 1450 , average reward of last 25 episode 0.36\n",
            "episode 1475 , average reward of last 25 episode 0.2\n",
            "episode 1500 , average reward of last 25 episode 0.36\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "episode 1525 , average reward of last 25 episode 0.52\n",
            "episode 1550 , average reward of last 25 episode 0.56\n",
            "episode 1575 , average reward of last 25 episode 0.36\n",
            "episode 1600 , average reward of last 25 episode 0.64\n",
            "episode 1625 , average reward of last 25 episode 0.48\n",
            "episode 1650 , average reward of last 25 episode 0.44\n",
            "episode 1675 , average reward of last 25 episode 0.72\n",
            "episode 1700 , average reward of last 25 episode 0.04\n",
            "episode 1725 , average reward of last 25 episode 0.0\n",
            "episode 1750 , average reward of last 25 episode 0.56\n",
            "episode 1775 , average reward of last 25 episode 0.88\n",
            "episode 1800 , average reward of last 25 episode 0.24\n",
            "episode 1825 , average reward of last 25 episode 0.08\n",
            "episode 1850 , average reward of last 25 episode 0.36\n",
            "episode 1875 , average reward of last 25 episode 0.44\n",
            "episode 1900 , average reward of last 25 episode 0.32\n",
            "episode 1925 , average reward of last 25 episode 0.4\n",
            "episode 1950 , average reward of last 25 episode 0.36\n",
            "episode 1975 , average reward of last 25 episode 0.16\n",
            "episode 2000 , average reward of last 25 episode 0.24\n",
            "Saved Model\n",
            "episode 2025 , average reward of last 25 episode 0.48\n",
            "episode 2050 , average reward of last 25 episode 0.48\n",
            "episode 2075 , average reward of last 25 episode 0.32\n",
            "episode 2100 , average reward of last 25 episode 0.6\n",
            "episode 2125 , average reward of last 25 episode 0.48\n",
            "episode 2150 , average reward of last 25 episode 0.08\n",
            "episode 2175 , average reward of last 25 episode 0.44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Itg5pk3zZO4r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
        "rMean = np.average(rMat,1)\n",
        "plt.plot(rMean)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}