{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "卷积神经网络.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/JozeeLin/google-tensorflow-exercise/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "PXeprKLxYe0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 卷积神经网络简介\n",
        "卷积神经网络最初是为解决图像识别等问题设计的，当然其现在的应用不仅限于图像和视频，也可用于时间序列信号，比如音频信号、文本数据等。\n",
        " \n",
        "在早期的图像识别研究中，最大的挑战是如何组织特征，因为图像数据不像其他类型的数据那样可以通过人工理解来提取特征。\n",
        " \n",
        "在股票预测等模型中，我们可以从原始数据中提取过往的交易价格波动、市盈率、市净率、盈利增长等金融因子，这既是特征工程。\n",
        " \n",
        "但是在图像中，我们很难根据人为理解提取出有效而丰富的特征。\n",
        " \n",
        "在深度学习出现之前，我们必须辅助SIFT、HoG等算法提取具有良好区分性的特征，再集合SVM等机器学习算法进行图像识别。\n",
        " \n",
        "SIFT对一定程度内的缩放、平移、旋转、视角改变、亮度调整等畸变，都具有不变性，是当时最重要的图像特征提取方法之一。\n",
        " \n",
        "在之前只能依靠SIFT等特征提取算法才能勉强进行可靠的图像识别。\n",
        " \n",
        "CNN可以直接使用图像的原始像素作为输入，而不必先使用SIFT等算法提取特征，减轻了使用传统算法如SVM时必需要做得大量重复、繁琐的数据预处理工作。\n",
        " \n",
        "CNN的最大特点在于卷积的权值共享结构，可以大幅减少神经网络的参数量，防止过拟合的同时又降低了神经网络的复杂度。\n",
        " \n",
        "一般的卷积神经网络由多个卷积层构成，每个卷积层中通常会进行如下几个操作:\n",
        " 1. 图像通过多个不同的卷积核的滤波，并加偏置(bias),提取出局部特征，每一个卷积核会映射出一个新的2D图像\n",
        " 2. 将前面卷积核的滤波输出结果，进行非线性的激活函数处理。目前最常见的是使用ReLU函数，而以前sigmoid函数用得比较多\n",
        " 3. 对激活函数的结果再进行池化操作(即降采样，比如将2x2的图片降为1x1的图片)，目前一般是使用最大池化，保留最显著的特征，并提升模型的畸变容忍能力\n",
        " <br>\n",
        " \n",
        "这几个步骤就构成了最常见的卷积层，当然也可以再加上一个LRN（局部响应归一化层）层，目前非常流行的Trick还有Batch Normalization等。\n",
        " \n",
        "一个卷积层中可以有多个不同的卷积核，而每一个卷积核都对应一个滤波后映射出的新图像，同一个新图像中每一个图像都来自完全相同的卷积核，这就是卷积核的全职共享。\n",
        " \n",
        "这一小块区域内的像素是相互关联的，每一个神经元不需要接收全部像素点的信息，只需要接收局部的像素点作为输入，而后将所有这些神经元收到的局部信息综合起来就可以得到全局的信息。\n",
        "这样就可以将之前的全连接的模式修改为局部连接，之前隐含层的每一个隐含节点都和全部像素相连，现在我们只需要将每一个隐含节点连接到局部的像素节点。\n",
        " \n",
        "通过局部连接的方法，将连接数从1万亿降低到1亿，但仍然偏多，需要继续降低参数量。现在隐含层每一个节点都与10x10的像素相连，也就是每一个隐含节点都拥有100个参数。假设我们的局部连接方式是卷积操作，即默认每一个**隐藏节点的参数都完全一样**，那我们的参数不再是1亿，而是100。**参数量只跟卷积核的大小有关，这也就是所谓的权值共享**。\n",
        " \n",
        "卷积神经网络的要点就是局部连接、权值共享和池化层中的降采样。其中，局部连接和权值共享降低了参数数量，使训练复杂度降低，并减轻了过拟合。\n",
        " \n",
        "同时权值共享还赋予了卷积神经网络对平移的容忍性，而池化层降采样则进一步降低了输出餐数量，并赋予模型对轻度型变得容忍性，提高了模型的泛化能力。\n",
        " \n",
        "LeNet5当时的特性有如下几点：\n",
        "1. 每个卷积层包含三个部分：卷积、池化和非线性激活函数\n",
        "2. 使用卷积提取空间特征\n",
        "3. 降采样(Subsample)的平均池化层(Average Pooling)\n",
        "4. 双曲正切(Tanh)或S型(Sigmoid)的激活函数\n",
        "5. MLP作为最后的分类器\n",
        "6. 层与层之间的稀疏连接减少计算复杂度\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "Htf-YtoEYSiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "a716268f-c81d-418a-cdc6-3adac1f9f436"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ndLPAYgxAqO5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weight_variable(shape):\n",
        "  #用于重复初始化权重和偏置\n",
        "  \n",
        "  #使用截断的正态分布噪声来打破完全对称，标准差设为0.1\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "  #使用ReLU，也需要给偏置增加一些小的正值0.1来避免死亡节点\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MSfjNLU2B03M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "卷积层、池化层也是接下来要重复使用的，因此也为他们分别定义创建函数。"
      ]
    },
    {
      "metadata": {
        "id": "vAZt8RH9BGIg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv2d(x,W):\n",
        "  '''\n",
        "  tf.nn.conv2d是tf中的2维卷积函数，参数中x是输入，W是卷积的参数，如[5,5,1,32],前两个表示卷积核的尺寸，\n",
        "  第三个表示有多少个channel，因为是灰度，所以为1，如果是RGB的为3，最后一个表示卷积核的数量\n",
        "  '''\n",
        "  return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  '''\n",
        "  strides代表卷积模块移动的步长，都是1代表会不遗漏的划过的图片的每一个点\n",
        "  padding代表卷积模板移动的步长，这里的SAME代表给边界加上padding让卷积的输出和输入保持同样的尺寸\n",
        "  tf.nn.max_pool是tf中的最大池化函数，这里使用2x2的最大池化，即将一个2x2的像素降为1x1的像素。最大池化会保留原始像素块中灰度值最高的那一个像素，即保留最显著的特征。\n",
        "  '''\n",
        "  return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "finXQKkYEAYg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y_ = tf.placeholder(tf.float32, [None, 10])\n",
        "#因为卷积神经网络会利用空间结构信息，因此需要将1D的输入向量转为2D的图片结构，即从1x784的形式转为原始的28x28的结构。[-1,28,28,1]，-1表示样本数量不固定，1代表颜色通道数量\n",
        "x_image = tf.reshape(x, [-1,28,28,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ou6QjaXJETmh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义第一个卷积层。使用前面写好的函数进行参数初始化，包括weights和bias\n",
        "W_conv1 = weight_variable([5,5,1,32])\n",
        "b_conv1 = bias_variable([32])\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1)+b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)#对卷积的输出结果进行池化操作"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8zLUNvXjFbAo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义第二个卷积层，唯一不同的是，这一层的卷积核数量为64\n",
        "W_conv2 = weight_variable([5,5,32,64])\n",
        "b_conv2 = bias_variable([64])\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2)+b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gvYrl_szGE7r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W_fc1 = weight_variable([7*7*64,1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OXeanempHUoF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#为了减轻过拟合，使用一个dropout层。在训练时，我们通过随机丢弃一部分节点的数据来减轻过拟合，预测时则保留全部数据来追求最好的预测性能\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "it8csMCTH3iE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W_fc2 = weight_variable([1024,10])\n",
        "b_fc2 = bias_variable([10])\n",
        "#最后我们将dropout层的输出连接一个softmax层，得到最后的概率输出\n",
        "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c2rz9I_3IIZU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义损失函数为cross entropy和之前一样，但是优化器使用Adam，并使用一个比较小的学习速率\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y_conv),reduction_indices=[1]))\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8B8tjKXAI_iM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#再继续定义评测准确率的操作\n",
        "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MM1rFQtwJR7C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "d7168b6e-da50-4769-d0d7-521c5c18775a"
      },
      "cell_type": "code",
      "source": [
        "#开始训练过程。首先初始化所有参数，设置训练时dropout的keep_prob为0.5\n",
        "tf.global_variables_initializer().run()\n",
        "for i in range(20000):\n",
        "  batch = mnist.train.next_batch(50)\n",
        "  if i%100 == 0:\n",
        "    train_accuracy = accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.0})\n",
        "    \n",
        "  train_step.run(feed_dict={x:batch[0], y_:batch[1], keep_prob:0.5})"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-dbb4d0d54670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m print('test accuracy %g' % accuracy.eval(feed_idct={\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m }))\n",
            "\u001b[0;31mTypeError\u001b[0m: eval() got an unexpected keyword argument 'feed_idct'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Bkb77IwpUbBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "042c3ce0-cfda-4f85-98e5-08e8ab8efd58"
      },
      "cell_type": "code",
      "source": [
        "print('test accuracy %g' % accuracy.eval(feed_dict={\n",
        "    x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0\n",
        "}))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.9911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EZMytsNLK6xk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 实现进阶的卷积网络\n",
        "本节使用的数据集是CIFAR-10，包含60000张32x32的彩色图像，其中训练集为50000张，测试集为10000张。10种类别。每一类6000张，分别为airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck。\n",
        "\n",
        "在这个卷积神经网络中，我们使用一些新的技巧\n",
        "- 对weight进行L2的正则化\n",
        "- 对图片进行了翻转、随机剪切等数据增强，制造了更多样本\n",
        "- 在每个卷积-最大池化层后面使用了LRN层，增强了模型的泛化能力"
      ]
    },
    {
      "metadata": {
        "id": "Xq6gzisnUvq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "d7b121f4-8035-46c7-db98-36d2532bf222"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models.git 'mymodels'"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mymodels'...\n",
            "remote: Counting objects: 16005, done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 16005 (delta 8), reused 16 (delta 6), pack-reused 15958\u001b[K\n",
            "Receiving objects: 100% (16005/16005), 423.97 MiB | 36.35 MiB/s, done.\n",
            "Resolving deltas: 100% (9459/9459), done.\n",
            "Checking out files: 100% (2161/2161), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hIA85xJXW46w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "985b77b5-a295-4508-d8be-7b65d4dc2b35"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab  MNIST_data  models  mymodels\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VpZ3f1tYXf2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ec1ab0f-9b6a-4e91-b90b-3d74c0b46be2"
      },
      "cell_type": "code",
      "source": [
        "!ls mymodels/tutorials/image/"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alexnet  cifar10  cifar10_estimator  imagenet  __init__.py  mnist  __pycache__\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AKbW-aAeKY7e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RlTRcMyrZndS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bea4f566-05db-4588-b158-bfd1c42ca479"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "F-fgohv2bztd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('mymodels/tutorials/image/cifar10')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "af7PMD0Cb678",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cifar10,cifar10_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OV6cK4VsM0sC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_steps = 3000\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nGz13sLTcGBp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "92190ac5-d363-4c85-89f0-80219b10ab9c"
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz /tmp/"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-05-03 05:24:34--  http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170052171 (162M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-binary.tar.gz’\n",
            "\n",
            "cifar-10-binary.tar 100%[===================>] 162.17M  3.84MB/s    in 44s     \n",
            "\n",
            "2018-05-03 05:25:19 (3.67 MB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\n",
            "\n",
            "/tmp/: Scheme missing.\n",
            "FINISHED --2018-05-03 05:25:19--\n",
            "Total wall clock time: 45s\n",
            "Downloaded: 1 files, 162M in 44s (3.67 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FZqOK9F5dj7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -zxf cifar-10-binary.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CoAn2nuid9Kn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = 'cifar-10-batches-bin'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yNTwGPRXNCMO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def variable_with_weight_loss(shape, stddev, w1):\n",
        "  var = tf.Variable(tf.truncated_normal(shape,stddev=stddev))\n",
        "  if w1 is not None:\n",
        "    weight_loss = tf.multiply(tf.nn.l2_loss(var),w1,name='weight_loss')\n",
        "    tf.add_to_collection('losses',weight_loss)\n",
        "  return var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJylxpUy63b6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "outputId": "6a45033f-138a-4264-a55d-6f4c52e12087"
      },
      "cell_type": "code",
      "source": [
        "#使用cifar10类下载数据集，并解压、展开到其默认位置\n",
        "cifar10.maybe_download_and_extract()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnrecognizedFlagError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-02a754d7036a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_download_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/mymodels/tutorials/image/cifar10/cifar10.py\u001b[0m in \u001b[0;36mmaybe_download_and_extract\u001b[0;34m()\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmaybe_download_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m   \u001b[0;34m\"\"\"Download and extract the tarball from Alex's website.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m   \u001b[0mdest_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0;32m--> 630\u001b[0;31m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "aR2FG6-n8cb5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 数据增强(Data Augmentation)\n",
        "数据增强操作包括随机的水平翻转、随机剪切一块24x24大小的图片(tf.random_crop)、设置随机的量度和对比度(tf.image.random_brightness、tf.image.random_contrast),\n",
        "\n",
        "以及对数据进行标准化tf.image.per_image_whitening(对数据减去均值，除以方差，保证数据0均值，方差为1)。通过这些操作，我们可以获得更多的样本(带噪声的)，\n",
        "\n",
        "原来的一张图片样本可以变为多张图片，相当于扩大样本量，对提高准确率非常有帮助。但是数据增强操作需要耗费大量的CPU时间。"
      ]
    },
    {
      "metadata": {
        "id": "rcUKOswo7P6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "33ba6957-b780-4dfe-84d6-fe2ab5562657"
      },
      "cell_type": "code",
      "source": [
        "#使用distorted_inputs函数产生训练需要使用的数据，包括特征及其对应的label，这里返回的是已经封装好的tensor\n",
        "images_train,labels_train=cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yYlXgYrW7crC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#生成测试数据\n",
        "images_test, labels_test = cifar10_input.inputs(eval_data=True,data_dir=data_dir,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TAgR9f_l93Id",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#输入数据的placeholder，包括特征和label\n",
        "image_holder = tf.placeholder(tf.float32, [batch_size,24,24,3])\n",
        "label_holder = tf.placeholder(tf.int32,[batch_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2LyrDUyf-lmD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weight1 = variable_with_weight_loss(shape=[5,5,3,64],stddev=5e-2,w1=0.0)\n",
        "kernel1 = tf.nn.conv2d(image_holder,weight1,[1,1,1,1],padding='SAME')\n",
        "bias1 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
        "conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\n",
        "pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1], strides=[1,2,2,1],padding='SAME')\n",
        "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.01/9.0,beta=0.75)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZyldOm4B96J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#创建第二个卷积层\n",
        "weight2 = variable_with_weight_loss(shape=[5,5,64,64],stddev=5e-2,w1=0.0)\n",
        "kernel2 = tf.nn.conv2d(norm1, weight2,[1,1,1,1],padding='SAME')\n",
        "bias2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
        "conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\n",
        "norm2 = tf.nn.lrn(conv2,4,bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
        "pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,2,2,1],padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z-ZKxouNC4hF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#全连接层，先把第二个卷积层的输出结果flatten\n",
        "reshape = tf.reshape(pool2, [batch_size,-1])\n",
        "dim = reshape.get_shape()[1].value\n",
        "weight3 = variable_with_weight_loss(shape=[dim, 384], stddev=0.04, w1=0.004)\n",
        "bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
        "local3 = tf.nn.relu(tf.matmul(reshape,weight3)+bias3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-e5dpmbDg0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#全连接层，隐藏节点下降一半\n",
        "weight4 = variable_with_weight_loss(shape=[384,192], stddev=0.04, w1=0.004)\n",
        "bias4 = tf.Variable(tf.constant(0.1, shape=[192]))\n",
        "local4 = tf.nn.relu(tf.matmul(local3, weight4)+bias4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W9QeHqOID5Jm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#最后一层\n",
        "weight5 = variable_with_weight_loss(shape=[192,10], stddev=1/192.0, w1=0.0)\n",
        "bias5 = tf.Variable(tf.constant(0.0, shape=[10]))\n",
        "logits = tf.add(tf.matmul(local4, weight5),bias5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GUDojsuREUVF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(logits, labels):\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      logits = logits, labels=labels, name='cross_entropy_per_example'\n",
        "  )\n",
        "  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
        "  tf.add_to_collection('losses', cross_entropy_mean)\n",
        "  \n",
        "  return tf.add_n(tf.get_collection('losses'), name='total_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1fgKou4E5zT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = loss(logits,label_holder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBD85zmzFAeM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hrkSjRPmFHji",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_k_op = tf.nn.in_top_k(logits, label_holder, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UIgqg_OvFPOm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "boYuBPZ7FWzs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "f7f7844f-8571-4683-cd04-25a6ffafad4a"
      },
      "cell_type": "code",
      "source": [
        "tf.train.start_queue_runners()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Thread(QueueRunnerThread-input_producer-input_producer/input_producer_EnqueueMany, started daemon 140556638086912)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556629694208)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556621301504)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556612908800)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556604516096)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556595861248)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556587468544)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556579075840)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556570683136)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556562290432)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556553897728)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556545505024)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556537112320)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556528719616)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556520326912)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556511934208)>,\n",
              " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140556503541504)>,\n",
              " <Thread(QueueRunnerThread-input/input_producer-input/input_producer/input_producer_EnqueueMany, started daemon 140556495148800)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556486756096)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556478363392)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556469970688)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556461577984)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556453185280)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556444792576)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556436399872)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556428007168)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556419614464)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556411221760)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556402829056)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556394436352)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556386043648)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556377650944)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556369258240)>,\n",
              " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 140556360865536)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "metadata": {
        "id": "1hAKvNUMFaJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5118
        },
        "outputId": "250a2580-5fad-4171-c235-c98eac0275a8"
      },
      "cell_type": "code",
      "source": [
        "for step in range(max_steps):\n",
        "  start_time = time.time()\n",
        "  image_batch, label_batch=sess.run([images_train,labels_train])\n",
        "  _, loss_value = sess.run([train_op, loss],\n",
        "                          feed_dict={image_holder:image_batch, label_holder:label_batch})\n",
        "  duration = time.time()-start_time\n",
        "  \n",
        "  if step % 10 ==0:\n",
        "    examples_per_sec = batch_size/duration\n",
        "    sec_per_batch=float(duration)\n",
        "    \n",
        "    format_str=('step %d, loss=%.2f(%.1f examples/sec; %.3f sec/batch)')\n",
        "    print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, loss=4.68(152.4 examples/sec; 0.840 sec/batch)\n",
            "step 10, loss=3.76(176.4 examples/sec; 0.726 sec/batch)\n",
            "step 20, loss=3.09(175.7 examples/sec; 0.729 sec/batch)\n",
            "step 30, loss=2.59(172.1 examples/sec; 0.744 sec/batch)\n",
            "step 40, loss=2.64(176.8 examples/sec; 0.724 sec/batch)\n",
            "step 50, loss=2.40(177.1 examples/sec; 0.723 sec/batch)\n",
            "step 60, loss=2.18(148.4 examples/sec; 0.862 sec/batch)\n",
            "step 70, loss=1.99(176.5 examples/sec; 0.725 sec/batch)\n",
            "step 80, loss=2.23(175.1 examples/sec; 0.731 sec/batch)\n",
            "step 90, loss=2.05(178.3 examples/sec; 0.718 sec/batch)\n",
            "step 100, loss=1.98(175.8 examples/sec; 0.728 sec/batch)\n",
            "step 110, loss=2.02(180.2 examples/sec; 0.710 sec/batch)\n",
            "step 120, loss=1.88(174.7 examples/sec; 0.733 sec/batch)\n",
            "step 130, loss=1.96(178.4 examples/sec; 0.718 sec/batch)\n",
            "step 140, loss=1.85(179.2 examples/sec; 0.714 sec/batch)\n",
            "step 150, loss=1.77(177.8 examples/sec; 0.720 sec/batch)\n",
            "step 160, loss=1.95(177.4 examples/sec; 0.722 sec/batch)\n",
            "step 170, loss=1.73(178.9 examples/sec; 0.716 sec/batch)\n",
            "step 180, loss=1.75(175.4 examples/sec; 0.730 sec/batch)\n",
            "step 190, loss=1.72(179.7 examples/sec; 0.712 sec/batch)\n",
            "step 200, loss=1.72(179.5 examples/sec; 0.713 sec/batch)\n",
            "step 210, loss=1.59(179.7 examples/sec; 0.712 sec/batch)\n",
            "step 220, loss=1.53(177.9 examples/sec; 0.720 sec/batch)\n",
            "step 230, loss=1.81(180.5 examples/sec; 0.709 sec/batch)\n",
            "step 240, loss=1.66(178.7 examples/sec; 0.716 sec/batch)\n",
            "step 250, loss=1.86(178.2 examples/sec; 0.718 sec/batch)\n",
            "step 260, loss=1.79(180.3 examples/sec; 0.710 sec/batch)\n",
            "step 270, loss=1.60(182.1 examples/sec; 0.703 sec/batch)\n",
            "step 280, loss=1.63(179.4 examples/sec; 0.713 sec/batch)\n",
            "step 290, loss=1.52(179.8 examples/sec; 0.712 sec/batch)\n",
            "step 300, loss=1.63(183.2 examples/sec; 0.699 sec/batch)\n",
            "step 310, loss=1.47(182.6 examples/sec; 0.701 sec/batch)\n",
            "step 320, loss=1.67(179.1 examples/sec; 0.715 sec/batch)\n",
            "step 330, loss=1.62(179.7 examples/sec; 0.712 sec/batch)\n",
            "step 340, loss=1.66(183.4 examples/sec; 0.698 sec/batch)\n",
            "step 350, loss=1.55(180.9 examples/sec; 0.707 sec/batch)\n",
            "step 360, loss=1.40(180.0 examples/sec; 0.711 sec/batch)\n",
            "step 370, loss=1.51(180.9 examples/sec; 0.707 sec/batch)\n",
            "step 380, loss=1.56(183.2 examples/sec; 0.699 sec/batch)\n",
            "step 390, loss=1.50(180.9 examples/sec; 0.707 sec/batch)\n",
            "step 400, loss=1.58(179.5 examples/sec; 0.713 sec/batch)\n",
            "step 410, loss=1.43(181.3 examples/sec; 0.706 sec/batch)\n",
            "step 420, loss=1.69(177.6 examples/sec; 0.721 sec/batch)\n",
            "step 430, loss=1.42(177.2 examples/sec; 0.723 sec/batch)\n",
            "step 440, loss=1.60(176.8 examples/sec; 0.724 sec/batch)\n",
            "step 450, loss=1.51(179.2 examples/sec; 0.714 sec/batch)\n",
            "step 460, loss=1.70(174.4 examples/sec; 0.734 sec/batch)\n",
            "step 470, loss=1.48(175.3 examples/sec; 0.730 sec/batch)\n",
            "step 480, loss=1.36(179.6 examples/sec; 0.713 sec/batch)\n",
            "step 490, loss=1.50(179.4 examples/sec; 0.714 sec/batch)\n",
            "step 500, loss=1.48(179.1 examples/sec; 0.715 sec/batch)\n",
            "step 510, loss=1.47(179.3 examples/sec; 0.714 sec/batch)\n",
            "step 520, loss=1.42(175.9 examples/sec; 0.727 sec/batch)\n",
            "step 530, loss=1.26(176.7 examples/sec; 0.724 sec/batch)\n",
            "step 540, loss=1.55(179.3 examples/sec; 0.714 sec/batch)\n",
            "step 550, loss=1.52(181.7 examples/sec; 0.704 sec/batch)\n",
            "step 560, loss=1.36(180.7 examples/sec; 0.708 sec/batch)\n",
            "step 570, loss=1.58(179.7 examples/sec; 0.712 sec/batch)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 580, loss=1.57(182.0 examples/sec; 0.703 sec/batch)\n",
            "step 590, loss=1.41(179.5 examples/sec; 0.713 sec/batch)\n",
            "step 600, loss=1.39(178.9 examples/sec; 0.716 sec/batch)\n",
            "step 610, loss=1.52(179.5 examples/sec; 0.713 sec/batch)\n",
            "step 620, loss=1.50(181.9 examples/sec; 0.704 sec/batch)\n",
            "step 630, loss=1.28(181.5 examples/sec; 0.705 sec/batch)\n",
            "step 640, loss=1.23(177.5 examples/sec; 0.721 sec/batch)\n",
            "step 650, loss=1.58(180.2 examples/sec; 0.710 sec/batch)\n",
            "step 660, loss=1.43(180.2 examples/sec; 0.710 sec/batch)\n",
            "step 670, loss=1.35(180.2 examples/sec; 0.710 sec/batch)\n",
            "step 680, loss=1.41(178.7 examples/sec; 0.716 sec/batch)\n",
            "step 690, loss=1.39(180.0 examples/sec; 0.711 sec/batch)\n",
            "step 700, loss=1.41(179.9 examples/sec; 0.711 sec/batch)\n",
            "step 710, loss=1.31(178.1 examples/sec; 0.719 sec/batch)\n",
            "step 720, loss=1.42(182.0 examples/sec; 0.703 sec/batch)\n",
            "step 730, loss=1.34(182.0 examples/sec; 0.703 sec/batch)\n",
            "step 740, loss=1.25(177.0 examples/sec; 0.723 sec/batch)\n",
            "step 750, loss=1.28(177.7 examples/sec; 0.720 sec/batch)\n",
            "step 760, loss=1.23(182.8 examples/sec; 0.700 sec/batch)\n",
            "step 770, loss=1.25(183.3 examples/sec; 0.698 sec/batch)\n",
            "step 780, loss=1.22(181.2 examples/sec; 0.706 sec/batch)\n",
            "step 790, loss=1.32(177.6 examples/sec; 0.721 sec/batch)\n",
            "step 800, loss=1.22(178.1 examples/sec; 0.719 sec/batch)\n",
            "step 810, loss=1.32(181.4 examples/sec; 0.706 sec/batch)\n",
            "step 820, loss=1.37(176.6 examples/sec; 0.725 sec/batch)\n",
            "step 830, loss=1.25(180.4 examples/sec; 0.710 sec/batch)\n",
            "step 840, loss=1.30(178.3 examples/sec; 0.718 sec/batch)\n",
            "step 850, loss=1.45(180.1 examples/sec; 0.711 sec/batch)\n",
            "step 860, loss=1.19(177.3 examples/sec; 0.722 sec/batch)\n",
            "step 870, loss=1.40(178.8 examples/sec; 0.716 sec/batch)\n",
            "step 880, loss=1.34(179.1 examples/sec; 0.715 sec/batch)\n",
            "step 890, loss=1.43(180.1 examples/sec; 0.711 sec/batch)\n",
            "step 900, loss=1.41(182.8 examples/sec; 0.700 sec/batch)\n",
            "step 910, loss=1.28(181.9 examples/sec; 0.704 sec/batch)\n",
            "step 920, loss=1.03(181.6 examples/sec; 0.705 sec/batch)\n",
            "step 930, loss=1.34(180.6 examples/sec; 0.709 sec/batch)\n",
            "step 940, loss=1.35(181.1 examples/sec; 0.707 sec/batch)\n",
            "step 950, loss=1.31(181.6 examples/sec; 0.705 sec/batch)\n",
            "step 960, loss=1.31(180.4 examples/sec; 0.710 sec/batch)\n",
            "step 970, loss=1.38(180.2 examples/sec; 0.710 sec/batch)\n",
            "step 980, loss=1.33(182.2 examples/sec; 0.703 sec/batch)\n",
            "step 990, loss=1.35(183.1 examples/sec; 0.699 sec/batch)\n",
            "step 1000, loss=1.36(181.5 examples/sec; 0.705 sec/batch)\n",
            "step 1010, loss=1.45(179.3 examples/sec; 0.714 sec/batch)\n",
            "step 1020, loss=1.19(181.2 examples/sec; 0.707 sec/batch)\n",
            "step 1030, loss=1.25(182.4 examples/sec; 0.702 sec/batch)\n",
            "step 1040, loss=1.40(177.8 examples/sec; 0.720 sec/batch)\n",
            "step 1050, loss=1.07(179.7 examples/sec; 0.712 sec/batch)\n",
            "step 1060, loss=1.29(174.5 examples/sec; 0.733 sec/batch)\n",
            "step 1070, loss=1.41(182.3 examples/sec; 0.702 sec/batch)\n",
            "step 1080, loss=1.30(182.1 examples/sec; 0.703 sec/batch)\n",
            "step 1090, loss=1.21(178.1 examples/sec; 0.719 sec/batch)\n",
            "step 1100, loss=1.29(179.7 examples/sec; 0.712 sec/batch)\n",
            "step 1110, loss=1.21(180.8 examples/sec; 0.708 sec/batch)\n",
            "step 1120, loss=1.52(181.5 examples/sec; 0.705 sec/batch)\n",
            "step 1130, loss=1.26(179.4 examples/sec; 0.713 sec/batch)\n",
            "step 1140, loss=1.42(177.2 examples/sec; 0.722 sec/batch)\n",
            "step 1150, loss=1.44(177.8 examples/sec; 0.720 sec/batch)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1160, loss=1.22(181.6 examples/sec; 0.705 sec/batch)\n",
            "step 1170, loss=1.36(181.3 examples/sec; 0.706 sec/batch)\n",
            "step 1180, loss=1.32(178.4 examples/sec; 0.717 sec/batch)\n",
            "step 1190, loss=1.31(179.0 examples/sec; 0.715 sec/batch)\n",
            "step 1200, loss=1.00(179.7 examples/sec; 0.712 sec/batch)\n",
            "step 1210, loss=1.19(178.6 examples/sec; 0.717 sec/batch)\n",
            "step 1220, loss=1.45(180.6 examples/sec; 0.709 sec/batch)\n",
            "step 1230, loss=1.28(181.5 examples/sec; 0.705 sec/batch)\n",
            "step 1240, loss=1.06(180.3 examples/sec; 0.710 sec/batch)\n",
            "step 1250, loss=1.36(179.4 examples/sec; 0.714 sec/batch)\n",
            "step 1260, loss=1.26(180.4 examples/sec; 0.710 sec/batch)\n",
            "step 1270, loss=1.37(179.5 examples/sec; 0.713 sec/batch)\n",
            "step 1280, loss=1.26(183.0 examples/sec; 0.699 sec/batch)\n",
            "step 1290, loss=1.28(179.6 examples/sec; 0.713 sec/batch)\n",
            "step 1300, loss=1.24(181.6 examples/sec; 0.705 sec/batch)\n",
            "step 1310, loss=1.16(183.3 examples/sec; 0.698 sec/batch)\n",
            "step 1320, loss=1.30(180.8 examples/sec; 0.708 sec/batch)\n",
            "step 1330, loss=1.38(181.2 examples/sec; 0.706 sec/batch)\n",
            "step 1340, loss=1.21(160.9 examples/sec; 0.796 sec/batch)\n",
            "step 1350, loss=1.30(183.4 examples/sec; 0.698 sec/batch)\n",
            "step 1360, loss=1.20(182.8 examples/sec; 0.700 sec/batch)\n",
            "step 1370, loss=1.19(182.4 examples/sec; 0.702 sec/batch)\n",
            "step 1380, loss=1.53(181.2 examples/sec; 0.706 sec/batch)\n",
            "step 1390, loss=1.12(183.0 examples/sec; 0.699 sec/batch)\n",
            "step 1400, loss=1.19(179.8 examples/sec; 0.712 sec/batch)\n",
            "step 1410, loss=1.28(182.9 examples/sec; 0.700 sec/batch)\n",
            "step 1420, loss=1.26(180.7 examples/sec; 0.708 sec/batch)\n",
            "step 1430, loss=1.12(180.9 examples/sec; 0.708 sec/batch)\n",
            "step 1440, loss=1.26(179.4 examples/sec; 0.714 sec/batch)\n",
            "step 1450, loss=1.20(184.1 examples/sec; 0.695 sec/batch)\n",
            "step 1460, loss=1.03(174.7 examples/sec; 0.732 sec/batch)\n",
            "step 1470, loss=1.08(179.7 examples/sec; 0.712 sec/batch)\n",
            "step 1480, loss=1.31(179.3 examples/sec; 0.714 sec/batch)\n",
            "step 1490, loss=1.38(180.1 examples/sec; 0.711 sec/batch)\n",
            "step 1500, loss=1.25(183.3 examples/sec; 0.698 sec/batch)\n",
            "step 1510, loss=1.34(182.4 examples/sec; 0.702 sec/batch)\n",
            "step 1520, loss=1.32(183.9 examples/sec; 0.696 sec/batch)\n",
            "step 1530, loss=1.36(184.0 examples/sec; 0.696 sec/batch)\n",
            "step 1540, loss=1.21(181.1 examples/sec; 0.707 sec/batch)\n",
            "step 1550, loss=1.10(181.6 examples/sec; 0.705 sec/batch)\n",
            "step 1560, loss=1.13(180.9 examples/sec; 0.708 sec/batch)\n",
            "step 1570, loss=1.20(183.7 examples/sec; 0.697 sec/batch)\n",
            "step 1580, loss=1.26(182.7 examples/sec; 0.701 sec/batch)\n",
            "step 1590, loss=1.29(181.5 examples/sec; 0.705 sec/batch)\n",
            "step 1600, loss=1.18(180.7 examples/sec; 0.708 sec/batch)\n",
            "step 1610, loss=1.23(184.4 examples/sec; 0.694 sec/batch)\n",
            "step 1620, loss=1.26(183.1 examples/sec; 0.699 sec/batch)\n",
            "step 1630, loss=1.04(182.1 examples/sec; 0.703 sec/batch)\n",
            "step 1640, loss=1.24(184.6 examples/sec; 0.693 sec/batch)\n",
            "step 1650, loss=1.25(183.5 examples/sec; 0.698 sec/batch)\n",
            "step 1660, loss=1.13(186.2 examples/sec; 0.688 sec/batch)\n",
            "step 1670, loss=1.17(187.1 examples/sec; 0.684 sec/batch)\n",
            "step 1680, loss=1.20(185.5 examples/sec; 0.690 sec/batch)\n",
            "step 1690, loss=1.18(181.1 examples/sec; 0.707 sec/batch)\n",
            "step 1700, loss=1.36(185.1 examples/sec; 0.691 sec/batch)\n",
            "step 1710, loss=1.27(182.7 examples/sec; 0.700 sec/batch)\n",
            "step 1720, loss=1.42(182.5 examples/sec; 0.701 sec/batch)\n",
            "step 1730, loss=1.11(181.6 examples/sec; 0.705 sec/batch)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 1740, loss=1.28(182.2 examples/sec; 0.703 sec/batch)\n",
            "step 1750, loss=1.18(179.9 examples/sec; 0.712 sec/batch)\n",
            "step 1760, loss=1.08(180.1 examples/sec; 0.711 sec/batch)\n",
            "step 1770, loss=1.24(179.9 examples/sec; 0.711 sec/batch)\n",
            "step 1780, loss=1.15(179.8 examples/sec; 0.712 sec/batch)\n",
            "step 1790, loss=1.17(180.7 examples/sec; 0.708 sec/batch)\n",
            "step 1800, loss=1.12(181.8 examples/sec; 0.704 sec/batch)\n",
            "step 1810, loss=1.01(183.3 examples/sec; 0.698 sec/batch)\n",
            "step 1820, loss=1.14(181.7 examples/sec; 0.704 sec/batch)\n",
            "step 1830, loss=1.06(183.4 examples/sec; 0.698 sec/batch)\n",
            "step 1840, loss=1.06(179.0 examples/sec; 0.715 sec/batch)\n",
            "step 1850, loss=1.12(181.7 examples/sec; 0.704 sec/batch)\n",
            "step 1860, loss=1.17(182.7 examples/sec; 0.701 sec/batch)\n",
            "step 1870, loss=1.05(183.6 examples/sec; 0.697 sec/batch)\n",
            "step 1880, loss=1.24(181.3 examples/sec; 0.706 sec/batch)\n",
            "step 1890, loss=1.21(179.1 examples/sec; 0.715 sec/batch)\n",
            "step 1900, loss=1.24(181.0 examples/sec; 0.707 sec/batch)\n",
            "step 1910, loss=1.37(179.8 examples/sec; 0.712 sec/batch)\n",
            "step 1920, loss=1.34(181.2 examples/sec; 0.707 sec/batch)\n",
            "step 1930, loss=1.19(181.3 examples/sec; 0.706 sec/batch)\n",
            "step 1940, loss=1.17(182.4 examples/sec; 0.702 sec/batch)\n",
            "step 1950, loss=0.93(178.3 examples/sec; 0.718 sec/batch)\n",
            "step 1960, loss=1.13(180.7 examples/sec; 0.708 sec/batch)\n",
            "step 1970, loss=1.10(185.0 examples/sec; 0.692 sec/batch)\n",
            "step 1980, loss=1.12(184.3 examples/sec; 0.695 sec/batch)\n",
            "step 1990, loss=1.12(185.6 examples/sec; 0.690 sec/batch)\n",
            "step 2000, loss=1.17(183.6 examples/sec; 0.697 sec/batch)\n",
            "step 2010, loss=1.11(187.1 examples/sec; 0.684 sec/batch)\n",
            "step 2020, loss=1.19(185.7 examples/sec; 0.689 sec/batch)\n",
            "step 2030, loss=1.00(185.0 examples/sec; 0.692 sec/batch)\n",
            "step 2040, loss=1.03(185.6 examples/sec; 0.690 sec/batch)\n",
            "step 2050, loss=1.22(182.3 examples/sec; 0.702 sec/batch)\n",
            "step 2060, loss=1.18(183.6 examples/sec; 0.697 sec/batch)\n",
            "step 2070, loss=1.03(183.2 examples/sec; 0.699 sec/batch)\n",
            "step 2080, loss=1.10(183.3 examples/sec; 0.698 sec/batch)\n",
            "step 2090, loss=1.20(181.8 examples/sec; 0.704 sec/batch)\n",
            "step 2100, loss=1.05(182.0 examples/sec; 0.703 sec/batch)\n",
            "step 2110, loss=1.28(183.2 examples/sec; 0.699 sec/batch)\n",
            "step 2120, loss=1.15(181.9 examples/sec; 0.704 sec/batch)\n",
            "step 2130, loss=0.96(181.9 examples/sec; 0.704 sec/batch)\n",
            "step 2140, loss=1.04(182.6 examples/sec; 0.701 sec/batch)\n",
            "step 2150, loss=1.13(184.8 examples/sec; 0.693 sec/batch)\n",
            "step 2160, loss=1.12(185.1 examples/sec; 0.691 sec/batch)\n",
            "step 2170, loss=1.04(181.9 examples/sec; 0.704 sec/batch)\n",
            "step 2180, loss=1.25(185.7 examples/sec; 0.689 sec/batch)\n",
            "step 2190, loss=1.24(184.5 examples/sec; 0.694 sec/batch)\n",
            "step 2200, loss=1.08(183.8 examples/sec; 0.696 sec/batch)\n",
            "step 2210, loss=1.13(182.9 examples/sec; 0.700 sec/batch)\n",
            "step 2220, loss=1.00(180.4 examples/sec; 0.710 sec/batch)\n",
            "step 2230, loss=1.22(179.5 examples/sec; 0.713 sec/batch)\n",
            "step 2240, loss=1.04(182.5 examples/sec; 0.701 sec/batch)\n",
            "step 2250, loss=1.12(180.3 examples/sec; 0.710 sec/batch)\n",
            "step 2260, loss=1.11(183.8 examples/sec; 0.696 sec/batch)\n",
            "step 2270, loss=1.17(178.4 examples/sec; 0.717 sec/batch)\n",
            "step 2280, loss=0.93(178.5 examples/sec; 0.717 sec/batch)\n",
            "step 2290, loss=1.08(180.7 examples/sec; 0.708 sec/batch)\n",
            "step 2300, loss=0.95(180.6 examples/sec; 0.709 sec/batch)\n",
            "step 2310, loss=1.02(180.3 examples/sec; 0.710 sec/batch)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2320, loss=0.98(179.3 examples/sec; 0.714 sec/batch)\n",
            "step 2330, loss=0.96(181.3 examples/sec; 0.706 sec/batch)\n",
            "step 2340, loss=1.17(180.8 examples/sec; 0.708 sec/batch)\n",
            "step 2350, loss=1.02(180.5 examples/sec; 0.709 sec/batch)\n",
            "step 2360, loss=1.22(182.9 examples/sec; 0.700 sec/batch)\n",
            "step 2370, loss=1.10(184.3 examples/sec; 0.695 sec/batch)\n",
            "step 2380, loss=0.92(183.5 examples/sec; 0.698 sec/batch)\n",
            "step 2390, loss=0.95(183.3 examples/sec; 0.698 sec/batch)\n",
            "step 2400, loss=1.01(186.0 examples/sec; 0.688 sec/batch)\n",
            "step 2410, loss=1.16(179.3 examples/sec; 0.714 sec/batch)\n",
            "step 2420, loss=1.10(180.9 examples/sec; 0.708 sec/batch)\n",
            "step 2430, loss=1.27(186.3 examples/sec; 0.687 sec/batch)\n",
            "step 2440, loss=0.96(180.6 examples/sec; 0.709 sec/batch)\n",
            "step 2450, loss=0.89(186.2 examples/sec; 0.688 sec/batch)\n",
            "step 2460, loss=1.14(184.4 examples/sec; 0.694 sec/batch)\n",
            "step 2470, loss=1.04(185.4 examples/sec; 0.691 sec/batch)\n",
            "step 2480, loss=1.06(183.9 examples/sec; 0.696 sec/batch)\n",
            "step 2490, loss=1.01(183.0 examples/sec; 0.699 sec/batch)\n",
            "step 2500, loss=0.99(183.7 examples/sec; 0.697 sec/batch)\n",
            "step 2510, loss=1.18(185.1 examples/sec; 0.692 sec/batch)\n",
            "step 2520, loss=1.06(184.1 examples/sec; 0.695 sec/batch)\n",
            "step 2530, loss=1.23(178.7 examples/sec; 0.716 sec/batch)\n",
            "step 2540, loss=1.08(184.6 examples/sec; 0.693 sec/batch)\n",
            "step 2550, loss=1.16(185.5 examples/sec; 0.690 sec/batch)\n",
            "step 2560, loss=1.16(182.2 examples/sec; 0.703 sec/batch)\n",
            "step 2570, loss=0.96(182.5 examples/sec; 0.701 sec/batch)\n",
            "step 2580, loss=1.04(184.2 examples/sec; 0.695 sec/batch)\n",
            "step 2590, loss=1.02(178.6 examples/sec; 0.717 sec/batch)\n",
            "step 2600, loss=1.08(182.2 examples/sec; 0.702 sec/batch)\n",
            "step 2610, loss=1.09(182.2 examples/sec; 0.703 sec/batch)\n",
            "step 2620, loss=1.18(182.5 examples/sec; 0.701 sec/batch)\n",
            "step 2630, loss=1.13(182.9 examples/sec; 0.700 sec/batch)\n",
            "step 2640, loss=1.04(180.9 examples/sec; 0.708 sec/batch)\n",
            "step 2650, loss=0.98(183.7 examples/sec; 0.697 sec/batch)\n",
            "step 2660, loss=1.00(181.0 examples/sec; 0.707 sec/batch)\n",
            "step 2670, loss=1.07(180.6 examples/sec; 0.709 sec/batch)\n",
            "step 2680, loss=0.98(182.0 examples/sec; 0.703 sec/batch)\n",
            "step 2690, loss=1.15(183.3 examples/sec; 0.698 sec/batch)\n",
            "step 2700, loss=1.09(180.5 examples/sec; 0.709 sec/batch)\n",
            "step 2710, loss=1.09(183.9 examples/sec; 0.696 sec/batch)\n",
            "step 2720, loss=0.99(180.5 examples/sec; 0.709 sec/batch)\n",
            "step 2730, loss=1.09(184.0 examples/sec; 0.696 sec/batch)\n",
            "step 2740, loss=1.12(180.7 examples/sec; 0.708 sec/batch)\n",
            "step 2750, loss=1.12(181.2 examples/sec; 0.706 sec/batch)\n",
            "step 2760, loss=1.07(182.8 examples/sec; 0.700 sec/batch)\n",
            "step 2770, loss=1.23(182.2 examples/sec; 0.702 sec/batch)\n",
            "step 2780, loss=1.04(183.8 examples/sec; 0.696 sec/batch)\n",
            "step 2790, loss=1.03(185.9 examples/sec; 0.688 sec/batch)\n",
            "step 2800, loss=0.94(182.2 examples/sec; 0.703 sec/batch)\n",
            "step 2810, loss=1.14(184.4 examples/sec; 0.694 sec/batch)\n",
            "step 2820, loss=1.02(184.0 examples/sec; 0.695 sec/batch)\n",
            "step 2830, loss=1.05(184.3 examples/sec; 0.694 sec/batch)\n",
            "step 2840, loss=1.03(181.3 examples/sec; 0.706 sec/batch)\n",
            "step 2850, loss=0.99(182.9 examples/sec; 0.700 sec/batch)\n",
            "step 2860, loss=1.18(183.2 examples/sec; 0.699 sec/batch)\n",
            "step 2870, loss=1.08(185.6 examples/sec; 0.690 sec/batch)\n",
            "step 2880, loss=1.17(186.0 examples/sec; 0.688 sec/batch)\n",
            "step 2890, loss=1.04(182.1 examples/sec; 0.703 sec/batch)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "step 2900, loss=1.10(181.6 examples/sec; 0.705 sec/batch)\n",
            "step 2910, loss=1.04(183.5 examples/sec; 0.698 sec/batch)\n",
            "step 2920, loss=1.08(186.6 examples/sec; 0.686 sec/batch)\n",
            "step 2930, loss=0.97(185.2 examples/sec; 0.691 sec/batch)\n",
            "step 2940, loss=1.00(183.6 examples/sec; 0.697 sec/batch)\n",
            "step 2950, loss=1.01(183.0 examples/sec; 0.699 sec/batch)\n",
            "step 2960, loss=1.23(188.1 examples/sec; 0.681 sec/batch)\n",
            "step 2970, loss=1.09(183.7 examples/sec; 0.697 sec/batch)\n",
            "step 2980, loss=0.98(182.6 examples/sec; 0.701 sec/batch)\n",
            "step 2990, loss=1.26(181.8 examples/sec; 0.704 sec/batch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tr1ghM7xFypI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c0674bc0-258b-4084-c227-4562f3dda2c6"
      },
      "cell_type": "code",
      "source": [
        "num_examples = 10000\n",
        "import math\n",
        "num_iter = int(math.ceil(num_examples/batch_size))\n",
        "true_count = 0\n",
        "total_sample_count = num_iter*batch_size\n",
        "step = 0\n",
        "while step < num_iter:\n",
        "  image_batch, label_batch = sess.run([images_test,labels_test])\n",
        "  predictions = sess.run([top_k_op], feed_dict={image_holder:image_batch,label_holder:label_batch})\n",
        "  true_count += np.sum(predictions)\n",
        "  step+=1\n",
        "  \n",
        "  \n",
        "precision = true_count/total_sample_count\n",
        "print('precision @ 1 = %.3f' % precision)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision @ 1 = 0.712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ifm6v1a1plUH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}