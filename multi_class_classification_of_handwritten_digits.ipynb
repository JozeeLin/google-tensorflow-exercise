{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-class_classification_of_handwritten_digits.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/JozeeLin/google-tensorflow-exercise/blob/master/multi_class_classification_of_handwritten_digits.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "ND63J0KcQA-I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 使用神经网络对手写数字进行分类\n",
        "学习目标：\n",
        "  - 训练线性模型和神经网络，以对传统MNIST数据集中的手写数字进行分类\n",
        "  - 比较线性分类模型和神经网络分类模型的效果\n",
        "  \n",
        "我们的目标是将每个输入图片与正确的数字相对应。我们会创建一个包含几个隐藏层的神经网络，并在顶部放置一个归一化指数层，以选出最合适的类别。\n",
        "\n",
        "## 设置\n",
        "加载数据，并随机选取20000行"
      ]
    },
    {
      "metadata": {
        "id": "BICoEl4KPNe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "27d7269a-3f8c-4576-fd95-18ffac45c4e3"
      },
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/mledu-datasets/mnist_train_small.csv -O /tmp/mnist_train_small.csv"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-04-27 13:29:15--  https://storage.googleapis.com/mledu-datasets/mnist_train_small.csv\r\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\r\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36523880 (35M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/mnist_train_small.csv’\n",
            "\n",
            "/tmp/mnist_train_sm 100%[===================>]  34.83M  48.8MB/s    in 0.7s    \n",
            "\n",
            "2018-04-27 13:29:16 (48.8 MB/s) - ‘/tmp/mnist_train_small.csv’ saved [36523880/36523880]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CRrMV3zFQ_8V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNGaNwqeRw9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "bed1c30b-6a88-4087-8cee-d64882a75ef6"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe = pd.read_csv(\n",
        "  io.open(\"/tmp/mnist_train_small.csv\", \"r\"),\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "# Use just the first 10,000 records for training/validation\n",
        "mnist_dataframe = mnist_dataframe.head(10000)\n",
        "\n",
        "mnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
        "mnist_dataframe.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6286</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7882</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3687</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5127</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2512</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1    2    3    4    5    6    7    8    9   ...   775  776  777  \\\n",
              "6286    6    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "7882    6    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "3687    6    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "5127    1    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "2512    7    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "\n",
              "      778  779  780  781  782  783  784  \n",
              "6286    0    0    0    0    0    0    0  \n",
              "7882    0    0    0    0    0    0    0  \n",
              "3687    0    0    0    0    0    0    0  \n",
              "5127    0    0    0    0    0    0    0  \n",
              "2512    0    0    0    0    0    0    0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "FnHvtwGTUqUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_labels_and_features(dataset):\n",
        "  \"\"\"Extracts labels and features.\n",
        "  \n",
        "  This is a good place to scale or transform the features if needed.\n",
        "  \n",
        "  Args:\n",
        "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
        "      monochrome pixel values on the remaining columns, in row major order.\n",
        "  Returns:\n",
        "    A `tuple` `(labels, features)`:\n",
        "      labels: A Pandas `Series`.\n",
        "      features: A Pandas `DataFrame`.\n",
        "  \"\"\"\n",
        "  labels = dataset[0]\n",
        "\n",
        "  # DataFrame.loc index ranges are inclusive at both ends.\n",
        "  features = dataset.loc[:,1:784]\n",
        "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
        "  features = features / 255\n",
        "\n",
        "  return labels, features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OA44_SGBkxzE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "de190eab-c730-4c38-db1b-881c4ac81054"
      },
      "cell_type": "code",
      "source": [
        "training_targets, training_examples = parse_labels_and_features(mnist_dataframe[:7500])\n",
        "training_examples.describe()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>...</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1      2      3      4      5      6      7      8      9      10   \\\n",
              "count 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0   \n",
              "mean     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "       ...      775    776    777    778    779    780    781    782    783  \\\n",
              "count  ...   7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0   \n",
              "mean   ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max    ...      1.0    1.0    0.8    0.2    1.0    0.2    0.0    0.0    0.0   \n",
              "\n",
              "         784  \n",
              "count 7500.0  \n",
              "mean     0.0  \n",
              "std      0.0  \n",
              "min      0.0  \n",
              "25%      0.0  \n",
              "50%      0.0  \n",
              "75%      0.0  \n",
              "max      0.0  \n",
              "\n",
              "[8 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "Ix0kzFZWkzwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "c504c570-7019-4aef-8f69-6ff90c8ab442"
      },
      "cell_type": "code",
      "source": [
        "validation_targets, validation_examples = parse_labels_and_features(mnist_dataframe[7500:10000])\n",
        "validation_examples.describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1      2      3      4      5      6      7      8      9      10   \\\n",
              "count 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0   \n",
              "mean     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "       ...      775    776    777    778    779    780    781    782    783  \\\n",
              "count  ...   2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0   \n",
              "mean   ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max    ...      1.0    0.8    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "         784  \n",
              "count 2500.0  \n",
              "mean     0.0  \n",
              "std      0.0  \n",
              "min      0.0  \n",
              "25%      0.0  \n",
              "50%      0.0  \n",
              "75%      0.0  \n",
              "max      0.0  \n",
              "\n",
              "[8 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "TvMSMISqk1I8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "278f6b49-31f8-4cc1-a975-6c0deb6ce29a"
      },
      "cell_type": "code",
      "source": [
        "rand_example = np.random.choice(training_examples.index)\n",
        "_, ax = plt.subplots()\n",
        "ax.matshow(training_examples.loc[rand_example].values.reshape(28, 28))\n",
        "ax.set_title(\"Label: %i\" % training_targets.loc[rand_example])\n",
        "ax.grid(False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFXCAYAAAAro2x+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPSCoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6bdabd1438>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "mR1X4Yvxk5P4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 任务1：为MNIST构建线性模型\n",
        "首先，我们创建一个基准模型(baseline)。LinearClassifier可提供一组k类一对多分类器，每个分类对应一个分类器。\n",
        "\n",
        "你会发现，除了报告准确率和绘制对数损失函数随时间变化情况的曲线图之外，我们还展示了一个混淆矩阵。混淆矩阵会显示错误分类为其他类别的类别。哪些数字相互之间容易混淆？\n",
        "\n",
        "另外请注意，我们会只用log_loss函数跟踪模型的错误。不应该将此函数与用于训练的LinearClassifier内部损失函数相混淆。"
      ]
    },
    {
      "metadata": {
        "id": "ilSAlWTBk2ub",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def construct_feature_columns():\n",
        "  \"\"\"Construct the TensorFlow Feature Columns.\n",
        "\n",
        "  Returns:\n",
        "    A set of feature columns\n",
        "  \"\"\" \n",
        "  \n",
        "  # There are 784 pixels in each image \n",
        "  return set([tf.feature_column.numeric_column('pixels', shape=784)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-hQxaBAvR7bz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " 在本次练习中，我们会对训练和预测使用单独的输入函数，并将这些函数分别嵌套在 `create_training_input_fn()` 和 `create_predict_input_fn()` 中，这样一来，我们就可以调用这些函数，以返回相应的 `_input_fn`，并将其传递到 `.train()` 和 `.predict()` 调用。"
      ]
    },
    {
      "metadata": {
        "id": "22YGZ3sTR5GB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_training_input_fn(features, labels, batch_size, num_epochs=None, shuffle=True):\n",
        "  \"\"\"A custom input_fn for sending MNIST data to the estimator for training.\n",
        "\n",
        "  Args:\n",
        "    features: The training features.\n",
        "    labels: The training labels.\n",
        "    batch_size: Batch size to use during training.\n",
        "\n",
        "  Returns:\n",
        "    A function that returns batches of training features and labels during\n",
        "    training.\n",
        "  \"\"\"\n",
        "  def _input_fn(num_epochs=None, shuffle=True):\n",
        "    # Input pipelines are reset with each call to .train(). To ensure model\n",
        "    # gets a good sampling of data, even when steps is small, we \n",
        "    # shuffle all the data before creating the Dataset object\n",
        "    idx = np.random.permutation(features.index)\n",
        "    raw_features = {\"pixels\":features.reindex(idx)}\n",
        "    raw_targets = np.array(labels[idx])\n",
        "   \n",
        "    ds = Dataset.from_tensor_slices((raw_features,raw_targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(10000)\n",
        "    \n",
        "    # Return the next batch of data\n",
        "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
        "    return feature_batch, label_batch\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifqTlWXRR-sZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_predict_input_fn(features, labels, batch_size):\n",
        "  \"\"\"A custom input_fn for sending mnist data to the estimator for predictions.\n",
        "\n",
        "  Args:\n",
        "    features: The features to base predictions on.\n",
        "    labels: The labels of the prediction examples.\n",
        "\n",
        "  Returns:\n",
        "    A function that returns features and labels for predictions.\n",
        "  \"\"\"\n",
        "  def _input_fn():\n",
        "    raw_features = {\"pixels\": features.values}\n",
        "    raw_targets = np.array(labels)\n",
        "    \n",
        "    ds = Dataset.from_tensor_slices((raw_features, raw_targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size)\n",
        "    \n",
        "        \n",
        "    # Return the next batch of data\n",
        "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
        "    return feature_batch, label_batch\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAEvMmmsSBT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_linear_classification_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a linear classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, and a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: An `int`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `LinearClassifier` object.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "\n",
        "  steps_per_period = steps / periods  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create a LinearClassifier object.\n",
        "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.LinearClassifier(\n",
        "      feature_columns=construct_feature_columns(),\n",
        "      n_classes=10,\n",
        "      optimizer=my_optimizer,\n",
        "      config=tf.estimator.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "  \n",
        "    # Take a break and compute probabilities.\n",
        "    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "    training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
        "        \n",
        "    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "    validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
        "    \n",
        "    # Compute training and validation errors.\n",
        "    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_errors.append(training_log_loss)\n",
        "    validation_errors.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class)\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CwByedMhSRtv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "在本次练习中，为自己设定限制，仅使用批量大小、学习速率和步数这三个超参数进行试验。\n"
      ]
    },
    {
      "metadata": {
        "id": "1rTlqArISQL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "ba4d185d-cb09-487b-aa9f-b0ddbc7e459d"
      },
      "cell_type": "code",
      "source": [
        "classifier = train_linear_classification_model(\n",
        "             learning_rate=0.02,\n",
        "             steps=100,\n",
        "             batch_size=10,\n",
        "             training_examples=training_examples,\n",
        "             training_targets=training_targets,\n",
        "             validation_examples=validation_examples,\n",
        "             validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "LogLoss error (on validation data):\n",
            "  period 00 : 16.91\n",
            "  period 01 : 17.46\n",
            "  period 02 : 13.59\n",
            "  period 03 : 8.81\n",
            "  period 04 : 7.32\n",
            "  period 05 : 7.96\n",
            "  period 06 : 7.02\n",
            "  period 07 : 6.40\n",
            "  period 08 : 6.04\n",
            "  period 09 : 6.29\n",
            "Model training finished.\n",
            "Final accuracy (on validation data): 0.82\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFnCAYAAACLnxFFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+ZsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6bd28ec6d8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "AddEoYmcSgqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_ = train_linear_classification_model(\n",
        "    learning_rate=0.03,\n",
        "    steps=1000,\n",
        "    batch_size=30,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X0TN3l1cS7BI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 任务2：使用神经网络替换线性分类器\n",
        "使用DNNClassifier替换上面的LinearClassifier，并查找可现实0.95或更高准确率的参数组合\n",
        "\n",
        "可以尝试Dropout等其他正则化方法。这些额外的正则化方法已记录在DNNClassifier类的注释中。"
      ]
    },
    {
      "metadata": {
        "id": "Q41Wfz0ASrsd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_nn_classification_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    hidden_units,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a neural network classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, as well as a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: An `int`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `DNNClassifier` object.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  # Caution: input pipelines are reset with each call to train. \n",
        "  # If the number of steps is small, your model may never see most of the data.  \n",
        "  # So with multiple `.train` calls like this you may want to control the length \n",
        "  # of training with num_epochs passed to the input_fn. Or, you can do a really-big shuffle, \n",
        "  # or since it's in-memory data, shuffle all the data in the `input_fn`.\n",
        "  steps_per_period = steps / periods  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create feature columns.\n",
        "  feature_columns = [tf.feature_column.numeric_column('pixels', shape=784)]\n",
        "\n",
        "  # Create a DNNClassifier object.\n",
        "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.DNNClassifier(\n",
        "      feature_columns=feature_columns,\n",
        "      n_classes=10,\n",
        "      hidden_units=hidden_units,\n",
        "      optimizer=my_optimizer,\n",
        "      config=tf.contrib.learn.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "  \n",
        "    # Take a break and compute probabilities.\n",
        "    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "    training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
        "        \n",
        "    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "    validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
        "    \n",
        "    # Compute training and validation errors.\n",
        "    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_errors.append(training_log_loss)\n",
        "    validation_errors.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class)\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9GsqRQUYUXK9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = train_nn_classification_model(\n",
        "    learning_rate=0.05,\n",
        "    steps=1000,\n",
        "    batch_size=30,\n",
        "    hidden_units=[100, 100],\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "taDdQFA1UZJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/mledu-datasets/mnist_test.csv -O /tmp/mnist_test.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OouQRzUbUcR9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_test_dataframe = pd.read_csv(\n",
        "  io.open(\"/tmp/mnist_test.csv\", \"r\"),\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
        "test_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TxrfSZC8UeKl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_test_input_fn = create_predict_input_fn(\n",
        "    test_examples, test_targets, batch_size=100)\n",
        "\n",
        "test_predictions = classifier.predict(input_fn=predict_test_input_fn)\n",
        "test_predictions = np.array([item['class_ids'][0] for item in test_predictions])\n",
        "  \n",
        "accuracy = metrics.accuracy_score(test_targets, test_predictions)\n",
        "print(\"Accuracy on test data: %0.2f\" % accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96SoM-3wUloX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 任务3：可视化第一个隐藏层的权重\n",
        "我们花几分钟时间看看模型的weights_属性，以深入探索我们的神经网络，并了解它学到了哪些规律。\n",
        "\n",
        "模型的输入层有784个权重，对应于28x28像素输入图片。第一个隐藏层将有784xN个权重，其中N指的是该层中的节点数。我们可以将这些权重重新变回28x28像素的图片，具体方法是将N个1x784权重数组变形为N个28x28大小数组。\n",
        "\n",
        "运行一下单元格，绘制权重曲线图。请注意此单元格要求名为\"classifier\"的DNNClassifier已经过训练。"
      ]
    },
    {
      "metadata": {
        "id": "3CLigR2rUifj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(classifier.get_variable_names())\n",
        "\n",
        "weights0 = classifier.get_variable_value(\"dnn/hiddenlayer_0/kernel\")\n",
        "\n",
        "print(\"weights0 shape:\", weights0.shape)\n",
        "\n",
        "num_nodes = weights0.shape[1]\n",
        "num_rows = int(math.ceil(num_nodes / 10.0))\n",
        "fig, axes = plt.subplots(num_rows, 10, figsize=(20, 2 * num_rows))\n",
        "for coef, ax in zip(weights0.T, axes.ravel()):\n",
        "    # Weights in coef is reshaped from 1x784 to 28x28.\n",
        "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.pink)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "upYAgAbBWFxJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " 神经网络的第一个隐藏层应该会对一些级别特别低的特征进行建模，因此可视化权重可能只显示一些模糊的区域，也可能只显示数字的某几个部分。此外，您可能还会看到一些基本上是噪点（这些噪点要么不收敛，要么被更高的层忽略）的神经元。\n",
        "\n",
        "在迭代不同的次数后停止训练并查看效果，可能会发现有趣的结果。\n",
        "\n",
        "**分别用 10、100 和 1000 步训练分类器。然后重新运行此可视化。**\n",
        "\n",
        "您看到不同级别的收敛之间有哪些直观上的差异？"
      ]
    },
    {
      "metadata": {
        "id": "DlP5rHpVWDq2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}