{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "经典卷积神经网络-AlexNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/JozeeLin/google-tensorflow-exercise/blob/master/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_AlexNet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "euinGF00qXt6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "本章将介绍4种经典的卷积神经网络，分别是AlexNet、VGGNet、Google Inception Net、ResNet，这4中网络依照出现的先后顺序排列，深度和复杂度也依次递进。\n",
        "\n",
        "## AlexNet\n",
        "AlexNet主要使用到的新技术点如下：\n",
        "- 成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了sigmoid，成功解决了sigmoid在网络较深时的梯度弥散问题。\n",
        "- 训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。\n",
        "- 在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化和的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性\n",
        "- 提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力\n",
        "- 使用CUDA加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算。\n",
        "- 数据增强，随机的从256x256的原始图像中街区224x224大小的区域(以及水平翻转的镜像)，相当于增加了$(256-224)^2x2=2048$倍的数据量。防止过拟合\n"
      ]
    },
    {
      "metadata": {
        "id": "jEf8DA9OqOwB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import math\n",
        "import time\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "toCHM_LRuRTN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_batches=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzLRfpRxujNy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#显示网络每一层结构的函数print_actications，展示每一个卷积层或池化层输出tensor的尺寸\n",
        "def print_activations(t):\n",
        "  print(t.op.name, ' ', t.get_shape().as_list())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mvKqH9MPvAq-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 设计AlexNet的网络结构"
      ]
    },
    {
      "metadata": {
        "id": "UHMUFuAvvAPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4c296017-61ea-4674-aaeb-b2cce298229d"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "#接受images作为输入，返回最后一层pool5(第5个池化层)及parameters(AlexNet中所有需要训练的模型参数)\n",
        "\n",
        "def inference(images):\n",
        "  parameters = []\n",
        "  \n",
        "  with tf.name_scope('conv1') as scope:\n",
        "    kernel = tf.Variable(tf.truncated_normal([11,11,3,64],\n",
        "                                            dtype=tf.float32, stddev=1e-1), name='weights')\n",
        "    conv = tf.nn.conv2d(images, kernel,[1,4,4,1], padding='SAME')\n",
        "    biases = tf.Variable(tf.constant(0.0, shape=[64],dtype=tf.float32),trainable=True, name='biases')\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv1 = tf.nn.relu(bias, name=scope)# 使用relu激活函数对结果进行非线性处理\n",
        "    print_activations(conv1)#输出卷积层的结构\n",
        "    parameters += [kernel, biases] #把这一层可训练的参数添加到parameters中\n",
        "    \n",
        "    #在第一个卷积层后再添加LRN层和最大池化层\n",
        "    lrn1 = tf.nn.lrn(conv, 4, bias=1.0, alpha=0.001/9, beta=0.75, name='lrn1')#对前面输出的tensor conv1进行LRN处理\n",
        "    pool1 = tf.nn.max_pool(lrn1, ksize=[1,3,3,1], strides=[1,2,2,1],padding='VALID',name='pool1')#最大池化处理，padding为VALID，即取样时不能超过边框，不想SAME模式那样可以填充边界外的点\n",
        "    print_activations(pool1)#可视化池化结果\n",
        "    \n",
        "    \n",
        "  #设计第二个卷积层  \n",
        "  with tf.name_scope('conv2') as scope:\n",
        "    kernel = tf.Variable(tf.truncated_normal([5,5,64,192],\n",
        "                                            dtype=tf.float32, stddev=1e-1),name='weights')\n",
        "    conv = tf.nn.conv2d(pool1, kernel, [1,1,1,1], padding='SAME')\n",
        "    biases = tf.Variable(tf.constant(0.0, shape=[192],\n",
        "                                    dtype=tf.float32), trainable=True, name='biases')\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv2 = tf.nn.relu(bias, name=scope)\n",
        "    parameters += [kernel, biases]\n",
        "    print_activations(conv2)\n",
        "  \n",
        "    #对第2个卷积层的输出conv2进行处理，同样是先做LRN处理，在进行最大池化处理，参数和之前完全一样\n",
        "    lrn2 = tf.nn.lrn(conv2, 4,bias=1.0,alpha=0.001/9, beta=0.75, name='lrn2')\n",
        "    pool2 = tf.nn.max_pool(lrn2, ksize=[1,3,3,1], strides=[1,2,2,1],\n",
        "                        padding='VALID', name='pool2')\n",
        "    print_activations(pool2)\n",
        "  \n",
        "  #创建第三层卷积层\n",
        "  with tf.name_scope('conv3') as scope:\n",
        "    kernel = tf.Variable(tf.truncated_normal([3,3,192,384],\n",
        "                                            dtype=tf.float32, stddev=1e-1), name='weights')\n",
        "    conv = tf.nn.conv2d(pool2, kernel,[1,1,1,1], padding='SAME')\n",
        "    biases = tf.Variable(tf.constant(0.0, shape=[384],\n",
        "                                    dtype=tf.float32),trainable=True, name='biases')\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv3 = tf.nn.relu(bias, name=scope)\n",
        "    parameters += [kernel, biases]\n",
        "    print_activations(conv3)\n",
        "  \n",
        "  #第四个卷积层\n",
        "  with tf.name_scope('conv4') as scope:\n",
        "    kernel = tf.Variable(tf.truncated_normal([3,3,384,256],\n",
        "                                            dtype=tf.float32, stddev=1e-1),name='weights')\n",
        "    conv = tf.nn.conv2d(conv3, kernel, [1,1,1,1], padding='SAME')\n",
        "    biases = tf.Variable(tf.constant(0.0, shape=[256],\n",
        "                                    dtype=tf.float32), trainable=True, name='biases')\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv4 = tf.nn.relu(bias, name=scope)\n",
        "    parameters += [kernel, biases]\n",
        "    print_activations(conv4)\n",
        "    \n",
        "  #第五个卷积层\n",
        "  with tf.name_scope('conv5') as scope:\n",
        "    kernel = tf.Variable(tf.truncated_normal([3,3,256,256],\n",
        "                        dtype=tf.float32, stddev=1e-1),name='weights')\n",
        "    conv = tf.nn.conv2d(conv4, kernel, [1,1,1,1], padding='SAME')\n",
        "    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases')\n",
        "    bias = tf.nn.bias_add(conv, biases)\n",
        "    conv5 = tf.nn.relu(bias, name=scope)\n",
        "    parameters += [kernel, biases]\n",
        "    print_activations(conv5)\n",
        "    \n",
        "    pool5 = tf.nn.max_pool(conv5, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID', name='pool5')\n",
        "    print_activations(pool5)\n",
        "  \n",
        "  return pool5,parameters\n",
        "  '''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#接受images作为输入，返回最后一层pool5(第5个池化层)及parameters(AlexNet中所有需要训练的模型参数)\\n\\ndef inference(images):\\n  parameters = []\\n  \\n  with tf.name_scope('conv1') as scope:\\n    kernel = tf.Variable(tf.truncated_normal([11,11,3,64],\\n                                            dtype=tf.float32, stddev=1e-1), name='weights')\\n    conv = tf.nn.conv2d(images, kernel,[1,4,4,1], padding='SAME')\\n    biases = tf.Variable(tf.constant(0.0, shape=[64],dtype=tf.float32),trainable=True, name='biases')\\n    bias = tf.nn.bias_add(conv, biases)\\n    conv1 = tf.nn.relu(bias, name=scope)# 使用relu激活函数对结果进行非线性处理\\n    print_activations(conv1)#输出卷积层的结构\\n    parameters += [kernel, biases] #把这一层可训练的参数添加到parameters中\\n    \\n    #在第一个卷积层后再添加LRN层和最大池化层\\n    lrn1 = tf.nn.lrn(conv, 4, bias=1.0, alpha=0.001/9, beta=0.75, name='lrn1')#对前面输出的tensor conv1进行LRN处理\\n    pool1 = tf.nn.max_pool(lrn1, ksize=[1,3,3,1], strides=[1,2,2,1],padding='VALID',name='pool1')#最大池化处理，padding为VALID，即取样时不能超过边框，不想SAME模式那样可以填充边界外的点\\n    print_activations(pool1)#可视化池化结果\\n    \\n    \\n  #设计第二个卷积层  \\n  with tf.name_scope('conv2') as scope:\\n    kernel = tf.Variable(tf.truncated_normal([5,5,64,192],\\n                                            dtype=tf.float32, stddev=1e-1),name='weights')\\n    conv = tf.nn.conv2d(pool1, kernel, [1,1,1,1], padding='SAME')\\n    biases = tf.Variable(tf.constant(0.0, shape=[192],\\n                                    dtype=tf.float32), trainable=True, name='biases')\\n    bias = tf.nn.bias_add(conv, biases)\\n    conv2 = tf.nn.relu(bias, name=scope)\\n    parameters += [kernel, biases]\\n    print_activations(conv2)\\n  \\n    #对第2个卷积层的输出conv2进行处理，同样是先做LRN处理，在进行最大池化处理，参数和之前完全一样\\n    lrn2 = tf.nn.lrn(conv2, 4,bias=1.0,alpha=0.001/9, beta=0.75, name='lrn2')\\n    pool2 = tf.nn.max_pool(lrn2, ksize=[1,3,3,1], strides=[1,2,2,1],\\n                        padding='VALID', name='pool2')\\n    print_activations(pool2)\\n  \\n  #创建第三层卷积层\\n  with tf.name_scope('conv3') as scope:\\n    kernel = tf.Variable(tf.truncated_normal([3,3,192,384],\\n                                            dtype=tf.float32, stddev=1e-1), name='weights')\\n    conv = tf.nn.conv2d(pool2, kernel,[1,1,1,1], padding='SAME')\\n    biases = tf.Variable(tf.constant(0.0, shape=[384],\\n                                    dtype=tf.float32),trainable=True, name='biases')\\n    bias = tf.nn.bias_add(conv, biases)\\n    conv3 = tf.nn.relu(bias, name=scope)\\n    parameters += [kernel, biases]\\n    print_activations(conv3)\\n  \\n  #第四个卷积层\\n  with tf.name_scope('conv4') as scope:\\n    kernel = tf.Variable(tf.truncated_normal([3,3,384,256],\\n                                            dtype=tf.float32, stddev=1e-1),name='weights')\\n    conv = tf.nn.conv2d(conv3, kernel, [1,1,1,1], padding='SAME')\\n    biases = tf.Variable(tf.constant(0.0, shape=[256],\\n                                    dtype=tf.float32), trainable=True, name='biases')\\n    bias = tf.nn.bias_add(conv, biases)\\n    conv4 = tf.nn.relu(bias, name=scope)\\n    parameters += [kernel, biases]\\n    print_activations(conv4)\\n    \\n  #第五个卷积层\\n  with tf.name_scope('conv5') as scope:\\n    kernel = tf.Variable(tf.truncated_normal([3,3,256,256],\\n                        dtype=tf.float32, stddev=1e-1),name='weights')\\n    conv = tf.nn.conv2d(conv4, kernel, [1,1,1,1], padding='SAME')\\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases')\\n    bias = tf.nn.bias_add(conv, biases)\\n    conv5 = tf.nn.relu(bias, name=scope)\\n    parameters += [kernel, biases]\\n    print_activations(conv5)\\n    \\n    pool5 = tf.nn.max_pool(conv5, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID', name='pool5')\\n    print_activations(pool5)\\n  \\n  return pool5,parameters\\n  \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "YP121y9CfuNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inference(images):\n",
        "    parameters = []\n",
        "    \n",
        "    with tf.name_scope('conv1') as scope:\n",
        "        kernel = tf.Variable(tf.truncated_normal([11,11,3,64],\n",
        "                                                 stddev=1e-1, dtype=tf.float32,name='weights'))\n",
        "        conv = tf.nn.conv2d(images,kernel,[1,4,4,1],padding='SAME')\n",
        "        biases = tf.Variable(tf.constant(0.0,shape=[64], dtype=tf.float32, \n",
        "                                           name='biaes'))\n",
        "        \n",
        "        bias = tf.nn.bias_add(conv,biases)\n",
        "        \n",
        "        conv1 = tf.nn.relu(bias, name=scope)\n",
        "        print_activations(conv1)\n",
        "        parameters += [kernel,biases]\n",
        "        \n",
        "        \n",
        "        lrn1 = tf.nn.lrn(conv1, 4, bias=1.0, alpha=0.001/9, beta=0.75, name='lrn1')\n",
        "        pool1 = tf.nn.max_pool(lrn1, ksize=[1,3,3,1], strides=[1,2,2,1],\n",
        "                                padding='VALID', name='pool1')\n",
        "        \n",
        "        print_activations(pool1)\n",
        "        \n",
        "    with tf.name_scope('conv2') as scope:\n",
        "        kernel = tf.Variable(tf.truncated_normal([5,5,64,192],\n",
        "                                                 stddev=1e-1, dtype=tf.float32,name='weights'))\n",
        "        conv = tf.nn.conv2d(pool1,kernel,[1,1,1,1],padding='SAME')\n",
        "        biases = tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[192], name='biaes'))\n",
        "        \n",
        "        bias = tf.nn.bias_add(conv,biases)\n",
        "        \n",
        "        conv2 = tf.nn.relu(bias, name=scope)\n",
        "        \n",
        "        parameters += [kernel,biases]\n",
        "        print_activations(conv2)    \n",
        "            \n",
        "        lrn2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001/9, beta=0.75, name='lrn2')\n",
        "        pool2 = tf.nn.max_pool(lrn2, ksize=[1,3,3,1], strides=[1,2,2,1],\n",
        "                                padding='VALID', name='pool2')\n",
        "        \n",
        "        print_activations(pool2)\n",
        "      \n",
        "\n",
        "    with tf.name_scope('conv3') as scope:\n",
        "        kernel = tf.Variable(tf.truncated_normal([3,3,192,384],\n",
        "                                                 stddev=1e-1, dtype=tf.float32,name='weights'))\n",
        "        conv = tf.nn.conv2d(pool2,kernel,[1,1,1,1],padding='SAME')\n",
        "        biases = tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[384], name='biaes'))\n",
        "        \n",
        "        bias = tf.nn.bias_add(conv,biases)\n",
        "        \n",
        "        conv3 = tf.nn.relu(bias, name=scope)\n",
        "        \n",
        "        parameters += [kernel,biases]\n",
        "        print_activations(conv3)    \n",
        "            \n",
        "    \n",
        "    with tf.name_scope('conv4') as scope:\n",
        "        kernel = tf.Variable(tf.truncated_normal([3,3,384,256],\n",
        "                                                 stddev=1e-1, dtype=tf.float32,name='weights'))\n",
        "        conv = tf.nn.conv2d(conv3,kernel,[1,1,1,1],padding='SAME')\n",
        "        biases = tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[256],\n",
        "                                          name='biaes'))\n",
        "        \n",
        "        bias = tf.nn.bias_add(conv,biases)\n",
        "        \n",
        "        conv4 = tf.nn.relu(bias, name=scope)\n",
        "        \n",
        "        parameters += [kernel,biases]\n",
        "        print_activations(conv4)    \n",
        "            \n",
        "    with tf.name_scope('conv5') as scope:\n",
        "        kernel = tf.Variable(tf.truncated_normal([3,3,256,256],\n",
        "                                                 stddev=1e-1, dtype=tf.float32,name='weights'))\n",
        "        conv = tf.nn.conv2d(conv4,kernel,[1,1,1,1],padding='SAME')\n",
        "        biases = tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[256],\n",
        "                                          name='biaes'))\n",
        "        \n",
        "        bias = tf.nn.bias_add(conv,biases)\n",
        "        \n",
        "        conv5 = tf.nn.relu(bias, name=scope)\n",
        "        \n",
        "        parameters += [kernel,biases]\n",
        "        print_activations(conv5)   \n",
        "        \n",
        "        pool5 = tf.nn.max_pool(conv5, ksize=[1,3,3,1], strides=[1,2,2,1],\n",
        "                                    padding='VALID', name='pool5')\n",
        "            \n",
        "        print_activations(pool5)\n",
        "        \n",
        "    return pool5,parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkyQhkXIu4g7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def time_tensorflow_run(session,target,info_string):\n",
        "    num_steps_burn_in = 10\n",
        "    total_duration = 0.0\n",
        "    total_duration_squared = 0.0\n",
        "    \n",
        "    for i in range(num_batches + num_steps_burn_in):\n",
        "        start_time = time.time()\n",
        "        _ = session.run(target)\n",
        "        duration = time.time() - start_time\n",
        "        \n",
        "        if i >= num_steps_burn_in:\n",
        "            if not i % 10:\n",
        "                print('%s:step %d,duration = %.3f' %(datetime.now(),i - num_steps_burn_in,duration))\n",
        "            \n",
        "            total_duration += duration\n",
        "            total_duration_squared += duration * duration\n",
        "    mn = total_duration / num_batches\n",
        "    vr = total_duration_squared / num_batches - mn * mn\n",
        "    sd = math.sqrt(vr)\n",
        "    print('%s:%s across %d steps,%.3f +/- %.3f sec / batch'%\n",
        "          (datetime.now(),info_string,num_batches,mn,sd))\n",
        "    \n",
        "def run_benchmark():\n",
        "  #主函数run_benchmark\n",
        "    with tf.Graph().as_default():\n",
        "        image_size = 224\n",
        "        #使用random_normal函数构造正态分布(标准差为0.1)的随机tensor\n",
        "        images = tf.Variable(tf.random_normal([batch_size,\n",
        "                                               image_size,\n",
        "                                               image_size,3],\n",
        "                                               stddev=1e-1,\n",
        "                                               dtype=tf.float32))\n",
        "        pool5,parameters = inference(images)\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess = tf.Session()\n",
        "        sess.run(init)\n",
        "        \n",
        "        time_tensorflow_run(sess, pool5, \"Forword\")\n",
        "        objective = tf.nn.l2_loss(pool5)\n",
        "        grad = tf.gradients(objective,parameters)\n",
        "        time_tensorflow_run(sess, grad, \"Forword-backward\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97i4hDfTWTT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "78574097-66d7-43e3-cf8a-c190749a64c5"
      },
      "cell_type": "code",
      "source": [
        "run_benchmark()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1   [32, 56, 56, 64]\n",
            "conv1/pool1   [32, 27, 27, 64]\n",
            "conv2   [32, 27, 27, 192]\n",
            "conv2/pool2   [32, 13, 13, 192]\n",
            "conv3   [32, 13, 13, 384]\n",
            "conv4   [32, 13, 13, 256]\n",
            "conv5   [32, 13, 13, 256]\n",
            "conv5/pool5   [32, 6, 6, 256]\n",
            "2018-05-03 10:14:31.962916:step 0,duration = 0.049\n",
            "2018-05-03 10:14:32.441134:step 10,duration = 0.047\n",
            "2018-05-03 10:14:32.915056:step 20,duration = 0.047\n",
            "2018-05-03 10:14:33.390633:step 30,duration = 0.048\n",
            "2018-05-03 10:14:33.865000:step 40,duration = 0.047\n",
            "2018-05-03 10:14:34.339283:step 50,duration = 0.047\n",
            "2018-05-03 10:14:34.813369:step 60,duration = 0.047\n",
            "2018-05-03 10:14:35.287235:step 70,duration = 0.048\n",
            "2018-05-03 10:14:35.760969:step 80,duration = 0.047\n",
            "2018-05-03 10:14:36.235670:step 90,duration = 0.047\n",
            "2018-05-03 10:14:36.662592:Forword across 100 steps,0.047 +/- 0.000 sec / batch\n",
            "2018-05-03 10:14:38.462426:step 0,duration = 0.134\n",
            "2018-05-03 10:14:39.813483:step 10,duration = 0.136\n",
            "2018-05-03 10:14:41.165236:step 20,duration = 0.135\n",
            "2018-05-03 10:14:42.514956:step 30,duration = 0.135\n",
            "2018-05-03 10:14:43.864355:step 40,duration = 0.135\n",
            "2018-05-03 10:14:45.212588:step 50,duration = 0.135\n",
            "2018-05-03 10:14:46.562215:step 60,duration = 0.135\n",
            "2018-05-03 10:14:47.911919:step 70,duration = 0.135\n",
            "2018-05-03 10:14:49.263379:step 80,duration = 0.135\n",
            "2018-05-03 10:14:50.613461:step 90,duration = 0.135\n",
            "2018-05-03 10:14:51.827298:Forword-backward across 100 steps,0.135 +/- 0.000 sec / batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eRRvF08PZlM0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}