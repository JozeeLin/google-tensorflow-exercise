{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/JozeeLin/google-tensorflow-exercise/blob/master/word2vec.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "XHfJ0Jel4aEW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "word2vec是将语言中的字词转化为计算机可以理解的稠密向量(Dense Vector),进而可以做其他自然语言处理任务，比如文本分类、词性标注、机器翻译等\n",
        "\n",
        "Word2Vec也称Word Embedding,中文也有很多叫法，比如词向量或词嵌入。Word2Vec是一个可以将语言中字词转化为向量形式表达的模型，我们先来看看为什么要把字词转为向量。\n",
        "\n",
        "在Word2Vec出现之前，自然语言处理通常将字词转成离散的单独的符号，如词袋模型，使用的独热编码技术。\n",
        "\n",
        "**向量空间模型可以将字词转为连续值(相对于独热编码的离散值)的向量表达，并且其中意思相近的词将被映射到向量空间中相近的位置。**\n",
        "\n",
        "向量空间模型的假设前提是:Distributional Hypothesis,即在相同语境中出现的词其语义也相近。\n",
        "\n",
        "向量空间模型大致分成两类，一类是计数模型，比如Latent Semantic Analysis；另一类是预测模型(比如Neural Probabilistic Language Models)。\n",
        "\n",
        "计数模型统计在语料库中，相邻出现的词的频率，再把这些计数统计结果转为小而稠密的矩阵；而预测模型则根据一个词周围相邻的词推测出这个词，以及它的空间向量。\n",
        "\n",
        "Word2vec 是一种计算非常高效的，可以从原始语料中学习字词空间向量的预测模型。它主要分为CBOW和Skip-Gram两种模式，其中CBOW是从原始语句推测目标字词，\n",
        "\n",
        "而skip-gram则正好相反，它是从目标字词推测出原始语句。其中CBOW对小型数据比较合适，而Skip-Gram在大型语料中表现得更好。\n",
        "\n",
        "这里我们主要使用skip-gram模式的word2vec"
      ]
    },
    {
      "metadata": {
        "id": "OAv7U2GV4VIy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import urllib\n",
        "import tensorflow as tf\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hAWH_5wTA2Mm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cea203f8-7c76-4391-a715-35037beeef0b"
      },
      "cell_type": "code",
      "source": [
        "url = 'http://mattmahoney.net/dc/'\n",
        "\n",
        "def maybe_download(filename, expected_bytes):\n",
        "  if not os.path.exists(filename):\n",
        "    filename, _ = urllib.request.urlretrieve(url+filename, filename)\n",
        "  statinfo = os.stat(filename)\n",
        "  \n",
        "  if statinfo.st_size == expected_bytes:\n",
        "    print('Found and Verified', filename)\n",
        "  else:\n",
        "    print(statinfo.st_size)\n",
        "    raise Exception('Failed to verify '+filename+'. Can you get to it with a browser?')\n",
        "    \n",
        "  return filename\n",
        "\n",
        "filename = maybe_download('text8.zip', 31344016)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and Verified text8.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-mxAXPH7BpTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e1aa9d5-e5d9-4421-ea12-fa8357041115"
      },
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "  with zipfile.ZipFile(filename) as f:\n",
        "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "    \n",
        "  return data\n",
        "\n",
        "words = read_data(filename)\n",
        "print('Data size', len(words))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size 17005207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hkLMqUtRB6fK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocabulary_size = 50000\n",
        "#创建词汇表\n",
        "def build_dataset(words):\n",
        "  count = [['UNK',-1]]\n",
        "  #统计单词列表中单词的频数，然后使用most_common方法提取top 50000频数的单词作为词汇表\n",
        "  count.extend(collections.Counter(words).most_common(vocabulary_size-1)) \n",
        "  #将top 50000词汇的vocabulary放入dictionary中，以便快速查询，时间复杂度为O(1)\n",
        "  dictionary = dict()#词汇表\n",
        "  for word, _ in count:\n",
        "    dictionary[word] = len(dictionary) #使用当前的字典长度作为索引号\n",
        "    \n",
        "  data = list() #字典对应的编码列表\n",
        "  unk_count = 0\n",
        "  \n",
        "  #将所有的单词转成索引编号\n",
        "  for word in words:\n",
        "    if word in dictionary:\n",
        "      index = dictionary[word]\n",
        "    else:\n",
        "      index = 0\n",
        "      unk_count += 1\n",
        "    data.append(index)\n",
        "    \n",
        "  count[0][1] = unk_count #top 50000单词的频数统计以及其他unk词的统计\n",
        "  \n",
        "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))#词典的反转形式\n",
        "  \n",
        "  return data, count, dictionary, reverse_dictionary\n",
        "\n",
        "data,count,dictionary,reverse_dictionary = build_dataset(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gte13XJtiyy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "920cd605-cecc-4318-f0cd-f2b2a6957680"
      },
      "cell_type": "code",
      "source": [
        "del words #删除原始单词列表，可以节省内存\n",
        "\n",
        "print('Most common words (+UNK)', count[:5])\n",
        "print('Sample data', data[:10],[reverse_dictionary[i] for i in data[:10]])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
            "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MrsZNcfVjFyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#生成Word2Vec训练样本，根据Skip-Gram模式(从目标单词反推语境)\n",
        "\n",
        "data_index = 0\n",
        "\n",
        "def generate_batch(batch_size, num_skips, skip_window):\n",
        "  '''\n",
        "  用来生成训练用的batch数据，skip_window指单词最远可以联系的距离，设为1代表只能跟紧邻的两个单词生成样本\n",
        "  '''\n",
        "  global data_index #确保data_index可以在函数generate_batch中被修改\n",
        "  \n",
        "  assert batch_size % num_skips==0\n",
        "  #num_skips是对每个单词生成多少个样本，它不能大于skip_window值得两倍，并且batch_size必须是它的整数倍(确保每个batch包含了一个词汇对应的所有样本)\n",
        "  assert num_skips<=2 * skip_window \n",
        "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "  #span为对某个单词创建相关样本时会使用到的单词数量，包括目标单词本身和它前后的单词\n",
        "  span = 2*skip_window+1\n",
        "  buffer = collections.deque(maxlen=span) #创建一个双向队列\n",
        "  \n",
        "  #从序号data_index开始，把span个单词顺序读入buffer作为初始值。因为buffer是容量为span的deque\n",
        "  for _ in range(span):\n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index+1)%len(data)\n",
        "    \n",
        "  for i in range(batch_size // num_skips):\n",
        "    target = skip_window\n",
        "    targets_to_avoid = [skip_window]\n",
        "    for j in range(num_skips):\n",
        "      while target in targets_to_avoid:\n",
        "        target = random.randint(0, span-1)\n",
        "        \n",
        "      targets_to_avoid.append(target)\n",
        "      batch[i*num_skips+j] = buffer[skip_window]\n",
        "      labels[i*num_skips+j,0] = buffer[target]\n",
        "      \n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index+1) % len(data)\n",
        "  return batch, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xD33lSJqpVvA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "35ce33e9-f0d6-4651-875e-aff2196e039b"
      },
      "cell_type": "code",
      "source": [
        "#在这里简单调用一下generate_batch函数简单测试一下其功能。\n",
        "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
        "for i in range(8):\n",
        "  print(batch[i], reverse_dictionary[batch[i]], '->', labels[i,0],reverse_dictionary[labels[i,0]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3081 originated -> 12 as\n",
            "3081 originated -> 5234 anarchism\n",
            "12 as -> 6 a\n",
            "12 as -> 3081 originated\n",
            "6 a -> 12 as\n",
            "6 a -> 195 term\n",
            "195 term -> 2 of\n",
            "195 term -> 6 a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ArS_A-Fkp5aI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "embedding_size = 128\n",
        "skip_window = 1\n",
        "num_skips = 2\n",
        "\n",
        "valid_size = 16\n",
        "valid_window = 100\n",
        "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
        "num_sampled = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gjAdw73GqvWD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8110
        },
        "outputId": "2447631b-3979-41ee-d79a-e46014a52475"
      },
      "cell_type": "code",
      "source": [
        "# skip-Gram Word2Vec模型的网络结构\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "  train_labels = tf.placeholder(tf.int32, shape=[batch_size,1])\n",
        "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "  \n",
        "  with tf.device('/cpu:0'):\n",
        "    embeddings = tf.Variable(\n",
        "      tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
        "    )\n",
        "  embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
        "  \n",
        "  nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                                               stddev=1.0/math.sqrt(embedding_size)))\n",
        "  \n",
        "  nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "  \n",
        "  loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
        "                                    biases=nce_biases,\n",
        "                                    labels=train_labels,\n",
        "                                    inputs=embed,\n",
        "                                    num_sampled=num_sampled,\n",
        "                                    num_classes=vocabulary_size))\n",
        "  \n",
        "  #定义优化器为SGD，且学习率为1.0。然后计算嵌入向量embeddings的L2范数norm，再将embedding除以其L2范数得到标准化后的normalized_embeddings\n",
        "  #再使用tf.nn.embedding_lookup查询验证单词的嵌入向量，并计算验证单词的嵌入向量与词汇表中所有单词的相似性。\n",
        "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
        "\n",
        "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims=True))\n",
        "  normalized_embeddings = embeddings/norm\n",
        "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "  similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
        "  \n",
        "  init = tf.global_variables_initializer()\n",
        "  \n",
        "  #定义最大的迭代次数为10万次\n",
        "  num_steps = 1000001\n",
        "  \n",
        "  with tf.Session(graph=graph) as session:\n",
        "    init.run()\n",
        "    print('Initialized')\n",
        "    \n",
        "    average_loss=0\n",
        "    for step in range(num_steps):\n",
        "      batch_inputs,batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
        "      feed_dict = {train_inputs:batch_inputs, train_labels:batch_labels}\n",
        "      \n",
        "      _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "      average_loss += loss_val\n",
        "      #之后每2000次循环，计算一下平均loss并显示出来\n",
        "      if step % 2000 == 0:\n",
        "        if step > 0:\n",
        "          average_loss /= 2000\n",
        "        print('Average loss at step ', step, \": \", average_loss)\n",
        "        average_loss = 0\n",
        "      #每10000次循环，计算一次验证单词与全部单词的相似度，并将与每个验证单词最相似的8个单词展示出来。\n",
        "      if step%10000 == 0:\n",
        "        if step > 0:\n",
        "          average_loss /= 2000\n",
        "        print('Average loss at step ', step, \": \", average_loss)\n",
        "        average_loss = 0\n",
        "      #每10000次循环，计算一次验证单词与全部单词的相似度，并将与每个验证单词最相似的8个单词展示出来。\n",
        "      if step%10000 == 0:\n",
        "        sim = similarity.eval()\n",
        "        for i in range(valid_size):\n",
        "          valid_word = reverse_dictionary[valid_examples[i]]\n",
        "          top_k = 8\n",
        "          nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
        "          log_str = 'Nearest to %s:' % valid_word\n",
        "          for k in range(top_k):\n",
        "            close_word = reverse_dictionary[nearest[k]]\n",
        "            log_str = '%s %s,'%(log_str, close_word)\n",
        "          print(log_str)\n",
        "      final_embeddings = normalized_embeddings.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-52e1eaac24a9>:30: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "Initialized\n",
            "Average loss at step  0 :  283.8922119140625\n",
            "Average loss at step  0 :  0\n",
            "Nearest to often: prana, outflow, vos, management, spontaneity, prerequisite, indus, adrian,\n",
            "Nearest to s: harshly, donau, interpretations, undergone, longest, originated, browse, lark,\n",
            "Nearest to who: nicknamed, marijuana, magen, veiled, wilton, provides, unauthorized, strongholds,\n",
            "Nearest to history: se, guis, schama, copacabana, cations, pani, approve, statistical,\n",
            "Nearest to be: pago, daylights, nolan, beethoven, independents, lupus, expansionary, angkor,\n",
            "Nearest to such: independents, chapterhouse, fuerza, stiffer, regum, chaining, bribed, elton,\n",
            "Nearest to also: jesse, specially, ain, phonetically, breisgau, publicly, laurens, leonards,\n",
            "Nearest to people: alene, truman, imminent, mcduck, juanita, cleve, area, dismissive,\n",
            "Nearest to UNK: hwang, haaretz, strangled, portsmouth, tomb, ediacaran, archive, stuttgart,\n",
            "Nearest to years: pressured, closer, pierre, celestines, bodhi, leak, efficiency, havenco,\n",
            "Nearest to four: naturalization, pisces, lakers, futurist, alps, increments, ss, drains,\n",
            "Nearest to which: eurotunnel, kolbe, mallard, tms, phenylalanine, codepages, medial, capturing,\n",
            "Nearest to into: flynt, citadel, profitable, briton, bonds, derails, integrin, ccny,\n",
            "Nearest to in: twinned, palmerston, transfered, lorica, beauharnais, pied, continuo, stitches,\n",
            "Nearest to only: monsters, dolomite, jabberwocky, nipples, civilized, infinitesimals, bookrags, weaned,\n",
            "Nearest to have: gemstones, wabash, pods, idolatry, environmentalist, cargo, investigation, affliction,\n",
            "Average loss at step  2000 :  114.13510274600982\n",
            "Average loss at step  4000 :  52.333981927394866\n",
            "Average loss at step  6000 :  33.63459469366074\n",
            "Average loss at step  8000 :  23.605797833919524\n",
            "Average loss at step  10000 :  17.733574253559112\n",
            "Average loss at step  10000 :  0.0\n",
            "Nearest to often: victoriae, outflow, trade, gland, reginae, management, not, indus,\n",
            "Nearest to s: and, aa, four, american, zero, analogue, UNK, victoriae,\n",
            "Nearest to who: nicknamed, austin, provides, veiled, which, marijuana, maxim, gland,\n",
            "Nearest to history: statistical, se, victoriae, approve, reginae, modulating, computers, vs,\n",
            "Nearest to be: pago, gland, diesel, victoriae, reginae, sense, by, exercises,\n",
            "Nearest to such: timeline, cl, intelligence, independents, aa, achilles, performed, heuvelmans,\n",
            "Nearest to also: specially, missionary, plants, publicly, slot, ain, physics, isolate,\n",
            "Nearest to people: victoriae, vladimir, area, sworn, dismissive, austin, amun, gollancz,\n",
            "Nearest to UNK: and, reginae, the, one, basins, cl, victoriae, austin,\n",
            "Nearest to years: aa, pierre, earthquakes, varies, ki, seven, closer, therapies,\n",
            "Nearest to four: zero, nine, basins, soroban, victoriae, seven, cl, six,\n",
            "Nearest to which: and, phi, like, agave, hostility, who, austin, capturing,\n",
            "Nearest to into: citadel, bonds, profitable, april, businessman, ilya, solution, and,\n",
            "Nearest to in: and, of, on, UNK, with, one, is, reginae,\n",
            "Nearest to only: gollancz, monsters, reginae, timeline, laureate, tested, storm, thermometers,\n",
            "Nearest to have: environmentalist, tonnes, get, carrel, level, reginae, risk, pseudonyms,\n",
            "Average loss at step  12000 :  13.977221497416496\n",
            "Average loss at step  14000 :  11.601462384700776\n",
            "Average loss at step  16000 :  9.875135800838471\n",
            "Average loss at step  18000 :  8.613835062742233\n",
            "Average loss at step  20000 :  7.768179906010627\n",
            "Average loss at step  20000 :  0.0\n",
            "Nearest to often: victoriae, outflow, not, aquila, trade, is, ethiopic, gland,\n",
            "Nearest to s: and, zero, four, aa, of, victoriae, the, his,\n",
            "Nearest to who: and, nicknamed, which, austin, not, veiled, it, strongholds,\n",
            "Nearest to history: approve, se, statistical, dasyprocta, victoriae, computers, reasons, peptide,\n",
            "Nearest to be: by, pago, have, is, was, gland, claws, albuquerque,\n",
            "Nearest to such: timeline, independents, intelligence, cl, and, well, pharyngeal, arm,\n",
            "Nearest to also: missionary, ain, not, plants, specially, that, usually, isolate,\n",
            "Nearest to people: vladimir, victoriae, area, sworn, agouti, plaintiffs, gollancz, amun,\n",
            "Nearest to UNK: dasyprocta, cl, aquila, reginae, and, agouti, analogue, ames,\n",
            "Nearest to years: earthquakes, seven, aa, pierre, five, varies, therapies, pressured,\n",
            "Nearest to four: eight, nine, six, seven, zero, five, two, three,\n",
            "Nearest to which: and, phi, that, who, it, she, one, claudia,\n",
            "Nearest to into: in, and, derails, by, for, dasyprocta, citadel, amway,\n",
            "Nearest to in: and, of, on, from, dasyprocta, for, at, nine,\n",
            "Nearest to only: gollancz, dasyprocta, monsters, reginae, tested, timeline, laureate, in,\n",
            "Nearest to have: be, philanthropic, pseudonyms, environmentalist, extremophiles, get, and, particle,\n",
            "Average loss at step  22000 :  7.213516742825508\n",
            "Average loss at step  24000 :  6.899444301605224\n",
            "Average loss at step  26000 :  6.712092064261436\n",
            "Average loss at step  28000 :  6.185569616794586\n",
            "Average loss at step  30000 :  6.162071552693844\n",
            "Average loss at step  30000 :  0.0\n",
            "Nearest to often: not, victoriae, outflow, aquila, ethiopic, generally, still, he,\n",
            "Nearest to s: and, zero, his, of, victoriae, four, prizes, the,\n",
            "Nearest to who: which, and, nicknamed, austin, not, it, he, veiled,\n",
            "Nearest to history: approve, dasyprocta, statistical, se, computers, victoriae, reasons, peptide,\n",
            "Nearest to be: have, by, is, pago, was, as, nolan, are,\n",
            "Nearest to such: well, independents, timeline, pharyngeal, intelligence, thinkpad, arm, hostility,\n",
            "Nearest to also: ain, not, which, usually, that, missionary, it, they,\n",
            "Nearest to people: vladimir, victoriae, area, agouti, sworn, plaintiffs, bos, gollancz,\n",
            "Nearest to UNK: cl, dasyprocta, basins, trinomial, agouti, akita, four, five,\n",
            "Nearest to years: earthquakes, four, five, varies, pierre, amman, seven, pressured,\n",
            "Nearest to four: six, eight, seven, five, nine, three, two, zero,\n",
            "Nearest to which: that, phi, and, who, it, this, she, claudia,\n",
            "Nearest to into: and, from, in, by, under, derails, for, amway,\n",
            "Nearest to in: and, from, on, of, at, nine, for, with,\n",
            "Nearest to only: gollancz, dasyprocta, monsters, akita, aorta, reginae, timeline, tested,\n",
            "Nearest to have: be, abitibi, has, had, are, were, get, explosion,\n",
            "Average loss at step  32000 :  5.843790549635887\n",
            "Average loss at step  34000 :  5.868762283205986\n",
            "Average loss at step  36000 :  5.695866896748543\n",
            "Average loss at step  38000 :  5.2912884383201595\n",
            "Average loss at step  40000 :  5.464374469637871\n",
            "Average loss at step  40000 :  0.0\n",
            "Nearest to often: not, generally, victoriae, also, outflow, still, there, ethiopic,\n",
            "Nearest to s: and, his, victoriae, zero, prizes, agouti, auction, albury,\n",
            "Nearest to who: which, and, nicknamed, he, not, austin, it, veiled,\n",
            "Nearest to history: approve, dasyprocta, schama, victoriae, se, abet, statistical, peptide,\n",
            "Nearest to be: have, by, is, was, pago, are, been, as,\n",
            "Nearest to such: well, independents, timeline, pharyngeal, thinkpad, intelligence, and, arm,\n",
            "Nearest to also: which, usually, often, sometimes, not, ain, it, missionary,\n",
            "Nearest to people: vladimir, victoriae, agouti, plaintiffs, sworn, area, mcduck, dylan,\n",
            "Nearest to UNK: agouti, dasyprocta, akita, bos, albury, cl, analogue, victoriae,\n",
            "Nearest to years: four, earthquakes, seven, amman, varies, pressured, therapies, month,\n",
            "Nearest to four: six, seven, five, eight, three, two, nine, zero,\n",
            "Nearest to which: that, this, phi, and, who, it, also, she,\n",
            "Nearest to into: from, under, in, derails, by, ilya, dasyprocta, amway,\n",
            "Nearest to in: and, from, on, at, primigenius, dasyprocta, during, since,\n",
            "Nearest to only: gollancz, dasyprocta, monsters, aorta, akita, reginae, tested, timeline,\n",
            "Nearest to have: has, be, were, had, are, abitibi, get, explosion,\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss at step  42000 :  5.293747091174126\n",
            "Average loss at step  44000 :  5.308251688718796\n",
            "Average loss at step  46000 :  5.278701328039169\n",
            "Average loss at step  48000 :  5.055534006118775\n",
            "Average loss at step  50000 :  5.151082276582718\n",
            "Average loss at step  50000 :  0.0\n",
            "Nearest to often: generally, also, not, still, there, victoriae, outflow, ethiopic,\n",
            "Nearest to s: and, his, zero, victoriae, primigenius, prizes, agouti, of,\n",
            "Nearest to who: which, he, and, nicknamed, not, austin, there, it,\n",
            "Nearest to history: approve, dasyprocta, schama, thermometer, se, peptide, statistical, tabula,\n",
            "Nearest to be: have, by, is, was, are, been, were, nolan,\n",
            "Nearest to such: well, independents, timeline, pharyngeal, coveted, celluloid, thinkpad, known,\n",
            "Nearest to also: which, often, usually, sometimes, not, ain, it, riverboat,\n",
            "Nearest to people: vladimir, tabula, victoriae, agouti, dylan, mcduck, bos, sworn,\n",
            "Nearest to UNK: dasyprocta, thibetanus, agouti, albury, trinomial, reginae, basins, aquila,\n",
            "Nearest to years: earthquakes, four, amman, month, pressured, varies, therapies, harrier,\n",
            "Nearest to four: six, three, seven, five, eight, two, one, zero,\n",
            "Nearest to which: that, this, phi, who, also, it, and, she,\n",
            "Nearest to into: from, under, and, on, dasyprocta, ilya, derails, by,\n",
            "Nearest to in: and, from, at, on, primigenius, during, dasyprocta, of,\n",
            "Nearest to only: monsters, gollancz, thibetanus, dasyprocta, aorta, akita, infinitesimals, reginae,\n",
            "Nearest to have: had, has, were, be, are, abitibi, get, explosion,\n",
            "Average loss at step  52000 :  5.180446610331535\n",
            "Average loss at step  54000 :  5.104818750619888\n",
            "Average loss at step  56000 :  5.060129884362221\n",
            "Average loss at step  58000 :  5.093931724190712\n",
            "Average loss at step  60000 :  4.9219318302869794\n",
            "Average loss at step  60000 :  0.0\n",
            "Nearest to often: generally, also, still, not, there, tamarin, victoriae, cebus,\n",
            "Nearest to s: zero, his, victoriae, microsite, auction, ssbn, eight, tamarin,\n",
            "Nearest to who: which, he, nicknamed, and, not, austin, there, that,\n",
            "Nearest to history: dasyprocta, cebus, victoriae, approve, abet, tamarin, tabula, thermometer,\n",
            "Nearest to be: have, by, been, is, are, was, nolan, michelob,\n",
            "Nearest to such: well, independents, celluloid, timeline, pharyngeal, coveted, known, pulau,\n",
            "Nearest to also: which, often, usually, sometimes, not, it, ain, now,\n",
            "Nearest to people: tabula, vladimir, victoriae, agouti, gollancz, plaintiffs, dylan, michelob,\n",
            "Nearest to UNK: cebus, tamarin, ssbn, callithrix, dasyprocta, akita, agouti, thibetanus,\n",
            "Nearest to years: earthquakes, four, amman, month, pressured, pulau, therapies, harrier,\n",
            "Nearest to four: five, six, three, eight, seven, nine, two, zero,\n",
            "Nearest to which: that, this, it, who, also, phi, but, one,\n",
            "Nearest to into: from, under, on, derails, ilya, dasyprocta, in, citadel,\n",
            "Nearest to in: at, tamarin, primigenius, dasyprocta, during, from, since, on,\n",
            "Nearest to only: tamarin, monsters, thibetanus, dasyprocta, gollancz, aorta, until, infinitesimals,\n",
            "Nearest to have: had, has, were, are, be, abitibi, get, explosion,\n",
            "Average loss at step  62000 :  4.795876760840416\n",
            "Average loss at step  64000 :  4.765383581876755\n",
            "Average loss at step  66000 :  4.995991773486137\n",
            "Average loss at step  68000 :  4.899177716612816\n",
            "Average loss at step  70000 :  4.778745459437371\n",
            "Average loss at step  70000 :  0.0\n",
            "Nearest to often: generally, also, still, not, commonly, there, tamarin, sometimes,\n",
            "Nearest to s: victoriae, microsite, auction, mitral, primigenius, zero, agouti, his,\n",
            "Nearest to who: which, he, nicknamed, and, never, often, that, she,\n",
            "Nearest to history: dasyprocta, cebus, thermometer, victoriae, abet, approve, reign, heretical,\n",
            "Nearest to be: been, have, is, are, by, were, michelob, skaldic,\n",
            "Nearest to such: well, independents, many, known, pharyngeal, these, celluloid, pulau,\n",
            "Nearest to also: often, which, sometimes, usually, it, now, not, ain,\n",
            "Nearest to people: tabula, vladimir, agouti, victoriae, gollancz, dylan, sanjaks, pyle,\n",
            "Nearest to UNK: thaler, callithrix, tamarin, cebus, dasyprocta, ssbn, agouti, michelob,\n",
            "Nearest to years: earthquakes, seven, six, four, month, amman, pulau, pressured,\n",
            "Nearest to four: six, five, three, seven, eight, two, one, nine,\n",
            "Nearest to which: that, this, also, it, but, who, what, and,\n",
            "Nearest to into: from, under, in, derails, citadel, ilya, amway, on,\n",
            "Nearest to in: at, since, during, primigenius, tamarin, dasyprocta, from, including,\n",
            "Nearest to only: tamarin, dasyprocta, thibetanus, monsters, aorta, gollancz, until, infinitesimals,\n",
            "Nearest to have: had, has, were, are, be, abitibi, get, sem,\n",
            "Average loss at step  72000 :  4.8009613496065136\n",
            "Average loss at step  74000 :  4.779721226930619\n",
            "Average loss at step  76000 :  4.880615778207779\n",
            "Average loss at step  78000 :  4.780165301978588\n",
            "Average loss at step  80000 :  4.833629039883614\n",
            "Average loss at step  80000 :  0.0\n",
            "Nearest to often: generally, also, still, commonly, not, sometimes, usually, there,\n",
            "Nearest to s: victoriae, zero, microsite, his, mitral, primigenius, and, tamarin,\n",
            "Nearest to who: he, which, nicknamed, often, she, never, there, mib,\n",
            "Nearest to history: thermometer, heretical, reign, dasyprocta, cebus, approve, victoriae, peptide,\n",
            "Nearest to be: been, have, by, were, are, is, was, skaldic,\n",
            "Nearest to such: well, these, many, independents, known, pharyngeal, celluloid, are,\n",
            "Nearest to also: often, which, sometimes, usually, now, still, it, ain,\n",
            "Nearest to people: tabula, vladimir, agouti, victoriae, gollancz, pyle, plaintiffs, sanjaks,\n",
            "Nearest to UNK: callithrix, cegep, cebus, dasyprocta, thaler, thibetanus, tamarin, mitral,\n",
            "Nearest to years: earthquakes, four, seven, month, pressured, amman, six, therapies,\n",
            "Nearest to four: six, five, three, seven, eight, two, nine, michelob,\n",
            "Nearest to which: that, this, also, it, clodius, what, but, phi,\n",
            "Nearest to into: from, under, in, derails, amway, ilya, through, dasyprocta,\n",
            "Nearest to in: during, at, since, primigenius, dasyprocta, tamarin, and, on,\n",
            "Nearest to only: pontificia, tamarin, dasyprocta, thibetanus, until, monsters, aorta, gollancz,\n",
            "Nearest to have: had, has, were, are, be, abitibi, get, explosion,\n",
            "Average loss at step  82000 :  4.823427764415741\n",
            "Average loss at step  84000 :  4.7864458949565885\n",
            "Average loss at step  86000 :  4.7424036408662795\n",
            "Average loss at step  88000 :  4.68340490758419\n",
            "Average loss at step  90000 :  4.752689378023147\n",
            "Average loss at step  90000 :  0.0\n",
            "Nearest to often: generally, also, commonly, still, sometimes, usually, not, now,\n",
            "Nearest to s: his, microsite, victoriae, agouti, zero, mitral, primigenius, tamarin,\n",
            "Nearest to who: he, which, often, she, nicknamed, never, mib, and,\n",
            "Nearest to history: thermometer, reign, cebus, dasyprocta, heretical, victoriae, abet, approve,\n",
            "Nearest to be: been, have, is, are, were, by, was, michelob,\n",
            "Nearest to such: well, these, many, known, independents, pharyngeal, celluloid, sive,\n",
            "Nearest to also: often, which, sometimes, usually, now, still, it, ain,\n",
            "Nearest to people: tabula, agouti, vladimir, victoriae, plaintiffs, gollancz, pyle, magma,\n",
            "Nearest to UNK: cebus, thaler, cegep, callithrix, thibetanus, tamarin, dasyprocta, ssbn,\n",
            "Nearest to years: earthquakes, seven, month, pressured, amman, therapies, six, supervillain,\n",
            "Nearest to four: five, six, seven, three, eight, two, nine, michelob,\n",
            "Nearest to which: that, this, also, but, it, clodius, what, one,\n",
            "Nearest to into: from, under, textrm, through, ilya, on, dasyprocta, derails,\n",
            "Nearest to in: during, at, primigenius, dasyprocta, of, nine, since, tamarin,\n",
            "Nearest to only: pontificia, tamarin, dasyprocta, thibetanus, aorta, monsters, gollancz, until,\n",
            "Nearest to have: had, has, were, are, be, abitibi, get, sem,\n",
            "Average loss at step  92000 :  4.699707681417466\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss at step  94000 :  4.620864012122154\n",
            "Average loss at step  96000 :  4.7264276374578476\n",
            "Average loss at step  98000 :  4.610776684582233\n",
            "Average loss at step  100000 :  4.667103960514068\n",
            "Average loss at step  100000 :  0.0\n",
            "Nearest to often: generally, commonly, also, usually, sometimes, still, now, not,\n",
            "Nearest to s: victoriae, his, microsite, tamarin, the, mitral, ssbn, michelob,\n",
            "Nearest to who: he, often, never, she, which, nicknamed, mib, and,\n",
            "Nearest to history: reign, thermometer, cebus, heretical, form, dasyprocta, approve, victoriae,\n",
            "Nearest to be: been, have, are, by, is, were, skaldic, was,\n",
            "Nearest to such: well, these, known, many, independents, celluloid, pharyngeal, sive,\n",
            "Nearest to also: often, sometimes, usually, now, which, still, generally, ain,\n",
            "Nearest to people: tabula, agouti, vladimir, victoriae, gollancz, plaintiffs, pyle, michelob,\n",
            "Nearest to UNK: callithrix, thaler, cegep, cebus, dasyprocta, tamarin, thibetanus, mitral,\n",
            "Nearest to years: earthquakes, dbm, month, pressured, amman, six, legionnaire, therapies,\n",
            "Nearest to four: five, six, three, seven, eight, two, nine, michelob,\n",
            "Nearest to which: that, this, it, what, but, also, clodius, one,\n",
            "Nearest to into: from, under, textrm, through, ilya, on, in, derails,\n",
            "Nearest to in: during, from, dasyprocta, at, tamarin, scalia, primigenius, on,\n",
            "Nearest to only: pontificia, tamarin, dasyprocta, thibetanus, aorta, monsters, until, gollancz,\n",
            "Nearest to have: had, has, were, are, be, abitibi, get, having,\n",
            "Average loss at step  102000 :  4.676566014766693\n",
            "Average loss at step  104000 :  4.620229466199875\n",
            "Average loss at step  106000 :  4.630911651492119\n",
            "Average loss at step  108000 :  4.6112746781110765\n",
            "Average loss at step  110000 :  4.500114473223686\n",
            "Average loss at step  110000 :  0.0\n",
            "Nearest to often: generally, usually, commonly, sometimes, also, still, now, not,\n",
            "Nearest to s: victoriae, zero, microsite, the, whose, his, tamarin, mitral,\n",
            "Nearest to who: he, never, often, she, which, nicknamed, mib, they,\n",
            "Nearest to history: reign, cebus, dasyprocta, victoriae, thermometer, michelob, mitral, abet,\n",
            "Nearest to be: been, have, is, are, were, by, was, skaldic,\n",
            "Nearest to such: well, these, many, known, independents, including, celluloid, some,\n",
            "Nearest to also: often, sometimes, which, now, usually, still, generally, refrigerant,\n",
            "Nearest to people: tabula, agouti, gollancz, vladimir, victoriae, michelob, callithrix, those,\n",
            "Nearest to UNK: cebus, callithrix, thaler, tamarin, cegep, dasyprocta, agouti, marmoset,\n",
            "Nearest to years: earthquakes, five, dbm, month, six, seven, tamarin, pressured,\n",
            "Nearest to four: five, six, three, seven, eight, two, michelob, tamarin,\n",
            "Nearest to which: that, this, also, what, it, but, clodius, still,\n",
            "Nearest to into: from, under, through, textrm, derails, ilya, on, amway,\n",
            "Nearest to in: during, from, on, at, dasyprocta, tamarin, since, under,\n",
            "Nearest to only: pontificia, tamarin, dasyprocta, thibetanus, aorta, until, three, gollancz,\n",
            "Nearest to have: had, has, were, are, be, abitibi, get, having,\n",
            "Average loss at step  112000 :  4.562521131277085\n",
            "Average loss at step  114000 :  4.571719884157181\n",
            "Average loss at step  116000 :  4.5767022740840915\n",
            "Average loss at step  118000 :  4.562148822665215\n",
            "Average loss at step  120000 :  4.5420051455497745\n",
            "Average loss at step  120000 :  0.0\n",
            "Nearest to often: generally, usually, commonly, sometimes, also, still, now, not,\n",
            "Nearest to s: victoriae, his, whose, microsite, widehat, subtitled, zero, png,\n",
            "Nearest to who: he, she, never, often, which, they, and, nicknamed,\n",
            "Nearest to history: reign, dasyprocta, cebus, victoriae, eml, mitral, thermometer, abet,\n",
            "Nearest to be: been, have, are, by, is, were, skaldic, michelob,\n",
            "Nearest to such: these, well, known, many, including, celluloid, independents, dissociated,\n",
            "Nearest to also: often, sometimes, which, usually, now, still, refrigerant, it,\n",
            "Nearest to people: tabula, agouti, those, gollancz, vladimir, magma, victoriae, callithrix,\n",
            "Nearest to UNK: tamarin, callithrix, thaler, cebus, cegep, widehat, dasyprocta, marmoset,\n",
            "Nearest to years: month, earthquakes, six, dbm, months, seven, decades, five,\n",
            "Nearest to four: six, five, three, seven, eight, two, nine, zero,\n",
            "Nearest to which: that, this, also, but, it, what, however, clodius,\n",
            "Nearest to into: from, under, through, textrm, derails, in, ilya, entire,\n",
            "Nearest to in: during, at, on, dasyprocta, within, from, primigenius, of,\n",
            "Nearest to only: tamarin, pontificia, dasyprocta, thibetanus, eml, aorta, three, until,\n",
            "Nearest to have: had, has, were, are, be, abitibi, get, having,\n",
            "Average loss at step  122000 :  4.606185715675354\n",
            "Average loss at step  124000 :  4.512865809619426\n",
            "Average loss at step  126000 :  4.454112043738365\n",
            "Average loss at step  128000 :  4.52083384847641\n",
            "Average loss at step  130000 :  4.479972840666771\n",
            "Average loss at step  130000 :  0.0\n",
            "Nearest to often: generally, usually, sometimes, commonly, also, still, now, frequently,\n",
            "Nearest to s: victoriae, whose, microsite, his, zero, primigenius, neutronic, widehat,\n",
            "Nearest to who: he, she, never, often, and, they, which, also,\n",
            "Nearest to history: reign, cebus, dasyprocta, victoriae, eml, mitral, thermometer, michelob,\n",
            "Nearest to be: been, have, is, are, were, skaldic, by, often,\n",
            "Nearest to such: these, well, known, many, including, some, dissociated, celluloid,\n",
            "Nearest to also: often, sometimes, which, now, usually, still, refrigerant, it,\n",
            "Nearest to people: tabula, those, gollancz, agouti, vladimir, countries, magma, victoriae,\n",
            "Nearest to UNK: callithrix, widehat, thaler, cegep, tamarin, dasyprocta, thibetanus, four,\n",
            "Nearest to years: month, decades, earthquakes, months, six, dbm, times, year,\n",
            "Nearest to four: six, three, five, seven, eight, two, nine, michelob,\n",
            "Nearest to which: that, this, also, but, it, what, however, daigo,\n",
            "Nearest to into: from, under, through, textrm, derails, in, dasyprocta, ilya,\n",
            "Nearest to in: during, at, dasyprocta, primigenius, within, from, scalia, tamarin,\n",
            "Nearest to only: tamarin, dasyprocta, pontificia, thibetanus, eml, three, aorta, four,\n",
            "Nearest to have: had, has, were, are, be, abitibi, get, having,\n",
            "Average loss at step  132000 :  4.503298661708832\n",
            "Average loss at step  134000 :  4.517632678151131\n",
            "Average loss at step  136000 :  4.475681147515774\n",
            "Average loss at step  138000 :  4.526946028351784\n",
            "Average loss at step  140000 :  4.509473690509796\n",
            "Average loss at step  140000 :  0.0\n",
            "Nearest to often: generally, sometimes, usually, commonly, also, still, now, not,\n",
            "Nearest to s: his, victoriae, whose, primigenius, zero, mitral, tamarin, moslem,\n",
            "Nearest to who: he, she, never, they, often, which, also, daigo,\n",
            "Nearest to history: reign, cebus, dasyprocta, eml, mitral, victoriae, michelob, form,\n",
            "Nearest to be: been, have, were, is, are, was, skaldic, being,\n",
            "Nearest to such: these, well, known, many, including, some, celluloid, dissociated,\n",
            "Nearest to also: often, sometimes, which, now, usually, still, refrigerant, it,\n",
            "Nearest to people: those, tabula, gollancz, women, agouti, countries, vladimir, callithrix,\n",
            "Nearest to UNK: callithrix, thaler, dasyprocta, cegep, cebus, tamarin, agouti, thibetanus,\n",
            "Nearest to years: month, decades, months, earthquakes, times, dbm, year, words,\n",
            "Nearest to four: three, six, five, eight, seven, two, michelob, zero,\n",
            "Nearest to which: that, this, but, also, what, it, however, daigo,\n",
            "Nearest to into: from, through, under, in, derails, textrm, chilling, schwerin,\n",
            "Nearest to in: during, at, on, dasyprocta, widehat, within, scalia, tamarin,\n",
            "Nearest to only: tamarin, pontificia, dasyprocta, thibetanus, eml, until, aorta, neutronic,\n",
            "Nearest to have: had, has, were, are, be, abitibi, get, having,\n",
            "Average loss at step  142000 :  4.5028926653862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss at step  144000 :  4.377020892977715\n",
            "Average loss at step  146000 :  4.473313964486122\n",
            "Average loss at step  148000 :  4.512662992835045\n",
            "Average loss at step  150000 :  4.52354017162323\n",
            "Average loss at step  150000 :  0.0\n",
            "Nearest to often: generally, usually, sometimes, commonly, also, still, now, frequently,\n",
            "Nearest to s: victoriae, whose, widehat, stadtbahn, primigenius, his, microsite, moslem,\n",
            "Nearest to who: he, she, never, often, they, which, mib, also,\n",
            "Nearest to history: cebus, mitral, eml, dasyprocta, reign, michelob, victoriae, abet,\n",
            "Nearest to be: been, have, were, is, was, skaldic, become, being,\n",
            "Nearest to such: these, well, known, many, including, stadtbahn, celluloid, pharyngeal,\n",
            "Nearest to also: often, sometimes, usually, now, which, still, refrigerant, generally,\n",
            "Nearest to people: those, women, tabula, gollancz, countries, agouti, men, callithrix,\n",
            "Nearest to UNK: stadtbahn, cebus, callithrix, widehat, six, thaler, cegep, agouti,\n",
            "Nearest to years: month, decades, months, earthquakes, times, year, seven, five,\n",
            "Nearest to four: five, six, three, seven, eight, two, nine, zero,\n",
            "Nearest to which: this, that, but, what, also, it, however, daigo,\n",
            "Nearest to into: from, through, under, textrm, derails, schwerin, back, within,\n",
            "Nearest to in: during, tamarin, at, dasyprocta, on, primigenius, widehat, scalia,\n",
            "Nearest to only: tamarin, pontificia, four, dasyprocta, until, thibetanus, eml, neutronic,\n",
            "Nearest to have: had, has, were, are, be, abitibi, having, get,\n",
            "Average loss at step  152000 :  4.5359329936504365\n",
            "Average loss at step  154000 :  4.512476790189743\n",
            "Average loss at step  156000 :  4.492828402519226\n",
            "Average loss at step  158000 :  4.373340238571167\n",
            "Average loss at step  160000 :  4.444905438661575\n",
            "Average loss at step  160000 :  0.0\n",
            "Nearest to often: generally, usually, sometimes, commonly, also, still, now, frequently,\n",
            "Nearest to s: whose, victoriae, zero, moslem, widehat, his, primigenius, subtitled,\n",
            "Nearest to who: he, she, often, never, which, also, they, and,\n",
            "Nearest to history: cebus, mitral, eml, dasyprocta, victoriae, michelob, ssbn, abet,\n",
            "Nearest to be: been, have, become, were, is, are, was, skaldic,\n",
            "Nearest to such: these, well, known, many, including, stadtbahn, some, celluloid,\n",
            "Nearest to also: often, sometimes, which, now, usually, still, refrigerant, generally,\n",
            "Nearest to people: women, those, tabula, gollancz, countries, agouti, jews, men,\n",
            "Nearest to UNK: stadtbahn, thibetanus, cebus, widehat, tamarin, callithrix, cegep, thaler,\n",
            "Nearest to years: seven, decades, five, months, month, days, times, year,\n",
            "Nearest to four: six, five, three, seven, eight, two, one, nine,\n",
            "Nearest to which: that, this, but, what, also, it, however, daigo,\n",
            "Nearest to into: from, through, under, textrm, derails, schwerin, within, entire,\n",
            "Nearest to in: during, on, at, under, within, since, primigenius, dasyprocta,\n",
            "Nearest to only: tamarin, pontificia, dasyprocta, thibetanus, eml, until, aorta, reginae,\n",
            "Nearest to have: had, has, were, are, be, abitibi, having, get,\n",
            "Average loss at step  162000 :  4.511232511401176\n",
            "Average loss at step  164000 :  4.458364478468895\n",
            "Average loss at step  166000 :  4.473267237067223\n",
            "Average loss at step  168000 :  4.477565832018852\n",
            "Average loss at step  170000 :  4.459542991399765\n",
            "Average loss at step  170000 :  0.0\n",
            "Nearest to often: generally, usually, sometimes, commonly, also, still, now, frequently,\n",
            "Nearest to s: whose, victoriae, widehat, landesverband, subtitled, his, hyi, microsite,\n",
            "Nearest to who: he, never, she, often, also, they, which, daigo,\n",
            "Nearest to history: ebne, landesverband, cebus, list, eml, mitral, dasyprocta, microcebus,\n",
            "Nearest to be: been, have, were, was, become, is, are, skaldic,\n",
            "Nearest to such: these, well, known, many, including, some, those, stadtbahn,\n",
            "Nearest to also: often, now, sometimes, which, usually, still, refrigerant, generally,\n",
            "Nearest to people: women, those, tabula, gollancz, men, countries, jews, agouti,\n",
            "Nearest to UNK: stadtbahn, hyi, callithrix, cebus, widehat, cegep, thaler, tamarin,\n",
            "Nearest to years: decades, month, days, months, five, seven, year, times,\n",
            "Nearest to four: five, six, three, seven, eight, zero, nine, two,\n",
            "Nearest to which: that, this, also, but, what, however, it, and,\n",
            "Nearest to into: through, from, under, in, textrm, derails, schwerin, back,\n",
            "Nearest to in: during, within, at, dasyprocta, landesverband, primigenius, under, on,\n",
            "Nearest to only: tamarin, pontificia, until, dasyprocta, eml, thibetanus, landesverband, reginae,\n",
            "Nearest to have: had, has, were, are, be, abitibi, having, refer,\n",
            "Average loss at step  172000 :  4.475970704078675\n",
            "Average loss at step  174000 :  4.448919707655906\n",
            "Average loss at step  176000 :  4.436616892099381\n",
            "Average loss at step  178000 :  4.332596390962601\n",
            "Average loss at step  180000 :  4.452814555764198\n",
            "Average loss at step  180000 :  0.0\n",
            "Nearest to often: usually, generally, sometimes, commonly, still, also, now, frequently,\n",
            "Nearest to s: victoriae, whose, landesverband, hyi, primigenius, microsite, his, subtitled,\n",
            "Nearest to who: he, often, never, she, they, which, also, sometimes,\n",
            "Nearest to history: list, ebne, cebus, landesverband, mitral, eml, microcebus, dasyprocta,\n",
            "Nearest to be: been, have, is, were, was, being, are, become,\n",
            "Nearest to such: these, well, known, many, including, dissociated, regarded, those,\n",
            "Nearest to also: often, now, sometimes, still, which, usually, generally, refrigerant,\n",
            "Nearest to people: women, those, tabula, gollancz, men, agouti, jews, countries,\n",
            "Nearest to UNK: stadtbahn, callithrix, thaler, hyi, tamarin, landesverband, dasyprocta, widehat,\n",
            "Nearest to years: decades, days, month, year, months, seven, times, six,\n",
            "Nearest to four: five, six, seven, three, eight, two, nine, zero,\n",
            "Nearest to which: that, this, what, but, also, it, however, often,\n",
            "Nearest to into: through, from, under, textrm, within, back, schwerin, across,\n",
            "Nearest to in: during, within, at, tamarin, dasyprocta, landesverband, stadtbahn, primigenius,\n",
            "Nearest to only: tamarin, dasyprocta, pontificia, eml, until, landesverband, thibetanus, reginae,\n",
            "Nearest to have: had, has, were, are, be, abitibi, having, get,\n",
            "Average loss at step  182000 :  4.450957545757293\n",
            "Average loss at step  184000 :  4.449381316781044\n",
            "Average loss at step  186000 :  4.436810357689858\n",
            "Average loss at step  188000 :  4.407359811067582\n",
            "Average loss at step  190000 :  4.349346057891846\n",
            "Average loss at step  190000 :  0.0\n",
            "Nearest to often: usually, generally, sometimes, commonly, still, also, now, frequently,\n",
            "Nearest to s: whose, victoriae, his, microsite, zero, widehat, stadtbahn, subtitled,\n",
            "Nearest to who: often, he, never, she, also, which, they, sometimes,\n",
            "Nearest to history: list, ebne, cebus, landesverband, mitral, eml, microcebus, dasyprocta,\n",
            "Nearest to be: been, have, become, were, was, being, carbonic, prevent,\n",
            "Nearest to such: these, well, known, many, including, regarded, those, shines,\n",
            "Nearest to also: often, now, sometimes, which, still, usually, generally, never,\n",
            "Nearest to people: women, men, those, tabula, gollancz, jews, agouti, them,\n",
            "Nearest to UNK: stadtbahn, hyi, widehat, callithrix, cebus, pesce, tamarin, landesverband,\n",
            "Nearest to years: seven, decades, days, months, month, year, times, five,\n",
            "Nearest to four: five, six, seven, three, two, eight, hyi, landesverband,\n",
            "Nearest to which: that, this, also, what, but, however, often, it,\n",
            "Nearest to into: through, from, under, textrm, within, across, schwerin, entire,\n",
            "Nearest to in: during, at, dasyprocta, within, primigenius, landesverband, stadtbahn, cegep,\n",
            "Nearest to only: tamarin, dasyprocta, eml, pontificia, until, landesverband, thibetanus, but,\n",
            "Nearest to have: had, has, are, were, be, having, abitibi, get,\n",
            "Average loss at step  192000 :  3.9556518753767014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss at step  194000 :  4.239269185066223\n",
            "Average loss at step  196000 :  4.220592938065529\n",
            "Average loss at step  198000 :  4.324846914887428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tP-vaEQfYE7p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义一个用来可视化Word2Vec效果的函数。\n",
        "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
        "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
        "  plt.figure(figsize=(18,18))\n",
        "  for i, label in enumerate(labels):\n",
        "    x,y = low_dim_embs[i,:]\n",
        "    plt.scatter(x,y)\n",
        "    plt.annotate(label,\n",
        "                xy=(x,y),\n",
        "                xytext=(5,2),\n",
        "                textcoords='offset points',\n",
        "                ha='right',\n",
        "                va='bottom')\n",
        "    \n",
        "    plt.savefig(filename)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ItnLLwPEY8-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#使用TSNE实现降维，直接将原始128维的嵌入向量降到2维，再用前面的plot_with_labels函数进行展示\n",
        "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
        "plot_only = 100\n",
        "low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n",
        "labels = [reverse_dictionary[i] for i in range(plot_only)]\n",
        "plot_with_labels(low_dim_embs, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}