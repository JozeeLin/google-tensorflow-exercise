{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "经典卷积神经网络-ResNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/JozeeLin/google-tensorflow-exercise/blob/master/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_ResNet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "8xPvLzvf3py2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ResNet(Residual Neural Network)\n",
        "\n",
        "ResNet通过使用Residual Unit成功训练152层深的神经网络。ResNet的结构可以极快地加速超深神经网络的训练，模型的准确率也有非常大的提升。\n",
        "\n",
        "**注意：Highway NetWork原理与ResNet很相似。Highway NetWork的目标就是解决极深的神经网络难以训练的问题。**\n",
        "\n",
        "**Highway NetWork相当于修改了每一层的激活函数，此前的激活函数只是对输入做一个非线性变换，HN则允许保留一定比例的原始输入x。**\n",
        "\n",
        "**这样前面一层的信息，有一定比例可以不经过矩阵乘法和非线性变换，直接传输到下一层，仿佛一条信息高速公路，因此得名HN**\n",
        "\n",
        "**HN 主要通过gating units学习如何控制网络中的信息流，即学习原始信息应保留的比例。这个可学习的gating机制，\n",
        "借鉴于LSTM循环神经网络中的gating，使之可以使用梯度下降算法来训练。并可以配合多种非线性激活函数，学习极深的神经网络现在变得可行了。**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rWcilYUE3OTc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import math\n",
        "from datetime import datetime\n",
        "slim = tf.contrib.slim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PZygs6nVZ0Uh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 使用collections.namedtuple设计ResNet基本Block模块组的named tuple，并用它创建Block的类，但只包含数据结构，不包含具体方法。\n",
        "class Block(collections.namedtuple('Block',['scope','unit_fn','args'])):\n",
        "  'A named tuple describing a ResNet block'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-KwTv1D8Ph-c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#降采样subsample的方法\n",
        "def subsample(inputs, factor, scope=None):\n",
        "  if factor == 1:\n",
        "    return inputs\n",
        "  else:\n",
        "    return slim.max_pool2d(inputs,[1,1],stride=factor,scope=scope)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ih8-eVvnQSHt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#用于创建卷积层\n",
        "def conv2d_same(inputs, num_outputs, kernel_size, stride, scope=None):\n",
        "  if stride == 1:\n",
        "    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, padding='SAME', scope=scope)\n",
        "  else:\n",
        "    pad_total = kernel_size - 1\n",
        "    pad_beg = pad_total // 2\n",
        "    pad_end = pad_total - pad_beg\n",
        "    inputs = tf.pad(inputs, [[0,0], [pad_beg, pad_end],\n",
        "                            [pad_beg,pad_end],[0,0]])\n",
        "    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride, padding='VALID', scope=scope)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJHID3e-RHBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义堆叠Blocks函数\n",
        "@slim.add_arg_scope\n",
        "def stack_blocks_dense(net, blocks,outputs_collections=None):\n",
        "  for block in blocks:\n",
        "    with tf.variable_scope(block.scope, 'block',[net]) as sc:\n",
        "      for i, unit in enumerate(block.args):\n",
        "        with tf.variable_scope('unit_%d' % (i+1), values=[net]):\n",
        "          unit_depth, unit_depth_bottleneck, unit_stride=unit\n",
        "          net = block.unit_fn(net, \n",
        "                             depth=unit_depth,\n",
        "                             depth_bottleneck=unit_depth_bottleneck,\n",
        "                             stride=unit_stride)\n",
        "      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n",
        "  return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oufZ3lPrWf8J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#创建通用的arg_scope,其功能是用来定义某些函数的参数默认值\n",
        "def resnet_arg_scope(is_training=True,\n",
        "                    weight_decay=0.0001,\n",
        "                    batch_norm_decay=0.997,\n",
        "                    batch_norm_epsilon=1e-5,\n",
        "                    batch_norm_scale=True\n",
        "                    ):\n",
        "  batch_norm_params={\n",
        "      'is_training':is_training,\n",
        "      'decay':batch_norm_decay,\n",
        "      'epsilon':batch_norm_epsilon,\n",
        "      'scale':batch_norm_scale,\n",
        "      'updates_collections':tf.GraphKeys.UPDATE_OPS,\n",
        "  }\n",
        "  \n",
        "  with slim.arg_scope(\n",
        "      [slim.conv2d],\n",
        "      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
        "      weights_initializer=slim.variance_scaling_initializer(),\n",
        "      activation_fn=tf.nn.relu,\n",
        "      normalizer_fn=slim.batch_norm,\n",
        "      normalizer_params=batch_norm_params\n",
        "  ):\n",
        "    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
        "      with slim.arg_scope([slim.max_pool2d], padding='SAME') as arg_sc:\n",
        "        return arg_sc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpDpowN70byc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#bottleneck 残差学习单元，论文中提到的full preactivation residual unit的一个变种。\n",
        "#主要区别有两点:\n",
        "#一是在每一层前都用了batch normalizer，二是对输入进行preactivation，而不是在卷积进行激活函数处理\n",
        "@slim.add_arg_scope\n",
        "def bottleneck(inputs, depth, depth_bottleneck, stride, outputs_collections=None, scope=None):\n",
        "  with tf.variable_scope(scope,'bottleneck_v2', [inputs]) as sc:\n",
        "    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
        "    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope='preact')\n",
        "    \n",
        "    if depth == depth_in:\n",
        "      shortcut = subsample(inputs, stride, 'shortcut')\n",
        "    else:\n",
        "      shortcut = slim.conv2d(preact, depth, [1,1], stride=stride, \n",
        "                            normalizer_fn=None, activation_fn=None,\n",
        "                            scope='shortcut')\n",
        "      \n",
        "    residual = slim.conv2d(preact, depth_bottleneck, [1,1], stride=1, scope='conv1')\n",
        "    residual = conv2d_same(residual, depth_bottleneck, 3, stride, scope='conv2')\n",
        "    residual = slim.conv2d(residual, depth, [1,1], stride=1, normalizer_fn=None, activation_fn=None,scope='conv3')\n",
        "    \n",
        "    output = shortcut + residual\n",
        "    return slim.utils.collect_named_outputs(outputs_collections, sc.name, output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W5l8ZEG0kFMx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义生成ResNet V2的主函数，只要预先定义好网络的残差学习模块组blocks\n",
        "def resnet_v2(inputs, blocks,\n",
        "             num_classes=None,\n",
        "             global_pool=True,\n",
        "             include_root_block=True,\n",
        "             reuse=None,\n",
        "             scope=None):\n",
        "  with tf.variable_scope(scope, 'resnet_v2', [inputs], reuse=reuse) as sc:\n",
        "    \n",
        "    end_points_collection = sc.original_name_scope+'_end_points'\n",
        "    \n",
        "    with slim.arg_scope([slim.conv2d,bottleneck,stack_blocks_dense], \n",
        "                       outputs_collections=end_points_collection):\n",
        "      net = inputs\n",
        "      if include_root_block:\n",
        "        with slim.arg_scope([slim.conv2d], activation_fn=None,normalizer_fn=None):\n",
        "          net = conv2d_same(net, 64, 7, stride=2, scope='conv1')\n",
        "        net = slim.max_pool2d(net, [3,3], stride=2, scope='pool1')\n",
        "        \n",
        "      net = stack_blocks_dense(net, blocks)\n",
        "      \n",
        "      net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope='postnorm')\n",
        "      if global_pool:\n",
        "        net = tf.reduce_mean(net, [1,2], name='pool4', keep_dims=True) #实现全局平均池化，效率比直接用avg_pool高\n",
        "        \n",
        "      if num_classes is not None:\n",
        "        net = slim.conv2d(net, num_classes, [1,1], activation_fn=None, normalizer_fn=None, scope='logits') #添加输出通道\n",
        "      \n",
        "      #再添加一个softmax层输出网络结果\n",
        "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
        "      if num_classes is not None:\n",
        "        end_points['predictions'] = slim.softmax(net, scope='predictions') \n",
        "         \n",
        "      return net, end_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "toWyLQEfsAjX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 设置几个不同深度的ResNet网络配置，来设计层数分别为50，101，152和200的ResNet。"
      ]
    },
    {
      "metadata": {
        "id": "5rhMVGcprxPg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#50层的ResNet，4个残差学习Blocks的units数量分别为3，4，6和3，总层数为(3+4+6+3)x3+2=50.需要注意的是，\n",
        "#残差学习模块之前的卷积、池化已经将尺寸缩小了4倍，前3个blocks又都包含步长为2的层，因此总尺寸缩小了4x8=32倍，输入图片尺寸最后变为224/32=7。\n",
        "# 和Inception V3类似，ResNet不断使用步长为2的层来缩减尺寸，但同时输出通道数也在持续增加，最后达到了2048\n",
        "def resnet_v2_50(intpus,\n",
        "                num_classes=None,\n",
        "                global_pool=True,\n",
        "                reuse=None,\n",
        "                scope='resnet_v2_50'):\n",
        "  blocks=[\n",
        "      Block('block1', bottleneck, [(256, 64,1)]*2+[(256,64,2)]),\n",
        "      Block('block2', bottleneck, [(512,128,1)]*3+[(512,128,2)]),\n",
        "      Block('block3', bottleneck, [(1024,256,1)]*5+[1024,245,2]),\n",
        "      Block('block4', bottleneck, [(2048,512,1)]*3)\n",
        "  ]\n",
        "  \n",
        "  return resnet_v2(inputs, blocks, num_classes, global_pool, include_root_block=True, reuse=reuse, scope=scope)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "we6hxai7t4NU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#101层ResNet和50层相比，主要变化就是把4个Blocks的units数量从3，4，6，3，提升到了3，4，23，3.即将第三个残差学习block的units数增加到接近4倍\n",
        "\n",
        "def resnet_v2_101(intpus,\n",
        "                 num_classes=None,\n",
        "                 global_pool=True,\n",
        "                 reuse=None,\n",
        "                 scope='resnet_v2_101'):\n",
        "  blocks=[\n",
        "      Block('block1', bottleneck, [(256,64,1)]*2+[(256,64,2)]),\n",
        "      Block('block2', bottelnect, [(512,128,1)]*3+[(512,128,2)]),\n",
        "      Block('block3', bottleneck, [(1024,256,1)]*22+[(1024,256,2)]),\n",
        "      Block('block4', bottleneck, [(2048,512,1)]*3)\n",
        "  ]\n",
        "  \n",
        "  return resnet_v2(inputs, blocks, num_classes, global_pool, include_root_block=True, reuse=reuse, scope=scope)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_vn4wuQVvWMs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#152层的ResNet，则是将第二个Block的unit数提高到8，将第三个Block的unit提高到36。3,8,36,3\n",
        "def resnet_v2_152(inputs, num_classes=None,\n",
        "                 global_pool=True,\n",
        "                 reuse=None,\n",
        "                 scope='resnet_v2_152'):\n",
        "  blocks=[\n",
        "      Block('block1', bottleneck, [(256,64,1)]*2+[(256,64,2)]),\n",
        "      Block('block2', bottleneck, [(512,128,1)]*7+[(512,128,2)]),\n",
        "      Block('block3', bottleneck, [(1024,256,1)]*35+[(1024,256,2)]),\n",
        "      Block('block4', bottleneck, [(2048,512,1)]*3)\n",
        "  ]\n",
        "  \n",
        "  return resnet_v2(inputs, blocks, num_classes, global_pool, include_root_block=True, reuse=reuse, scope=scope)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4vFt0eCSwyiX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#200层ResNet，相比152层的ResNet，没有继续提升第三个Block的units数，而是将第二个Block的unit数一下子提升到了3,23,36,3\n",
        "def resnet_v2_200(inputs, num_classes=None,global_pool=True,\n",
        "                 reuse=None,\n",
        "                 scope='resnet_v2_200'):\n",
        "  blocks = [\n",
        "      Block('block1', bottleneck, [(256,64,1)]*2+[(256,64,2)]),\n",
        "      Block('block2', bottleneck, [(512,128,1)]*22+[(512,128,2)]),\n",
        "      Block('block3', bottleneck, [(1024,256,1)]*35+[(1024,256,2)]),\n",
        "      Block('block4', bottleneck, [(2048, 512,1)]*3)\n",
        "  ]\n",
        "  \n",
        "  return resnet_v2(inputs, blocks, num_classes, global_pool, include_root_block=True, reuse=reuse, scope=scope)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v5I5Alrlwujk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def time_tensorflow_run(session, target, info_string):\n",
        "    num_steps_burn_in = 10\n",
        "    total_duration = 0.0\n",
        "    total_duration_squared = 0.0\n",
        "\n",
        "    for i in range(num_batches + num_steps_burn_in):\n",
        "      start_time = time.time()\n",
        "      _ = session.run(target)\n",
        "      duration = time.time() - start_time\n",
        "      if i >= num_steps_burn_in:\n",
        "        if not i % 10:\n",
        "          print ('%s: step %d, duration = %.3f' % (datetime.now(), i - num_steps_burn_in, duration))\n",
        "          \n",
        "        total_duration += duration\n",
        "        total_duration_squared += duration * duration\n",
        "          \n",
        "    mn = total_duration / num_batches\n",
        "    vr = total_duration_squared / num_batches - mn * mn\n",
        "    sd = math.sqrt(vr)\n",
        "    print ('%s: %s across %d steps, %.3f +/- %.3f sec / batch' % (datetime.now(), info_string, num_batches, mn, sd))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p8NBvxsLx_6D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "a7d18abc-6110-4892-dfc7-a490b16066e6"
      },
      "cell_type": "code",
      "source": [
        "#最后使用一直以来的评测函数time_tensorflow_run来测试152层深的ResNet的forward性能。\n",
        "batch_size = 32\n",
        "height,width = 224,224\n",
        "inputs = tf.random_uniform((batch_size, height, width, 3))\n",
        "with slim.arg_scope(resnet_arg_scope(is_training=False)):\n",
        "  net, end_points = resnet_v2_152(inputs, 1000)\n",
        "  \n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "num_batches = 100\n",
        "time_tensorflow_run(sess, net, 'forward')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-016f6de5e4fa>:23: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "2018-05-06 09:52:35.813644: step 0, duration = 0.474\n",
            "2018-05-06 09:52:40.576186: step 10, duration = 0.479\n",
            "2018-05-06 09:52:45.367559: step 20, duration = 0.480\n",
            "2018-05-06 09:52:50.158952: step 30, duration = 0.476\n",
            "2018-05-06 09:52:54.960180: step 40, duration = 0.479\n",
            "2018-05-06 09:52:59.757984: step 50, duration = 0.480\n",
            "2018-05-06 09:53:04.556299: step 60, duration = 0.479\n",
            "2018-05-06 09:53:09.349402: step 70, duration = 0.478\n",
            "2018-05-06 09:53:14.141642: step 80, duration = 0.479\n",
            "2018-05-06 09:53:19.059099: step 90, duration = 0.485\n",
            "2018-05-06 09:53:23.416910: forward across 100 steps, 0.481 +/- 0.009 sec / batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ntabfA9_tGuN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}