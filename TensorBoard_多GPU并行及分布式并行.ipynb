{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorBoard 多GPU并行及分布式并行.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/JozeeLin/google-tensorflow-exercise/blob/master/TensorBoard_%E5%A4%9AGPU%E5%B9%B6%E8%A1%8C%E5%8F%8A%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "UEF6AJqPnCpk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TensorBoard\n",
        "\n",
        "TensorBoard是TensorFlow推出的可视化工具，它可以将模型训练过程中的各种汇总数据展示出来\n",
        "\n",
        "包括标量(Scalars)、图片(Images)、音频(Audio)、计算图(Graphs)、数据分布(Distributions)、直方图(Histograms)和嵌入向量(Embeddings)。\n",
        "\n",
        "我们在使用TensorFlow训练大型深度学习神经网络时，中间的计算过程可能非常复杂，因此为了理解、调试和优化我们设计的网络，可以使用 TensorBoard观察训练过程中的各种可视化数据。\n",
        "\n",
        "可视化流程:\n",
        "- 执行TensorFlow计算图的过程中，将各种类型的数据汇总并记录到日志文件中\n",
        "- 使用TensorBoard读取这些日志文件\n",
        "- 解析日志数据并生成数据可视化的Web页面，让我们可以在浏览器中观察各种汇总数据"
      ]
    },
    {
      "metadata": {
        "id": "LGiE8sSC29Pw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "ed9ae016-340a-433b-a9a3-d1cb994bb07a"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mixuala/colab_utils"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'colab_utils'...\n",
            "remote: Counting objects: 219, done.\u001b[K\n",
            "remote: Total 219 (delta 0), reused 0 (delta 0), pack-reused 219\u001b[K\n",
            "Receiving objects: 100% (219/219), 60.24 KiB | 2.87 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JINCgb3p24Rg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import colab_utils.tboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5c7zXo0ek7oL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "max_steps = 1000\n",
        "learning_rate = 0.001\n",
        "dropout = 0.9\n",
        "data_dir='/tmp/tensorflow/mnist/input_data'\n",
        "log_dir='/tmp/tensorflow/mnist/logs/mnist_with_summaries'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwOIPs_LpVF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "e17d7ca3-2707-446f-e67c-dbe9de54591d"
      },
      "cell_type": "code",
      "source": [
        "mnist = input_data.read_data_sets(data_dir, one_hot=True)\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-19-f367a144a33e>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cQUVkqTFpegd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('input'):\n",
        "  x = tf.placeholder(tf.float32,[None,784],name='x-input')\n",
        "  y_ = tf.placeholder(tf.float32, [None,10],name='y-input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zc0yzmylptyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('input_reshape'):\n",
        "  image_shaped_input = tf.reshape(x,[-1,28,28,1])\n",
        "  tf.summary.image('input',image_shaped_input,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lpPkB_Avp5LH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IP-Gt3ZSqCex",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quS5vA4EqJOy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def variable_summaries(var):\n",
        "  with tf.name_scope('summaries'):\n",
        "    mean = tf.reduce_mean(var)\n",
        "    tf.summary.scalar('mean', mean)\n",
        "    with tf.name_scope('stddev'):\n",
        "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
        "      \n",
        "    tf.summary.scalar('stddev', stddev)\n",
        "    tf.summary.scalar('max', tf.reduce_max(var))\n",
        "    tf.summary.scalar('min', tf.reduce_min(var))\n",
        "    tf.summary.histogram('histogram',var)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OH32Lv2WqrT_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nn_layer(input_tensor, input_dim, output_dim, layer_name,act=tf.nn.relu):\n",
        "  with tf.name_scope(layer_name):\n",
        "    with tf.name_scope('weights'):\n",
        "      weights = weight_variable([input_dim, output_dim])\n",
        "      variable_summaries(weights)\n",
        "      \n",
        "    with tf.name_scope('biases'):\n",
        "      biases = bias_variable([output_dim])\n",
        "      variable_summaries(biases)\n",
        "      \n",
        "    with tf.name_scope('Wx_plus_b'):\n",
        "      preactivate = tf.matmul(input_tensor, weights)+biases\n",
        "      tf.summary.histogram('pre_activations', preactivate)\n",
        "      \n",
        "    activations = act(preactivate, name='activation')\n",
        "    tf.summary.histogram('activations', activations)\n",
        "    return activations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oP38uqBWraSq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden1 = nn_layer(x, 784, 500, 'layer1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wbmshpTArfmi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('dropout'):\n",
        "  keep_prob = tf.placeholder(tf.float32)\n",
        "  tf.summary.scalar('dropout_keep_probability',keep_prob)\n",
        "  dropped = tf.nn.dropout(hidden1, keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WXPBshBQrvJc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7uvvfOYIr0n-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "17daca1c-104f-44fb-c73c-ee57f5c388bc"
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('cross_entropy'):\n",
        "  diff = tf.nn.softmax_cross_entropy_with_logits(logits=y,labels=y_)\n",
        "  with tf.name_scope('total'):\n",
        "    cross_entropy = tf.reduce_mean(diff)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-14-9c4ed87f9dfc>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yhaqda9QsMRn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6ff10261-564f-4128-e353-4ab2b2824c19"
      },
      "cell_type": "code",
      "source": [
        "tf.summary.scalar('cross_entropy',cross_entropy)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'cross_entropy_1:0' shape=() dtype=string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "e6GpGfaFsQtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bf32059c-0a96-4857-97ab-71b99cb6ca24"
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('train'):\n",
        "  train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
        "  \n",
        "with tf.name_scope('accuracy'):\n",
        "  with tf.name_scope('correct_prediction'):\n",
        "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "    \n",
        "  with tf.name_scope('accuracy'):\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "    \n",
        "tf.summary.scalar('accuracy', accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "R_Ig6FM0syIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "merged = tf.summary.merge_all()\n",
        "train_writer = tf.summary.FileWriter(log_dir+'/train', sess.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BDLAJYQRs8e2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_writer = tf.summary.FileWriter(log_dir+'/test')\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1TWYHzaew5fm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def feed_dict(train):\n",
        "  if train:\n",
        "    xs, ys = mnist.train.next_batch(100)\n",
        "    k = dropout\n",
        "  else:\n",
        "    xs, ys = mnist.test.images, mnist.test.labels\n",
        "    k = 1.0\n",
        "    \n",
        "  return {x:xs, y_:ys, keep_prob:k}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vmp9gOZIxLhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2017
        },
        "outputId": "49c97587-5708-4246-9a35-df318d9b86ce"
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()\n",
        "for i in range(max_steps):\n",
        "  if i%10==0:\n",
        "    summary,acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
        "    test_writer.add_summary(summary, i)\n",
        "    print('Accuracy at step %s: %s' % (i,acc))\n",
        "    \n",
        "  else:\n",
        "    if i%100 == 99:\n",
        "      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "      run_metadata = tf.RunMetadata()\n",
        "      summary,_ = sess.run([merged, train_step], feed_dict=feed_dict(True),\n",
        "                          options=run_options, run_metadata=run_metadata)\n",
        "      \n",
        "      train_writer.add_run_metadata(run_metadata, 'step%03d'%i)\n",
        "      train_writer.add_summary(summary, i)\n",
        "      saver.save(sess, log_dir+'/model.ckpt',i)\n",
        "      print('Adding run metadata for',i)\n",
        "      \n",
        "    else:\n",
        "      summary,_ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
        "      train_writer.add_summary(summary, i)\n",
        "      \n",
        "train_writer.close()\n",
        "test_writer.close()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy at step 0: 0.1243\n",
            "Accuracy at step 10: 0.693\n",
            "Accuracy at step 20: 0.8185\n",
            "Accuracy at step 30: 0.8523\n",
            "Accuracy at step 40: 0.8744\n",
            "Accuracy at step 50: 0.8949\n",
            "Accuracy at step 60: 0.8968\n",
            "Accuracy at step 70: 0.904\n",
            "Accuracy at step 80: 0.9109\n",
            "Accuracy at step 90: 0.9171\n",
            "Adding run metadata for 99\n",
            "Accuracy at step 100: 0.9182\n",
            "Accuracy at step 110: 0.9202\n",
            "Accuracy at step 120: 0.9246\n",
            "Accuracy at step 130: 0.9196\n",
            "Accuracy at step 140: 0.9226\n",
            "Accuracy at step 150: 0.9275\n",
            "Accuracy at step 160: 0.9285\n",
            "Accuracy at step 170: 0.9294\n",
            "Accuracy at step 180: 0.9334\n",
            "Accuracy at step 190: 0.934\n",
            "Adding run metadata for 199\n",
            "Accuracy at step 200: 0.9355\n",
            "Accuracy at step 210: 0.9363\n",
            "Accuracy at step 220: 0.9339\n",
            "Accuracy at step 230: 0.9364\n",
            "Accuracy at step 240: 0.9326\n",
            "Accuracy at step 250: 0.9391\n",
            "Accuracy at step 260: 0.9415\n",
            "Accuracy at step 270: 0.9438\n",
            "Accuracy at step 280: 0.9411\n",
            "Accuracy at step 290: 0.9461\n",
            "Adding run metadata for 299\n",
            "Accuracy at step 300: 0.9475\n",
            "Accuracy at step 310: 0.9471\n",
            "Accuracy at step 320: 0.9477\n",
            "Accuracy at step 330: 0.9449\n",
            "Accuracy at step 340: 0.9484\n",
            "Accuracy at step 350: 0.9485\n",
            "Accuracy at step 360: 0.9517\n",
            "Accuracy at step 370: 0.9527\n",
            "Accuracy at step 380: 0.953\n",
            "Accuracy at step 390: 0.954\n",
            "Adding run metadata for 399\n",
            "Accuracy at step 400: 0.9529\n",
            "Accuracy at step 410: 0.952\n",
            "Accuracy at step 420: 0.9533\n",
            "Accuracy at step 430: 0.9494\n",
            "Accuracy at step 440: 0.9513\n",
            "Accuracy at step 450: 0.9539\n",
            "Accuracy at step 460: 0.955\n",
            "Accuracy at step 470: 0.9572\n",
            "Accuracy at step 480: 0.9577\n",
            "Accuracy at step 490: 0.9569\n",
            "Adding run metadata for 499\n",
            "Accuracy at step 500: 0.9572\n",
            "Accuracy at step 510: 0.9567\n",
            "Accuracy at step 520: 0.9578\n",
            "Accuracy at step 530: 0.9585\n",
            "Accuracy at step 540: 0.9575\n",
            "Accuracy at step 550: 0.9591\n",
            "Accuracy at step 560: 0.9594\n",
            "Accuracy at step 570: 0.9604\n",
            "Accuracy at step 580: 0.9582\n",
            "Accuracy at step 590: 0.9595\n",
            "Adding run metadata for 599\n",
            "Accuracy at step 600: 0.9584\n",
            "Accuracy at step 610: 0.9592\n",
            "Accuracy at step 620: 0.9616\n",
            "Accuracy at step 630: 0.9605\n",
            "Accuracy at step 640: 0.9596\n",
            "Accuracy at step 650: 0.9639\n",
            "Accuracy at step 660: 0.9632\n",
            "Accuracy at step 670: 0.9619\n",
            "Accuracy at step 680: 0.9621\n",
            "Accuracy at step 690: 0.9636\n",
            "Adding run metadata for 699\n",
            "Accuracy at step 700: 0.9615\n",
            "Accuracy at step 710: 0.9615\n",
            "Accuracy at step 720: 0.9644\n",
            "Accuracy at step 730: 0.9643\n",
            "Accuracy at step 740: 0.9652\n",
            "Accuracy at step 750: 0.9645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy at step 760: 0.9645\n",
            "Accuracy at step 770: 0.9643\n",
            "Accuracy at step 780: 0.9602\n",
            "Accuracy at step 790: 0.9667\n",
            "Adding run metadata for 799\n",
            "Accuracy at step 800: 0.9625\n",
            "Accuracy at step 810: 0.9626\n",
            "Accuracy at step 820: 0.9642\n",
            "Accuracy at step 830: 0.9659\n",
            "Accuracy at step 840: 0.9669\n",
            "Accuracy at step 850: 0.9674\n",
            "Accuracy at step 860: 0.9678\n",
            "Accuracy at step 870: 0.9669\n",
            "Accuracy at step 880: 0.9675\n",
            "Accuracy at step 890: 0.9626\n",
            "Adding run metadata for 899\n",
            "Accuracy at step 900: 0.9659\n",
            "Accuracy at step 910: 0.9653\n",
            "Accuracy at step 920: 0.9658\n",
            "Accuracy at step 930: 0.9668\n",
            "Accuracy at step 940: 0.9682\n",
            "Accuracy at step 950: 0.9675\n",
            "Accuracy at step 960: 0.9655\n",
            "Accuracy at step 970: 0.9667\n",
            "Accuracy at step 980: 0.9677\n",
            "Accuracy at step 990: 0.9683\n",
            "Adding run metadata for 999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z3x2EYN4ydON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "462ade74-ff53-4cdb-fe2f-8a50a89954e2"
      },
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=/tmp/tensorflow/mnist/logs/mnist_with_summaries"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^C\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Im8ovKx3A9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "b9b2f04e-e9ce-4f7a-91b0-b2c91574e7f3"
      },
      "cell_type": "code",
      "source": [
        "ROOT = %pwd\n",
        "LOG_DIR = '/tmp/tensorflow/mnist/logs/mnist_with_summaries'\n",
        "\n",
        "# will install `ngrok`, if necessary\n",
        "# will create `log_dir` if path does not exist\n",
        "colab_utils.tboard.launch_tensorboard( bin_dir=ROOT, log_dir=LOG_DIR )"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calling wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ...\n",
            "calling unzip ngrok-stable-linux-amd64.zip ...\n",
            "ngrok installed. path=/content/ngrok\n",
            "status: tensorboard=False, ngrok=False\n",
            "tensorboard url= http://33cc225b.ngrok.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://33cc225b.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "ywawjglHz357",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 多GPU并行\n",
        "\n",
        "TensorFlow中的并行主要分为模型并行和数据并行。模型并行需要根据不同模型设计不同的并行方式，其主要原理是将模型中不同计算节点放在不同硬件资源上运算。\n",
        "\n",
        "本节我们主要讲解同步的数据并行，即等待所有GPU都计算完一个batch数据的梯度后，再统一将多个梯度合在一起，并更新共享的模型参数，这种方法类似于使用了一个较大的batch。使用数据并行时，GPU的型号、速度最好一致，这样效率最高。\n",
        "\n",
        "异步的数据并行，则不等待所有GPU都完成一次训练，而是哪个GPU完成了训练，就立即将梯度更新到共享的模型参数中。\n",
        "\n",
        "通常来讲，同步的数据并行比异步的模式收敛速度更快，模型精度更高。"
      ]
    },
    {
      "metadata": {
        "id": "2LPuXqlN4wpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "09adb442-0352-4a26-8f3e-b5476d432d50"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models.git 'mymodels'"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mymodels'...\n",
            "remote: Counting objects: 16243, done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 16243 (delta 13), reused 13 (delta 7), pack-reused 16215\u001b[K\n",
            "Receiving objects: 100% (16243/16243), 424.11 MiB | 37.34 MiB/s, done.\n",
            "Resolving deltas: 100% (9590/9590), done.\n",
            "Checking out files: 100% (2164/2164), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HEJyUBsP43Mj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('mymodels/tutorials/image/cifar10')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7dAetYvz6tt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cifar10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "izrBduojD6Nj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "outputId": "8b1164a3-fff0-471d-a02e-f9434f00f0cf"
      },
      "cell_type": "code",
      "source": [
        "dir(cifar10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DATA_URL',\n",
              " 'FLAGS',\n",
              " 'IMAGE_SIZE',\n",
              " 'INITIAL_LEARNING_RATE',\n",
              " 'LEARNING_RATE_DECAY_FACTOR',\n",
              " 'MOVING_AVERAGE_DECAY',\n",
              " 'NUM_CLASSES',\n",
              " 'NUM_EPOCHS_PER_DECAY',\n",
              " 'NUM_EXAMPLES_PER_EPOCH_FOR_EVAL',\n",
              " 'NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN',\n",
              " 'TOWER_NAME',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__spec__',\n",
              " '_activation_summary',\n",
              " '_add_loss_summaries',\n",
              " '_variable_on_cpu',\n",
              " '_variable_with_weight_decay',\n",
              " 'absolute_import',\n",
              " 'cifar10_input',\n",
              " 'distorted_inputs',\n",
              " 'division',\n",
              " 'inference',\n",
              " 'inputs',\n",
              " 'loss',\n",
              " 'maybe_download_and_extract',\n",
              " 'os',\n",
              " 'print_function',\n",
              " 're',\n",
              " 'sys',\n",
              " 'tarfile',\n",
              " 'tf',\n",
              " 'train',\n",
              " 'urllib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "lX_mBfE03rLZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size=128\n",
        "max_steps=1000000\n",
        "num_gpus=4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJtzbvYK5AM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tower_loss(scope):\n",
        "  images,labels=cifar10.distorted_inputs()\n",
        "  logits = cifar10.inference(images)\n",
        "  _ = cifar10.loss(logits, labels)\n",
        "  losses = tf.get_collection('losses', scope)\n",
        "  total_loss = tf.add_n(losses, name='total_loss')\n",
        "  return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bauuh_O95Uk4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def average_gradients(tower_grads):\n",
        "  average_grads = []\n",
        "  for grad_and_vars in zip(*tower_grads):\n",
        "    grads = []\n",
        "    for g, _ in grad_and_vars:\n",
        "      expanded_g = tf.expand_dims(g,0)\n",
        "      grads.append(expanded_g)\n",
        "      \n",
        "    grad = tf.concat(grads, 0)\n",
        "    grad = tf.reduce_mean(grad, 0)\n",
        "    v = grad_and_vars[0][1]\n",
        "    grad_and_var = (grad,v)\n",
        "    average_grads.append(grad_and_var)\n",
        "    \n",
        "  return average_grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UNQkF3vm55_0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
        "    global_step = tf.get_variable('global_step', [], \n",
        "                                 initializer=tf.constant_initializer(0),\n",
        "                                 trainable=False)\n",
        "    \n",
        "    num_batches_per_epoch = cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN/batch_size\n",
        "    decay_steps = int(num_batches_per_epoch*cifar10.NUM_EPOCHS_PER_DECAY)\n",
        "    \n",
        "    lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE,\n",
        "                                   global_step,\n",
        "                                   decay_steps,\n",
        "                                   cifar10.LEARNING_RATE_DECAY_FACTOR,\n",
        "                                   staircase=True)\n",
        "    \n",
        "    opt = tf.train.GradientDescentOptimizer(lr)\n",
        "    \n",
        "    tower_grads = []\n",
        "    for i in range(num_gpus):\n",
        "      with tf.device('/gpu:%d' % i):\n",
        "        with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME,i)) as scope:\n",
        "          loss = tower_loss(scope)\n",
        "          tf.get_variable_scope().reuse_variables()\n",
        "          grads = opt.compute_gradients(loss)\n",
        "          tower_grads.append(grads)\n",
        "          \n",
        "    grads = average_gradients(tower_grads)\n",
        "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
        "    \n",
        "    saver = tf.train.Saver(tf.all_variables())\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
        "    sess.run(init)\n",
        "    tf.train.start_queue_runners(sess=sess)\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "      start_time = time.time()\n",
        "      _, loss_value = sess.run([apply_gradient_op, loss])\n",
        "      duration = time.time() - start_time\n",
        "      \n",
        "      if step % 10 == 0:\n",
        "        num_examples_per_step = batch_size * num_gpus\n",
        "        examples_per_sec = num_examples_per_step/duration\n",
        "        sec_per_batch = duration/num_gpus\n",
        "        \n",
        "        format_str = ('step %d,loss=%.2f (%.1f examples/sec; %.3f sec/batch)')\n",
        "        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))\n",
        "        \n",
        "      if step % 1000 == 0 or (step+1)==max_steps:\n",
        "        saver.save(sess, '/tmp/cifar10_train/model.ckpt', global_step=step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cp3nlQfz7NoY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cifar10.maybe_download_and_extract()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kpFwLA8YDRNl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "outputId": "1ff82a2f-a9d8-46f1-baee-2cbc16fc3cac"
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
            "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
            "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
            "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
            "WARNING:tensorflow:From <ipython-input-27-c14e81ee1e20>:30: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Please use tf.global_variables instead.\n",
            "step 0,loss=4.67 (16.1 examples/sec; 7.935 sec/batch)\n",
            "step 10,loss=4.61 (94.8 examples/sec; 1.350 sec/batch)\n",
            "step 20,loss=4.45 (95.9 examples/sec; 1.335 sec/batch)\n",
            "step 30,loss=4.40 (91.6 examples/sec; 1.397 sec/batch)\n",
            "step 40,loss=4.28 (93.8 examples/sec; 1.365 sec/batch)\n",
            "step 50,loss=4.33 (96.0 examples/sec; 1.334 sec/batch)\n",
            "step 60,loss=4.17 (92.4 examples/sec; 1.385 sec/batch)\n",
            "step 70,loss=4.27 (93.5 examples/sec; 1.369 sec/batch)\n",
            "step 80,loss=4.20 (92.4 examples/sec; 1.385 sec/batch)\n",
            "step 90,loss=4.16 (92.5 examples/sec; 1.383 sec/batch)\n",
            "step 100,loss=4.01 (91.7 examples/sec; 1.396 sec/batch)\n",
            "step 110,loss=4.33 (93.4 examples/sec; 1.371 sec/batch)\n",
            "step 120,loss=4.00 (93.7 examples/sec; 1.367 sec/batch)\n",
            "step 130,loss=4.04 (91.2 examples/sec; 1.403 sec/batch)\n",
            "step 140,loss=3.90 (90.7 examples/sec; 1.412 sec/batch)\n",
            "step 150,loss=3.89 (95.0 examples/sec; 1.347 sec/batch)\n",
            "step 160,loss=3.84 (93.4 examples/sec; 1.370 sec/batch)\n",
            "step 170,loss=3.95 (95.3 examples/sec; 1.343 sec/batch)\n",
            "step 180,loss=3.81 (92.0 examples/sec; 1.391 sec/batch)\n",
            "step 190,loss=3.80 (93.0 examples/sec; 1.376 sec/batch)\n",
            "step 200,loss=3.73 (96.0 examples/sec; 1.333 sec/batch)\n",
            "step 210,loss=3.82 (94.2 examples/sec; 1.359 sec/batch)\n",
            "step 220,loss=3.66 (94.5 examples/sec; 1.354 sec/batch)\n",
            "step 230,loss=3.65 (94.6 examples/sec; 1.353 sec/batch)\n",
            "step 240,loss=3.75 (92.5 examples/sec; 1.383 sec/batch)\n",
            "step 250,loss=3.64 (92.9 examples/sec; 1.377 sec/batch)\n",
            "step 260,loss=3.83 (91.3 examples/sec; 1.402 sec/batch)\n",
            "step 270,loss=3.54 (94.7 examples/sec; 1.352 sec/batch)\n",
            "step 280,loss=3.59 (95.4 examples/sec; 1.342 sec/batch)\n",
            "step 290,loss=3.54 (93.5 examples/sec; 1.369 sec/batch)\n",
            "step 300,loss=3.52 (96.6 examples/sec; 1.325 sec/batch)\n",
            "step 310,loss=3.49 (96.3 examples/sec; 1.329 sec/batch)\n",
            "step 320,loss=3.41 (95.9 examples/sec; 1.334 sec/batch)\n",
            "step 330,loss=3.34 (92.7 examples/sec; 1.381 sec/batch)\n",
            "step 340,loss=3.71 (91.9 examples/sec; 1.393 sec/batch)\n",
            "step 350,loss=3.34 (92.9 examples/sec; 1.378 sec/batch)\n",
            "step 360,loss=3.20 (92.6 examples/sec; 1.382 sec/batch)\n",
            "step 370,loss=3.41 (96.3 examples/sec; 1.330 sec/batch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NTqloBm5GyDE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 分布式并行\n",
        "TensorFlow的分布式并行基于gRPC通信框架，其中包括一个master负责创建session，还有多个worker负责执行计算图中的任务。\n",
        "\n",
        "首先创建一个TensorFlow Cluster对象，它包含了一组task(每个task一般是一台单独的机器)用来分布式的执行TensorFlow的计算图。\n",
        "\n",
        "一个Cluster可以切分成多个job，一个job是指一类特定的任务。我们需要为每一个task创建一个server，然后连接到Cluster上，通常每个task会执行在不同的机器上，当然也可以一台机器上执行多个task。"
      ]
    },
    {
      "metadata": {
        "id": "NqCnyB0y8qd2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import tempfile\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4qDzJW3ZIMyK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "这里使用tf.app.flags定义标记，用以在命令行执行TensorFlow程序时设置参数。在命令行中指定的参数会被TensorFlow读取，并直接转为flags。\n",
        "\n",
        "设定数据存储目录data_dir默认为/tmp/mnist-data,隐藏节点数默认为100，训练最大步数train_steps默认为1000000，batch_size默认为100，学习速率为默认0.01"
      ]
    }
  ]
}